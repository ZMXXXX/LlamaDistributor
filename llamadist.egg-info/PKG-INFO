Metadata-Version: 2.4
Name: llamadist
Version: 0.1.0
Summary: 基于QLLM的Llama模型自定义分层与分布式推理系统
Home-page: https://github.com/llamadist/LlamaDistributor
Author: LlamaDistributor Team
Author-email: contact@llamadist.dev
Project-URL: Bug Reports, https://github.com/llamadist/LlamaDistributor/issues
Project-URL: Source, https://github.com/llamadist/LlamaDistributor
Project-URL: Documentation, https://llamadist.readthedocs.io/
Keywords: llama,transformer,distributed-inference,model-partition,quantization,pytorch,qllm,nlp,large-language-model
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: torch>=1.12.0
Requires-Dist: transformers>=4.21.0
Requires-Dist: numpy>=1.21.0
Requires-Dist: psutil>=5.8.0
Requires-Dist: pyyaml>=6.0
Requires-Dist: accelerate>=0.20.0
Requires-Dist: safetensors>=0.3.0
Requires-Dist: sentencepiece>=0.1.95
Requires-Dist: tokenizers>=0.13.0
Provides-Extra: dev
Requires-Dist: pytest>=6.0; extra == "dev"
Requires-Dist: pytest-cov>=2.0; extra == "dev"
Requires-Dist: black>=22.0; extra == "dev"
Requires-Dist: isort>=5.0; extra == "dev"
Requires-Dist: flake8>=4.0; extra == "dev"
Provides-Extra: docs
Requires-Dist: sphinx>=4.0; extra == "docs"
Requires-Dist: sphinx-rtd-theme>=1.0; extra == "docs"
Requires-Dist: myst-parser>=0.18; extra == "docs"
Provides-Extra: gpu
Requires-Dist: torch>=1.12.0; extra == "gpu"
Provides-Extra: quantization
Requires-Dist: bitsandbytes>=0.40.0; extra == "quantization"
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: keywords
Dynamic: project-url
Dynamic: provides-extra
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# LlamaDistributor

基于QLLM的Llama模型自定义分层与分布式推理系统

## 🎯 项目概述

LlamaDistributor是一个基于QLLM框架的高级模型分层和分布式推理系统，专门设计用于将大型Llama模型进行智能分层、子模型保存和分布式推理。

## 🚀 核心功能

### 1. 自定义分层策略
- **灵活分层**: 支持按层数、内存、计算量等多种策略分层
- **智能切分**: 自动分析模型结构，优化分层边界
- **负载均衡**: 确保各子模型计算负载相对均衡

### 2. 子模型管理
- **模型拆分**: 将原始Llama模型按分层策略拆分成多个子模型
- **独立保存**: 每个子模型可独立保存、加载和部署
- **版本管理**: 支持子模型版本控制和回滚

### 3. 分布式推理
- **状态传递**: 基于QLLM的隐藏状态传递机制
- **KV缓存**: 高效的键值缓存在分布式环境中的同步
- **流水线**: 支持流水线并行推理

### 4. 推理协调
- **调度器**: 智能任务调度和负载分配
- **通信**: 高效的节点间通信机制
- **监控**: 实时性能监控和调试工具

## 📁 项目结构

```
LlamaDistributor/
├── README.md                 # 项目说明
├── requirements.txt          # 依赖包
├── setup.py                 # 安装脚本
├── llamadist/               # 核心模块
│   ├── __init__.py
│   ├── partitioner/         # 分层模块
│   │   ├── strategies.py    # 分层策略
│   │   ├── analyzer.py      # 模型分析
│   │   └── splitter.py      # 模型拆分
│   ├── submodels/           # 子模型管理
│   │   ├── manager.py       # 子模型管理器
│   │   ├── saver.py         # 保存加载
│   │   └── validator.py     # 验证工具
│   ├── inference/           # 推理引擎
│   │   ├── coordinator.py   # 推理协调器
│   │   ├── worker.py        # 工作节点
│   │   └── pipeline.py      # 推理流水线
│   ├── communication/       # 通信模块
│   │   ├── protocol.py      # 通信协议
│   │   ├── server.py        # 通信服务
│   │   └── client.py        # 通信客户端
│   └── utils/               # 工具模块
│       ├── config.py        # 配置管理
│       ├── monitoring.py    # 监控工具
│       └── profiler.py      # 性能分析
├── examples/                # 示例代码
│   ├── basic_partition.py   # 基础分层示例
│   ├── distributed_inference.py  # 分布式推理示例
│   └── benchmark.py         # 性能测试
├── tests/                   # 测试代码
└── configs/                 # 配置文件
    ├── partition_configs/   # 分层配置
    └── inference_configs/   # 推理配置
```

## 🛠️ 核心设计理念

### 分层策略
- **内存优先**: 根据可用内存限制进行分层
- **计算均衡**: 确保各层计算负载相对均衡
- **通信最小**: 最小化层间通信开销
- **容错性**: 支持节点故障时的自动恢复

### 状态管理
- **增量传递**: 仅传递必要的状态信息
- **缓存优化**: 智能的KV缓存管理策略
- **内存复用**: 最大化内存利用效率

### 性能优化
- **异步处理**: 支持异步推理和状态传递
- **批处理**: 支持批量推理优化
- **预加载**: 智能的模型和数据预加载

## 🎮 使用示例

### 基础分层
```python
from llamadist import LlamaPartitioner, PartitionStrategy

# 创建分层器
partitioner = LlamaPartitioner(model_path="llama-7b")

# 定义分层策略
strategy = PartitionStrategy(
    num_partitions=4,           # 分成4个子模型
    strategy_type="memory",     # 按内存分层
    max_memory_per_partition="4GB"
)

# 执行分层
submodels = partitioner.partition(strategy)

# 保存子模型
for i, submodel in enumerate(submodels):
    submodel.save(f"submodel_{i}")
```

### 分布式推理
```python
from llamadist import DistributedInference

# 创建分布式推理引擎
inference = DistributedInference(
    submodel_paths=["submodel_0", "submodel_1", "submodel_2", "submodel_3"],
    devices=["cuda:0", "cuda:1", "cuda:2", "cuda:3"]
)

# 执行推理
input_text = "Hello, how are you?"
output = inference.generate(input_text, max_length=100)
print(output)
```

## 🧪 性能特点

- **内存效率**: 相比原始模型减少60-80%的单设备内存需求
- **并行加速**: 支持多GPU/多节点并行推理
- **灵活部署**: 支持异构设备混合部署
- **高可用性**: 内置容错和恢复机制

## 📋 系统要求

- Python >= 3.8
- PyTorch >= 1.12
- CUDA >= 11.0 (可选，用于GPU加速)
- 至少8GB系统内存
- 支持的Llama模型: 7B, 13B, 30B, 65B

## 🤝 与QLLM的关系

本项目基于QLLM的核心技术，专注于：
1. 扩展QLLM的分层能力
2. 增强分布式推理功能
3. 提供更友好的用户接口
4. 优化生产环境部署

## 📄 许可证

本项目采用Apache 2.0许可证，详见LICENSE文件。

## 🙋‍♂️ 贡献指南

欢迎提交Issue和Pull Request！请参考CONTRIBUTING.md了解详细的贡献指南。 
