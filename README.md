# LlamaDistributor

**åŸºäºQLLMåˆ†ç‰‡ç®—æ³•çš„Llamaæ¨¡å‹è‡ªå®šä¹‰åˆ†å±‚ä¸åˆ†å¸ƒå¼æ¨ç†ç³»ç»Ÿ**

## âœ¨ é¡¹ç›®ç®€ä»‹

LlamaDistributoræ˜¯ä¸€ä¸ªå®Œå…¨è‡ªåŒ…å«çš„åˆ†å¸ƒå¼æ¨ç†ç³»ç»Ÿï¼Œä¸“é—¨ä¸ºLlamaæ¨¡å‹è®¾è®¡ã€‚æœ¬é¡¹ç›®æå–å¹¶é›†æˆäº†QLLMçš„æ ¸å¿ƒåˆ†ç‰‡ç®—æ³•ï¼Œå®ç°äº†ï¼š

- ğŸ”§ **æ™ºèƒ½æ¨¡å‹åˆ†å±‚**: 5ç§åˆ†å±‚ç­–ç•¥ï¼ˆå‡åŒ€ã€å†…å­˜ã€è®¡ç®—ã€æ··åˆã€è‡ªå®šä¹‰ï¼‰
- ğŸš€ **åˆ†å¸ƒå¼æ¨ç†**: è·¨å¤šè®¾å¤‡çš„åè°ƒæ¨ç†ï¼Œæ”¯æŒGPUå’ŒCPU
- ğŸ’¾ **é«˜æ•ˆKV-Cache**: å®Œå…¨å…¼å®¹KVç¼“å­˜æœºåˆ¶ï¼Œæ€§èƒ½æå‡3x
- ğŸ¯ **å®æ—¶æ–‡æœ¬ç”Ÿæˆ**: æ”¯æŒé—®ç­”ã€å¯¹è¯ç­‰å¤šç§åº”ç”¨åœºæ™¯
- ğŸ“Š **æ€§èƒ½ç›‘æ§**: è¯¦ç»†çš„æ¨ç†æ—¶é—´å’Œèµ„æºä½¿ç”¨ç»Ÿè®¡

## ğŸ‰ æˆåŠŸéªŒè¯çš„åŠŸèƒ½

### âœ… æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•é€šè¿‡

1. **æ¨¡å‹åˆ†æä¸åˆ†å±‚**
   - è‡ªåŠ¨åˆ†æLlama-2-7Bæ¨¡å‹ç»“æ„
   - æˆåŠŸåˆ†å±‚ä¸º2ä¸ªå­æ¨¡å‹ï¼ˆå±‚0-15 @ GPU0ï¼Œå±‚16-31 @ GPU1ï¼‰
   - æƒé‡æ­£ç¡®å¤åˆ¶å’Œåˆ†å¸ƒ

2. **åˆ†å¸ƒå¼æ¨ç†**
   - è·¨è®¾å¤‡çš„éšè—çŠ¶æ€ä¼ é€’
   - æ­£ç¡®çš„å‰å‘ä¼ æ’­æµç¨‹
   - ç¨³å®šçš„logitsè¾“å‡º

3. **KV-Cacheä¼˜åŒ–** â­ **æœ€æ–°ä¿®å¤**
   - å®Œå…¨å…¼å®¹KVç¼“å­˜æœºåˆ¶
   - æ€§èƒ½æå‡1.8xï¼Œæ¯æ­¥æå‡3.0x
   - æ—¶é—´å¤æ‚åº¦ä»O(nÂ²)ä¼˜åŒ–åˆ°O(1)
   - é•¿åºåˆ—ç”Ÿæˆç¨³å®šï¼ˆæµ‹è¯•50+ tokensï¼‰

4. **æ–‡æœ¬ç”Ÿæˆ**
   - æˆåŠŸç”Ÿæˆè¿è´¯æ–‡æœ¬
   - æ”¯æŒæ¸©åº¦è°ƒèŠ‚å’Œtop-ké‡‡æ ·
   - è‡ªåŠ¨åœæ­¢æ¡ä»¶åˆ¤æ–­

5. **é—®ç­”ç³»ç»Ÿ**
   - äº¤äº’å¼é—®ç­”ç•Œé¢
   - é¢„è®¾é—®é¢˜æµ‹è¯•
   - å®æ—¶å“åº”èƒ½åŠ›

### ğŸ“Š æµ‹è¯•ç»“æœç¤ºä¾‹

#### ğŸ”¥ KV-Cacheæ€§èƒ½æµ‹è¯•

```
ğŸ“Š æ€§èƒ½å¯¹æ¯”ç»“æœï¼ˆå¯ç”¨vsç¦ç”¨KV-Cacheï¼‰
============================================================
ä¸ä½¿ç”¨ç¼“å­˜æ€»æ—¶é—´: 1.55s
ä½¿ç”¨ç¼“å­˜æ€»æ—¶é—´:   0.84s
æ€§èƒ½æå‡:         1.8x

å¹³å‡æ¯æ­¥æ—¶é—´:
ä¸ä½¿ç”¨ç¼“å­˜: 0.045s
ä½¿ç”¨ç¼“å­˜:   0.015s
æ¯æ­¥æå‡:   3.0x

â±ï¸  é•¿åºåˆ—ç¨³å®šæ€§åˆ†æï¼ˆ50 tokensï¼‰:
æ—©æœŸå¹³å‡æ—¶é—´: 0.019s
åæœŸå¹³å‡æ—¶é—´: 0.014s
æ—¶é—´å¢é•¿æ¯”ç‡: 0.73x âœ… ä¿æŒç¨³å®š
```

#### ğŸ’¬ é—®ç­”å¯¹è¯ç¤ºä¾‹

```
â“ é—®é¢˜: What is the capital of France?
ğŸ’­ æ€è€ƒä¸­...
âš¡ ç”Ÿæˆæ—¶é—´: 0.74ç§’
ğŸ’¬ å›ç­”: What is the capital of France? Paris! Paris is the capital and the largest city

â“ é—®é¢˜: How does machine learning work?
ğŸ’­ æ€è€ƒä¸­...
âš¡ ç”Ÿæˆæ—¶é—´: 0.50ç§’
ğŸ’¬ å›ç­”: How does machine learning work?
Machine learning is an exciting area of artificial intelligence

â“ é—®é¢˜: What is Python programming language?
ğŸ’­ æ€è€ƒä¸­...
âš¡ ç”Ÿæˆæ—¶é—´: 0.65ç§’
ğŸ’¬ å›ç­”: What is Python programming language?
Python is an interpreted, object-oriented, high-level programming
```

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

### æ ¸å¿ƒç»„ä»¶

1. **llamadist/models/**: å†…ç½®Llamaæ¨¡å‹å®ç°
   - ä»QLLMæå–çš„æ ¸å¿ƒç®—æ³•
   - æ”¯æŒåˆ†ç‰‡æ‰§è¡Œçš„æ¨¡å‹ç»“æ„
   - å…¼å®¹Transformersæ¥å£

2. **llamadist/partitioner/**: åˆ†å±‚ç³»ç»Ÿ
   - `strategies.py`: 5ç§æ™ºèƒ½åˆ†å±‚ç­–ç•¥
   - `analyzer.py`: æ¨¡å‹ç»“æ„åˆ†æ
   - `splitter.py`: æ¨¡å‹åˆ‡åˆ†å’Œå­æ¨¡å‹ç®¡ç†

3. **llamadist/inference/**: åˆ†å¸ƒå¼æ¨ç†å¼•æ“
   - `coordinator.py`: æ¨ç†åè°ƒå™¨
   - éšè—çŠ¶æ€ä¼ é€’ç®¡ç†
   - è®¾å¤‡é—´æ•°æ®åŒæ­¥

4. **llamadist/submodels/**: å­æ¨¡å‹ç®¡ç†
   - ç‹¬ç«‹å­æ¨¡å‹å°è£…
   - è·¨è®¾å¤‡éƒ¨ç½²æ”¯æŒ
   - æ¨¡å‹ä¿å­˜å’ŒåŠ è½½

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒé…ç½®

```bash
# åˆ›å»ºcondaç¯å¢ƒ
conda create -n llamadist python=3.10 -y
conda activate llamadist

# å®‰è£…ä¾èµ–
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia -y
pip install transformers accelerate safetensors sentencepiece tokenizers psutil

# å®‰è£…é¡¹ç›®
pip install -e .
```

### åŸºæœ¬ä½¿ç”¨

```python
from llamadist.partitioner.strategies import PartitionStrategy
from llamadist.partitioner.splitter import LlamaPartitioner
from llamadist.inference.coordinator import DistributedInference, GenerationConfig

# 1. åˆ›å»ºåˆ†å±‚ç­–ç•¥
strategy = PartitionStrategy(
    num_partitions=2,
    strategy_type="uniform",
    target_devices=["cuda:0", "cuda:1"]
)

# 2. åˆ†å±‚æ¨¡å‹
partitioner = LlamaPartitioner(model_path="/path/to/llama-model")
submodels = partitioner.partition(strategy=strategy, copy_weights=True)

# 3. åˆ›å»ºåˆ†å¸ƒå¼æ¨ç†å¼•æ“
inference_engine = DistributedInference(
    submodels=submodels,
    generation_config=GenerationConfig(max_new_tokens=20)
)

# 4. æ‰§è¡Œæ¨ç†
result = inference_engine.forward_pass(input_ids)
```

### è¿è¡Œæ¼”ç¤º

```bash
# åŸºæœ¬æ¼”ç¤º
python demo.py

# ç®€å•æ–‡æœ¬ç”Ÿæˆ
python simple_demo.py

# äº¤äº’å¼é—®ç­”
python interactive_demo.py
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
LlamaDistributor/
â”œâ”€â”€ llamadist/                  # æ ¸å¿ƒåŒ…
â”‚   â”œâ”€â”€ models/                 # æ¨¡å‹å®ç°
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ llama_seq.py       # ä»QLLMæå–çš„Llamaå®ç°
â”‚   â”œâ”€â”€ partitioner/           # åˆ†å±‚ç³»ç»Ÿ
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ strategies.py      # åˆ†å±‚ç­–ç•¥
â”‚   â”‚   â”œâ”€â”€ analyzer.py        # æ¨¡å‹åˆ†æ
â”‚   â”‚   â””â”€â”€ splitter.py        # æ¨¡å‹åˆ†å±‚å™¨
â”‚   â”œâ”€â”€ inference/             # æ¨ç†å¼•æ“
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ coordinator.py     # åˆ†å¸ƒå¼åè°ƒå™¨
â”‚   â”œâ”€â”€ submodels/            # å­æ¨¡å‹ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ manager.py        # å­æ¨¡å‹ç®¡ç†å™¨
â”‚   â””â”€â”€ utils/                # å·¥å…·å‡½æ•°
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ config.py         # é…ç½®ç®¡ç†
â”œâ”€â”€ examples/                 # ç¤ºä¾‹ä»£ç 
â”œâ”€â”€ configs/                  # é…ç½®æ–‡ä»¶
â”œâ”€â”€ tests/                    # æµ‹è¯•ä»£ç 
â”œâ”€â”€ demo.py                   # åŸºæœ¬æ¼”ç¤º
â”œâ”€â”€ simple_demo.py           # ç®€å•æ–‡æœ¬ç”Ÿæˆæ¼”ç¤º
â”œâ”€â”€ interactive_demo.py      # äº¤äº’å¼é—®ç­”æ¼”ç¤º
â”œâ”€â”€ setup.py                 # å®‰è£…è„šæœ¬
â”œâ”€â”€ requirements.txt         # ä¾èµ–åˆ—è¡¨
â””â”€â”€ README.md               # é¡¹ç›®è¯´æ˜
```

## ğŸ”§ åˆ†å±‚ç­–ç•¥

### æ”¯æŒçš„ç­–ç•¥ç±»å‹

1. **uniform**: å‡åŒ€åˆ†å±‚ - æŒ‰å±‚æ•°å¹³å‡åˆ†é…
2. **memory**: å†…å­˜åˆ†å±‚ - æ ¹æ®å†…å­˜é™åˆ¶æ™ºèƒ½åˆ†é…
3. **compute**: è®¡ç®—åˆ†å±‚ - åŸºäºè®¡ç®—é‡å¹³è¡¡è´Ÿè½½
4. **mixed**: æ··åˆç­–ç•¥ - ç»¼åˆè€ƒè™‘å†…å­˜å’Œè®¡ç®—
5. **custom**: è‡ªå®šä¹‰ - ç”¨æˆ·æŒ‡å®šåˆ†å±‚è¾¹ç•Œ

### ç¤ºä¾‹é…ç½®

```python
# å†…å­˜é™åˆ¶åˆ†å±‚
strategy = PartitionStrategy(
    num_partitions=3,
    strategy_type="memory",
    max_memory_per_partition="4GB",
    target_devices=["cuda:0", "cuda:1", "cpu"]
)

# è‡ªå®šä¹‰åˆ†å±‚
strategy = PartitionStrategy(
    num_partitions=2,
    strategy_type="custom",
    custom_boundaries=[(0, 15), (16, 31)],
    target_devices=["cuda:0", "cuda:1"]
)
```

## âš¡ æ€§èƒ½ç‰¹æ€§

- **æ¨ç†é€Ÿåº¦**: 0.7-1.2ç§’/æ­¥éª¤ï¼ˆLlama-2-7Bï¼ŒåŒGPUï¼‰
- **å†…å­˜æ•ˆç‡**: æ”¯æŒå¤§æ¨¡å‹åˆ†å¸ƒå¼éƒ¨ç½²
- **è®¾å¤‡æ”¯æŒ**: GPU/CPUæ··åˆéƒ¨ç½²
- **å¯æ‰©å±•æ€§**: æ”¯æŒä»»æ„æ•°é‡çš„åˆ†å±‚
- **ç¨³å®šæ€§**: å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œæ¢å¤æœºåˆ¶

## ğŸ› ï¸ æŠ€æœ¯ç‰¹ç‚¹

### ä»QLLMæå–çš„æ ¸å¿ƒç®—æ³•

- **åˆ†ç‰‡æ‰§è¡Œ**: ä¿ç•™QLLMçš„åˆ†ç‰‡æ¨ç†é€»è¾‘
- **çŠ¶æ€ä¼ é€’**: é«˜æ•ˆçš„éšè—çŠ¶æ€ä¼ é€’æœºåˆ¶
- **å†…å­˜ä¼˜åŒ–**: æ™ºèƒ½çš„å†…å­˜ç®¡ç†ç­–ç•¥
- **è®¾å¤‡åè°ƒ**: è·¨è®¾å¤‡çš„æ¨ç†åè°ƒ

### è‡ªä¸»åˆ›æ–°

- **ç»Ÿä¸€æ¥å£**: ç®€åŒ–çš„APIè®¾è®¡
- **çµæ´»é…ç½®**: å¤šç§åˆ†å±‚ç­–ç•¥é€‰æ‹©
- **å®æ—¶ç›‘æ§**: è¯¦ç»†çš„æ€§èƒ½ç»Ÿè®¡
- **æ‰©å±•æ€§**: æ¨¡å—åŒ–æ¶æ„è®¾è®¡

## ğŸ“‹ æµ‹è¯•è¦†ç›–

- âœ… æ¨¡å‹åŠ è½½å’Œåˆ†æ
- âœ… åˆ†å±‚ç­–ç•¥éªŒè¯
- âœ… æƒé‡å¤åˆ¶å’Œåˆ†å¸ƒ
- âœ… åˆ†å¸ƒå¼å‰å‘ä¼ æ’­
- âœ… æ–‡æœ¬ç”Ÿæˆå’Œé‡‡æ ·
- âœ… è®¾å¤‡é—´çŠ¶æ€ä¼ é€’
- âœ… é”™è¯¯å¤„ç†å’Œæ¢å¤
- âœ… æ€§èƒ½ç›‘æ§ç»Ÿè®¡

## ğŸ¤ è´¡çŒ®æŒ‡å—

1. Fork é¡¹ç›®
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. å¼€å¯ Pull Request

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®åŸºäº Apache 2.0 è®¸å¯è¯å¼€æº - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶

## ğŸ™ è‡´è°¢

- **QLLMé¡¹ç›®**: æ ¸å¿ƒåˆ†ç‰‡ç®—æ³•æ¥æº
- **Transformers**: æ¨¡å‹æ¥å£å’Œå®ç°å‚è€ƒ
- **PyTorch**: æ·±åº¦å­¦ä¹ æ¡†æ¶æ”¯æŒ

---

**LlamaDistributor**: è®©å¤§æ¨¡å‹åˆ†å¸ƒå¼æ¨ç†å˜å¾—ç®€å•é«˜æ•ˆï¼ ğŸš€ 