# ğŸ” LlamaDistributor

**åŸºäºQLLMåˆ†ç‰‡ç®—æ³•çš„Llamaæ¨¡å‹åˆ†å±‚åˆ†å¸ƒå¼æ¨ç†ç³»ç»Ÿ**

## ğŸŒŸ æ¦‚è¿°

LlamaDistributoræ˜¯ä¸€ä¸ªä¸“é—¨é’ˆå¯¹Llamaæ¨¡å‹è®¾è®¡çš„ layer partition inference systemã€‚è¯¥ç³»ç»ŸåŸºäºQLLMé¡¹ç›®çš„æ ¸å¿ƒåˆ†ç‰‡ç®—æ³•ï¼Œé€šè¿‡æ™ºèƒ½æ¨¡å‹åˆ†å±‚å’Œè·¨è®¾å¤‡åè°ƒæ¨ç†ï¼Œå®ç°äº†LLMæ¨¡å‹åˆ’åˆ†å’Œåˆ†å¸ƒå¼éƒ¨ç½²ï¼Œè¯¥é¡¹ç›®æ­£åœ¨æŒç»­æ›´æ–°ä¸­ã€‚

### æ ¸å¿ƒç‰¹æ€§

- **æ™ºèƒ½æ¨¡å‹åˆ†å±‚**: æä¾›å‡åŒ€åˆ†å±‚ã€è‡ªå®šä¹‰åˆ†å±‚ã€å†…å­˜æ„ŸçŸ¥ã€è®¡ç®—è´Ÿè½½å‡è¡¡å’Œæ··åˆç­–ç•¥äº”ç§åˆ†å±‚ç­–ç•¥
- **åˆ†å¸ƒå¼æ¨ç†**: æ”¯æŒè·¨å¤šGPUå’ŒCPUè®¾å¤‡çš„åè°ƒæ¨ç†ï¼Œå®ç°å¤§æ¨¡å‹çš„åˆ†å¸ƒå¼éƒ¨ç½²
- **é«˜æ•ˆç¼“å­˜æœºåˆ¶**: å…¼å®¹KV-Cacheï¼Œç»´æŠ¤KV-Cacheå±‚é—´ä¼ é€’ï¼Œä¼˜åŒ–æ¨ç†æ€§èƒ½
- **çµæ´»é…ç½®**: æ”¯æŒå¤šç§è®¾å¤‡é…ç½®å’Œåˆ†å±‚ç­–ç•¥ï¼Œé€‚åº”ä¸åŒç¡¬ä»¶ç¯å¢ƒ
- **æ€§èƒ½ç›‘æ§**: é›†æˆæ¨ç†æ€§èƒ½benchmark

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

### æ ¸å¿ƒç»„ä»¶

**llamadist/models/**
- é›†æˆQLLMçš„Llamaæ¨¡å‹åˆ†å±‚æ¨ç†å®ç°
- æ”¯æŒåˆ†ç‰‡æ‰§è¡Œçš„æ¨¡å‹ç»“æ„
- ä¸Transformersåº“å…¼å®¹çš„æ¥å£è®¾è®¡

**llamadist/partitioner/**
- `strategies.py`: å®ç°å¤šç§åˆ†å±‚ç­–ç•¥
- `analyzer.py`: æ¨¡å‹ç»“æ„åˆ†æå’Œèµ„æºè¯„ä¼°
- `splitter.py`: æ¨¡å‹åˆ†å±‚å’Œå­æ¨¡å‹ç®¡ç†

**llamadist/inference/**
- `coordinator.py`: åˆ†å¸ƒå¼æ¨ç†åè°ƒå™¨
- éšè—çŠ¶æ€ä¼ é€’ç®¡ç†
- è·¨è®¾å¤‡æ•°æ®åŒæ­¥å’Œç¼“å­˜ç®¡ç†

**llamadist/submodels/**
- ç‹¬ç«‹å­æ¨¡å‹å°è£…å’Œç®¡ç†
- è·¨è®¾å¤‡éƒ¨ç½²æ”¯æŒ
- æ¨¡å‹åºåˆ—åŒ–å’ŒåŠ è½½

## âœ… åŠŸèƒ½éªŒè¯

### å·²éªŒè¯åŠŸèƒ½

**æ¨¡å‹åˆ†æä¸åˆ†å±‚**
- è‡ªåŠ¨åˆ†æLlama-2-7Bæ¨¡å‹ç»“æ„
- æˆåŠŸå®ç°æ¨¡å‹åˆ†å±‚ï¼ˆå¦‚ï¼šå±‚0-15éƒ¨ç½²è‡³GPU0ï¼Œå±‚16-31éƒ¨ç½²è‡³GPU1ï¼‰
- æƒé‡æ­£ç¡®å¤åˆ¶å’Œåˆ†å¸ƒ

**åˆ†å¸ƒå¼æ¨ç†**
- è·¨è®¾å¤‡éšè—çŠ¶æ€ä¼ é€’
- å®Œæ•´çš„å‰å‘ä¼ æ’­æµç¨‹
- ç¨³å®šçš„logitsè¾“å‡º

**KV-Cacheä¼˜åŒ–**
- å®Œå…¨å…¼å®¹KVç¼“å­˜æœºåˆ¶
- æ¨¡å‹å±‚é—´KV-Cacheè¿è´¯ä¼ é€’
- é•¿åºåˆ—ç”Ÿæˆç¨³å®šæ€§ä¿è¯

**æ–‡æœ¬ç”Ÿæˆ**
- è¿è´¯æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›
- æ”¯æŒtemperatureè°ƒèŠ‚å’Œtop-ké‡‡æ ·

### åº”ç”¨ç¤ºä¾‹

#### é—®ç­”ç³»ç»Ÿæ¼”ç¤º

```
è¾“å…¥: "What is the capital of France?"
è¾“å‡º: "What is the capital of France? Paris! Paris is the capital and the largest city..."
ç”Ÿæˆæ—¶é—´: 0.74ç§’

è¾“å…¥: "How does machine learning work?"
è¾“å‡º: "How does machine learning work? Machine learning is an exciting area of artificial intelligence..."
ç”Ÿæˆæ—¶é—´: 0.50ç§’
```

## ğŸ› ï¸ ç¯å¢ƒé…ç½®

### ä¾èµ–å®‰è£…

```bash
# åˆ›å»ºPythonç¯å¢ƒ
conda create -n llamadist python=3.10 -y
conda activate llamadist

# å®‰è£…PyTorchå’ŒCUDAæ”¯æŒ
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia -y

# å®‰è£…å…¶ä»–ä¾èµ–
pip install transformers accelerate safetensors sentencepiece tokenizers psutil

# å®‰è£…é¡¹ç›®
pip install -e .
```

### åŸºæœ¬ä½¿ç”¨

```python
from llamadist.partitioner.strategies import PartitionStrategy
from llamadist.partitioner.splitter import LlamaPartitioner
from llamadist.inference.coordinator import DistributedInference, GenerationConfig

# åˆ›å»ºåˆ†å±‚ç­–ç•¥
strategy = PartitionStrategy(
    num_partitions=2,
    strategy_type="uniform",
    target_devices=["cuda:0", "cuda:1"]
)

# æ¨¡å‹åˆ†å±‚
partitioner = LlamaPartitioner(model_path="/path/to/llama-model")
submodels = partitioner.partition(strategy=strategy, copy_weights=True)

# åˆ›å»ºåˆ†å¸ƒå¼æ¨ç†å¼•æ“
inference_engine = DistributedInference(
    submodels=submodels,
    generation_config=GenerationConfig(max_new_tokens=20)
)

# æ‰§è¡Œæ¨ç†
result = inference_engine.forward_pass(input_ids)
```

### æ¼”ç¤ºç¨‹åº

```bash
# åŸºæœ¬åŠŸèƒ½æ¼”ç¤º
python demo.py

# æ–‡æœ¬ç”Ÿæˆæ¼”ç¤º
python simple_demo.py

# äº¤äº’å¼é—®ç­”
python interactive_demo.py
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
LlamaDistributor/
â”œâ”€â”€ llamadist/                  # æ ¸å¿ƒæ¨¡å—
â”‚   â”œâ”€â”€ models/                 # æ¨¡å‹å®ç°
â”‚   â”‚   â””â”€â”€ llama_seq.py       # QLLMé›†æˆçš„Llamaå®ç°
â”‚   â”œâ”€â”€ partitioner/           # åˆ†å±‚ç³»ç»Ÿ
â”‚   â”‚   â”œâ”€â”€ strategies.py      # åˆ†å±‚ç­–ç•¥
â”‚   â”‚   â”œâ”€â”€ analyzer.py        # æ¨¡å‹åˆ†æ
â”‚   â”‚   â””â”€â”€ splitter.py        # æ¨¡å‹åˆ†å±‚å™¨
â”‚   â”œâ”€â”€ inference/             # æ¨ç†å¼•æ“
â”‚   â”‚   â””â”€â”€ coordinator.py     # åˆ†å¸ƒå¼åè°ƒå™¨
â”‚   â”œâ”€â”€ submodels/            # å­æ¨¡å‹ç®¡ç†
â”‚   â”‚   â””â”€â”€ manager.py        # å­æ¨¡å‹ç®¡ç†å™¨
â”‚   â””â”€â”€ utils/                # å·¥å…·æ¨¡å—
â”‚       â””â”€â”€ config.py         # é…ç½®ç®¡ç†
â”œâ”€â”€ examples/                 # ç¤ºä¾‹ä»£ç 
â”œâ”€â”€ configs/                  # åˆ†å±‚ç­–ç•¥ã€prompté…ç½®æ–‡ä»¶
â”œâ”€â”€ tests/                    # æµ‹è¯•ä»£ç 
â””â”€â”€ llama_partition.py       # åˆ†å±‚æ¨ç†æ‰§è¡Œå…¥å£
```

## ğŸ® åˆ†å±‚ç­–ç•¥

### ç­–ç•¥ç±»å‹

**uniformï¼ˆå‡åŒ€åˆ†å±‚ï¼‰**
- æŒ‰å±‚æ•°å¹³å‡åˆ†é…åˆ°å„è®¾å¤‡
- é€‚ç”¨äºè®¾å¤‡é…ç½®ç›¸è¿‘çš„ç¯å¢ƒ

**memoryï¼ˆå†…å­˜æ„ŸçŸ¥åˆ†å±‚ï¼‰**
- æ ¹æ®å†…å­˜é™åˆ¶æ™ºèƒ½åˆ†é…
- é€‚ç”¨äºè®¾å¤‡å†…å­˜å·®å¼‚è¾ƒå¤§çš„ç¯å¢ƒ

**computeï¼ˆè®¡ç®—è´Ÿè½½å‡è¡¡ï¼‰**
- åŸºäºè®¡ç®—é‡å¹³è¡¡è´Ÿè½½
- é€‚ç”¨äºè®¾å¤‡è®¡ç®—èƒ½åŠ›ä¸åŒçš„ç¯å¢ƒ

**mixedï¼ˆæ··åˆç­–ç•¥ï¼‰**
- ç»¼åˆè€ƒè™‘å†…å­˜å’Œè®¡ç®—èµ„æº
- é€‚ç”¨äºå¤æ‚çš„å¼‚æ„ç¯å¢ƒ

**customï¼ˆè‡ªå®šä¹‰åˆ†å±‚ï¼‰**
- ç”¨æˆ·æŒ‡å®šåˆ†å±‚è¾¹ç•Œ
- é€‚ç”¨äºç‰¹å®šä¼˜åŒ–éœ€æ±‚

### é…ç½®ç¤ºä¾‹

```python
# å†…å­˜é™åˆ¶åˆ†å±‚
strategy = PartitionStrategy(
    num_partitions=3,
    strategy_type="memory",
    max_memory_per_partition="4GB",
    target_devices=["cuda:0", "cuda:1", "cpu"]
)

# è‡ªå®šä¹‰åˆ†å±‚
strategy = PartitionStrategy(
    num_partitions=2,
    strategy_type="custom",
    custom_boundaries=[(0, 15), (16, 31)],
    target_devices=["cuda:0", "cuda:1"]
)

# å•è®¾å¤‡åˆ†å±‚ - å‡åŒ€åˆ†å±‚
strategy = PartitionStrategy(
    num_partitions=4,
    strategy_type="single_device",
    single_device="cuda:0"
)

# å•è®¾å¤‡åˆ†å±‚ - è‡ªå®šä¹‰åˆ†å±‚ç‚¹
strategy = PartitionStrategy(
    num_partitions=3,
    strategy_type="single_device",
    single_device="cuda:0",
    custom_boundaries=[(0, 10), (11, 21), (22, 31)]
)
```

## ğŸ¤ æŠ€æœ¯æ”¯æŒ

æœ¬é¡¹ç›®åŸºäºä»¥ä¸‹å¼€æºé¡¹ç›®æ„å»ºï¼š

- **QLLMé¡¹ç›®**: æä¾›æ ¸å¿ƒåˆ†ç‰‡ç®—æ³•
- **Transformers**: æ¨¡å‹æ¥å£å’Œå®ç°åŸºç¡€
- **PyTorch**: æ·±åº¦å­¦ä¹ æ¡†æ¶æ”¯æŒ

---

LlamaDistributoræ—¨åœ¨æµ‹è¯•LLM layer partition & distributed inferenceï¼Œä»åœ¨å¼€å‘ä¸­ã€‚ 