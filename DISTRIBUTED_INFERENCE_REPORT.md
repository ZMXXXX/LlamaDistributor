# LlamaDistributoråˆ†å±‚åˆ†å¸ƒå¼æ¨ç†ç³»ç»ŸæŠ€æœ¯æŠ¥å‘Š

## ğŸ“‹ é¡¹ç›®æ¦‚è¿°

LlamaDistributoræ˜¯ä¸€ä¸ªåŸºäºQLLMåˆ†ç‰‡ç®—æ³•çš„Llamaæ¨¡å‹è‡ªå®šä¹‰åˆ†å±‚ä¸åˆ†å¸ƒå¼æ¨ç†ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨å•è®¾å¤‡ä¸Šæ— æ³•è¿è¡Œæˆ–è¿è¡Œæ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œé€šè¿‡æ™ºèƒ½åˆ†å±‚å’Œåˆ†å¸ƒå¼åè°ƒå®ç°é«˜æ•ˆçš„è·¨è®¾å¤‡æ¨ç†ã€‚

## ğŸ¯ è®¾è®¡ç›®æ ‡

1. **æ¨¡å‹åˆ†å±‚**: å°†å¤§å‹Llamaæ¨¡å‹æ™ºèƒ½åœ°æ‹†åˆ†æˆå¤šä¸ªå­æ¨¡å‹
2. **åˆ†å¸ƒå¼æ¨ç†**: å®ç°è·¨å¤šä¸ªè®¾å¤‡ï¼ˆGPU/CPUï¼‰çš„åè°ƒæ¨ç†
3. **æ€§èƒ½ä¼˜åŒ–**: æ”¯æŒKV-Cacheç­‰ä¼˜åŒ–æŠ€æœ¯ï¼Œæå‡æ¨ç†æ•ˆç‡
4. **çµæ´»é…ç½®**: æ”¯æŒå¤šç§åˆ†å±‚ç­–ç•¥å’Œè®¾å¤‡é…ç½®
5. **æ˜“ç”¨æ€§**: æä¾›ç®€æ´çš„APIå’Œä¸°å¯Œçš„ç¤ºä¾‹

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

### æ•´ä½“æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LlamaDistributor System                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Application Layer                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚Interactive  â”‚  â”‚Simple Demo  â”‚  â”‚Batch Test   â”‚             â”‚
â”‚  â”‚Demo         â”‚  â”‚             â”‚  â”‚             â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  API Layer                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚           DistributedInference                              â”‚ â”‚
â”‚  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚ â”‚
â”‚  â”‚    â”‚Generation   â”‚  â”‚Forward Pass â”‚  â”‚KV-Cache Mgmtâ”‚       â”‚ â”‚
â”‚  â”‚    â”‚Config       â”‚  â”‚Coordinator  â”‚  â”‚             â”‚       â”‚ â”‚
â”‚  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Core Layer                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Partitioner    â”‚ â”‚  SubModels      â”‚ â”‚  Models         â”‚   â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚
â”‚  â”‚ â”‚Strategies   â”‚ â”‚ â”‚ â”‚LlamaSubModelâ”‚ â”‚ â”‚ â”‚LlamaSeq     â”‚ â”‚   â”‚
â”‚  â”‚ â”‚Analyzer     â”‚ â”‚ â”‚ â”‚Manager      â”‚ â”‚ â”‚ â”‚Components   â”‚ â”‚   â”‚
â”‚  â”‚ â”‚Splitter     â”‚ â”‚ â”‚ â”‚             â”‚ â”‚ â”‚ â”‚             â”‚ â”‚   â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Device Layer                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚   GPU:0     â”‚  â”‚   GPU:1     â”‚  â”‚   CPU       â”‚             â”‚
â”‚  â”‚SubModel_0   â”‚  â”‚SubModel_1   â”‚  â”‚SubModel_N   â”‚             â”‚
â”‚  â”‚Layers 0-15  â”‚  â”‚Layers 16-31 â”‚  â”‚Layers ...   â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ ¸å¿ƒæ¨¡å—ç»“æ„

```
llamadist/
â”œâ”€â”€ __init__.py                 # åŒ…åˆå§‹åŒ–å’Œå¯¼å‡º
â”œâ”€â”€ partitioner/                # æ¨¡å‹åˆ†å±‚æ¨¡å—
â”‚   â”œâ”€â”€ strategies.py          # åˆ†å±‚ç­–ç•¥
â”‚   â”œâ”€â”€ analyzer.py            # æ¨¡å‹åˆ†æ
â”‚   â””â”€â”€ splitter.py            # æ¨¡å‹åˆ†å±‚å™¨
â”œâ”€â”€ inference/                  # åˆ†å¸ƒå¼æ¨ç†æ¨¡å—
â”‚   â””â”€â”€ coordinator.py         # æ¨ç†åè°ƒå™¨
â”œâ”€â”€ models/                     # æ¨¡å‹å®ç°æ¨¡å—
â”‚   â””â”€â”€ llama_seq.py           # Llamaåºåˆ—åŒ–æ¨¡å‹
â”œâ”€â”€ submodels/                  # å­æ¨¡å‹ç®¡ç†æ¨¡å—
â”‚   â””â”€â”€ manager.py             # å­æ¨¡å‹ç®¡ç†å™¨
â”œâ”€â”€ communication/              # é€šä¿¡æ¨¡å—
â””â”€â”€ utils/                      # å·¥å…·æ¨¡å—
    â””â”€â”€ config.py              # é…ç½®ç®¡ç†
```

## ğŸ”§ æ ¸å¿ƒæŠ€æœ¯å®ç°

### 1. æ¨¡å‹åˆ†å±‚ç­–ç•¥ (`partitioner/strategies.py`)

ç³»ç»Ÿæ”¯æŒ5ç§åˆ†å±‚ç­–ç•¥ï¼š

```python
class PartitionStrategy:
    """åˆ†å±‚ç­–ç•¥ç±»"""
    
    def __init__(
        self,
        num_partitions: int,
        strategy_type: str = "uniform",
        target_devices: List[str] = None,
        max_memory_per_partition: str = None,
        custom_boundaries: List[Tuple[int, int]] = None
    ):
        self.num_partitions = num_partitions
        self.strategy_type = strategy_type
        self.target_devices = target_devices or ["cpu"] * num_partitions
        self.max_memory_per_partition = max_memory_per_partition
        self.custom_boundaries = custom_boundaries
    
    def create_partitions(self, model_info: ModelInfo) -> List[PartitionConfig]:
        """æ ¹æ®ç­–ç•¥åˆ›å»ºåˆ†å±‚é…ç½®"""
        if self.strategy_type == "uniform":
            return self._create_uniform_partitions(model_info)
        elif self.strategy_type == "memory":
            return self._create_memory_based_partitions(model_info)
        elif self.strategy_type == "compute":
            return self._create_compute_based_partitions(model_info)
        elif self.strategy_type == "mixed":
            return self._create_mixed_partitions(model_info)
        elif self.strategy_type == "custom":
            return self._create_custom_partitions(model_info)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„åˆ†å±‚ç­–ç•¥: {self.strategy_type}")
```

#### å‡åŒ€åˆ†å±‚ç­–ç•¥å®ç°

```python
def _create_uniform_partitions(self, model_info: ModelInfo) -> List[PartitionConfig]:
    """åˆ›å»ºå‡åŒ€åˆ†å±‚"""
    total_layers = model_info.num_layers
    layers_per_partition = total_layers // self.num_partitions
    remaining_layers = total_layers % self.num_partitions
    
    partitions = []
    current_start = 0
    
    for i in range(self.num_partitions):
        # åˆ†é…å±‚æ•°ï¼ˆå¹³å‡åˆ†é…ï¼Œä½™æ•°ç»™å‰å‡ ä¸ªåˆ†å±‚ï¼‰
        current_layers = layers_per_partition + (1 if i < remaining_layers else 0)
        current_end = current_start + current_layers - 1
        
        partition = PartitionConfig(
            layer_start=current_start,
            layer_end=current_end,
            device=self.target_devices[i] if i < len(self.target_devices) else "cpu",
            memory_limit=self.max_memory_per_partition
        )
        partitions.append(partition)
        
        current_start = current_end + 1
    
    return partitions
```

### 2. æ¨¡å‹åˆ†æå™¨ (`partitioner/analyzer.py`)

```python
class LlamaModelAnalyzer:
    """Llamaæ¨¡å‹åˆ†æå™¨"""
    
    def __init__(self, model_path: str, config: Optional[Dict] = None):
        self.model_path = model_path
        self.config = config
        self._model = None
    
    def analyze_model(self, detailed: bool = True) -> ModelInfo:
        """åˆ†ææ¨¡å‹ç»“æ„å’Œèµ„æºéœ€æ±‚"""
        model = self.load_model()
        config = model.config
        
        # åˆ†ææ¯å±‚çš„å‚æ•°å’Œå†…å­˜ä½¿ç”¨
        layer_infos = []
        layer_memory_costs = []
        layer_compute_costs = []
        layer_params = []
        
        for i in range(config.num_hidden_layers):
            layer_info = self._analyze_layer(model.model.layers[i], i)
            layer_infos.append(layer_info)
            layer_memory_costs.append(layer_info.memory_usage)
            layer_compute_costs.append(layer_info.compute_cost)
            layer_params.append(layer_info.param_count)
        
        total_params = sum(p.numel() for p in model.parameters())
        total_memory = self._estimate_memory_usage(model)
        
        return ModelInfo(
            model_name="llama",
            num_layers=config.num_hidden_layers,
            total_params=total_params,
            total_memory=total_memory,
            hidden_size=config.hidden_size,
            vocab_size=config.vocab_size,
            layer_infos=layer_infos,
            layer_memory_costs=layer_memory_costs,
            layer_compute_costs=layer_compute_costs,
            layer_params=layer_params
        )
```

### 3. æ¨¡å‹åˆ†å±‚å™¨ (`partitioner/splitter.py`)

#### å­æ¨¡å‹ç±»å®šä¹‰

```python
class LlamaSubModel(nn.Module):
    """Llamaå­æ¨¡å‹ - è¡¨ç¤ºåˆ†å±‚åçš„å•ä¸ªå­æ¨¡å‹"""
    
    def __init__(
        self,
        config: LlamaConfig,
        partition_config: PartitionConfig,
        partition_idx: int,
        total_partitions: int
    ):
        super().__init__()
        
        self.config = config
        self.partition_config = partition_config
        self.partition_idx = partition_idx
        self.total_partitions = total_partitions
        
        # å­æ¨¡å‹åŒ…å«çš„å±‚èŒƒå›´
        self.layer_start = partition_config.layer_start
        self.layer_end = partition_config.layer_end
        self.num_layers = self.layer_end - self.layer_start + 1
        
        # åˆ¤æ–­æ˜¯å¦æ˜¯ç¬¬ä¸€ä¸ªæˆ–æœ€åä¸€ä¸ªåˆ†å±‚
        self.is_first_partition = (partition_idx == 0)
        self.is_last_partition = (partition_idx == total_partitions - 1)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._init_components()
    
    def _init_components(self):
        """åˆå§‹åŒ–å­æ¨¡å‹ç»„ä»¶"""
        # å¦‚æœæ˜¯ç¬¬ä¸€ä¸ªåˆ†å±‚ï¼Œéœ€è¦åµŒå…¥å±‚
        if self.is_first_partition:
            self.embed_tokens = nn.Embedding(
                self.config.vocab_size, 
                self.config.hidden_size,
                self.config.pad_token_id
            )
        
        # è§£ç å™¨å±‚
        self.layers = nn.ModuleList([
            LlamaDecoderLayer(self.config) 
            for _ in range(self.num_layers)
        ])
        
        # å¦‚æœæ˜¯æœ€åä¸€ä¸ªåˆ†å±‚ï¼Œéœ€è¦å½’ä¸€åŒ–å±‚å’Œè¯­è¨€æ¨¡å‹å¤´
        if self.is_last_partition:
            self.norm = LlamaRMSNorm(
                self.config.hidden_size, 
                eps=self.config.rms_norm_eps
            )
            self.lm_head = nn.Linear(
                self.config.hidden_size, 
                self.config.vocab_size, 
                bias=False
            )
```

#### å­æ¨¡å‹å‰å‘ä¼ æ’­å®ç°

```python
def forward(
    self,
    hidden_states: Optional[torch.Tensor] = None,
    input_ids: Optional[torch.Tensor] = None,
    attention_mask: Optional[torch.Tensor] = None,
    position_ids: Optional[torch.Tensor] = None,
    past_key_values: Optional[List[torch.FloatTensor]] = None,
    use_cache: bool = False,
    output_attentions: bool = False,
    return_dict: bool = True,
) -> Dict[str, Any]:
    """å­æ¨¡å‹å‰å‘ä¼ æ’­"""
    
    # ç¬¬ä¸€ä¸ªåˆ†å±‚ï¼šä»input_idså¼€å§‹
    if self.is_first_partition:
        if input_ids is None:
            raise ValueError("ç¬¬ä¸€ä¸ªåˆ†å±‚å¿…é¡»æä¾›input_ids")
        hidden_states = self.embed_tokens(input_ids)
        batch_size, seq_length = input_ids.shape
    else:
        # åç»­åˆ†å±‚ï¼šä½¿ç”¨ä¼ å…¥çš„hidden_states
        if hidden_states is None:
            raise ValueError("éç¬¬ä¸€ä¸ªåˆ†å±‚å¿…é¡»æä¾›hidden_states")
        batch_size, seq_length, _ = hidden_states.shape
    
    # å¤„ç†KV-cacheç›¸å…³é€»è¾‘
    past_length = 0
    if past_key_values is not None and len(past_key_values) > 0:
        for pkv in past_key_values:
            if pkv is not None and len(pkv) > 0:
                past_length = pkv[0].shape[2]
                break
    
    # å‡†å¤‡æ³¨æ„åŠ›æ©ç ï¼ˆå…³é”®ä¿®å¤ç‚¹ï¼‰
    if attention_mask is None:
        total_length = past_length + seq_length
        attention_mask = torch.ones(
            (batch_size, total_length), 
            dtype=torch.bool, 
            device=hidden_states.device
        )
    else:
        # ç¡®ä¿attention_maskè¦†ç›–å®Œæ•´åºåˆ—
        if past_length > 0 and attention_mask.shape[1] != (past_length + seq_length):
            if attention_mask.shape[1] == seq_length:
                past_mask = torch.ones(
                    (batch_size, past_length), 
                    dtype=attention_mask.dtype,
                    device=attention_mask.device
                )
                attention_mask = torch.cat([past_mask, attention_mask], dim=1)
    
    # å‡†å¤‡ä½ç½®ID
    if position_ids is None:
        position_ids = torch.arange(
            past_length, 
            seq_length + past_length, 
            dtype=torch.long, 
            device=hidden_states.device
        ).unsqueeze(0).expand(batch_size, -1)
    
    # å‡†å¤‡å› æœæ©ç 
    causal_mask = self._prepare_decoder_attention_mask(
        attention_mask, 
        (batch_size, seq_length), 
        hidden_states, 
        past_length
    )
    
    # é€šè¿‡è§£ç å™¨å±‚
    next_decoder_cache = () if use_cache else None
    
    for idx, decoder_layer in enumerate(self.layers):
        # è·å–å¯¹åº”çš„KVç¼“å­˜ï¼ˆå…³é”®ä¿®å¤ç‚¹ï¼‰
        past_key_value = None
        if past_key_values is not None:
            local_layer_idx = idx  # ä½¿ç”¨æœ¬åœ°å±‚ç´¢å¼•
            if local_layer_idx < len(past_key_values):
                past_key_value = past_key_values[local_layer_idx]
        
        layer_outputs = decoder_layer(
            hidden_states,
            attention_mask=causal_mask,
            position_ids=position_ids,
            past_key_value=past_key_value,
            output_attentions=output_attentions,
            use_cache=use_cache,
        )
        
        hidden_states = layer_outputs[0]
        
        if use_cache:
            next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)
    
    # æœ€åä¸€ä¸ªåˆ†å±‚ï¼šåº”ç”¨å½’ä¸€åŒ–å’Œè¯­è¨€æ¨¡å‹å¤´
    logits = None
    if self.is_last_partition:
        hidden_states = self.norm(hidden_states)
        logits = self.lm_head(hidden_states)
    
    return {
        'hidden_states': hidden_states,
        'logits': logits,
        'past_key_values': next_decoder_cache if use_cache else None,
        'partition_idx': self.partition_idx,
        'is_last_partition': self.is_last_partition
    }
```

### 4. åˆ†å¸ƒå¼æ¨ç†åè°ƒå™¨ (`inference/coordinator.py`)

#### æ ¸å¿ƒæ¨ç†åè°ƒç±»

```python
class DistributedInference:
    """åˆ†å¸ƒå¼æ¨ç†å¼•æ“ - åè°ƒå¤šä¸ªå­æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹"""
    
    def __init__(
        self,
        submodels: List[LlamaSubModel],
        generation_config: Optional[GenerationConfig] = None,
        enable_async: bool = False,
        max_workers: int = 4
    ):
        self.submodels = submodels
        self.generation_config = generation_config or GenerationConfig()
        
        # éªŒè¯å­æ¨¡å‹
        self._validate_submodels()
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            'total_inference_time': 0.0,
            'token_generation_time': 0.0,
            'state_transfer_time': 0.0,
            'cache_management_time': 0.0,
            'total_tokens_generated': 0,
            'inference_count': 0
        }
```

#### åˆ†å¸ƒå¼å‰å‘ä¼ æ’­

```python
def forward_pass(
    self,
    input_ids: torch.Tensor,
    attention_mask: Optional[torch.Tensor] = None,
    past_key_values: Optional[List[torch.FloatTensor]] = None,
    use_cache: bool = True
) -> Dict[str, Any]:
    """æ‰§è¡Œåˆ†å¸ƒå¼å‰å‘ä¼ æ’­"""
    
    start_time = time.time()
    
    # åˆå§‹åŒ–æ¨ç†çŠ¶æ€
    inference_state = InferenceState(
        input_ids=input_ids,
        attention_mask=attention_mask,
        past_key_values=past_key_values,
        hidden_states=None,
        position_ids=None
    )
    
    # ä¾æ¬¡é€šè¿‡æ¯ä¸ªå­æ¨¡å‹è¿›è¡Œæ¨ç†
    result = None
    for i, submodel in enumerate(self.submodels):
        transfer_start = time.time()
        
        if submodel.is_first_partition:
            # ç¬¬ä¸€ä¸ªåˆ†å±‚å¤„ç†
            current_input_ids = input_ids
            current_attention_mask = attention_mask
            
            if past_key_values is not None:
                # æœ‰KV-cacheæ—¶ï¼Œåªä½¿ç”¨æœ€åä¸€ä¸ªtoken
                current_input_ids = input_ids[:, -1:]
                
                # é‡æ–°æ„å»ºå®Œæ•´çš„attention_mask
                if attention_mask is not None:
                    batch_size = input_ids.shape[0]
                    seq_length = current_input_ids.shape[1]
                    
                    # è®¡ç®—è¿‡å»åºåˆ—é•¿åº¦
                    past_length = 0
                    if past_key_values is not None:
                        for pkv in past_key_values:
                            if pkv is not None and len(pkv) > 0 and pkv[0] is not None:
                                past_length = pkv[0].shape[2]
                                break
                    
                    total_length = past_length + seq_length
                    current_attention_mask = torch.ones(
                        (batch_size, total_length),
                        dtype=attention_mask.dtype,
                        device=attention_mask.device
                    )
            
            model_input = {
                'input_ids': current_input_ids.to(submodel.get_info()['device']),
                'attention_mask': current_attention_mask.to(submodel.get_info()['device']) if current_attention_mask is not None else None,
                'past_key_values': self._extract_relevant_kv_cache(
                    past_key_values, 
                    submodel.layer_start, 
                    submodel.layer_end, 
                    submodel.get_info()['device']
                ),
                'use_cache': use_cache
            }
        else:
            # åç»­åˆ†å±‚å¤„ç†
            if inference_state.hidden_states is None:
                raise RuntimeError(f"å­æ¨¡å‹ {i} éœ€è¦hidden_statesï¼Œä½†å‰ä¸€ä¸ªå­æ¨¡å‹æ²¡æœ‰æä¾›")
            
            model_input = {
                'hidden_states': inference_state.hidden_states.to(submodel.get_info()['device']),
                'attention_mask': inference_state.attention_mask.to(submodel.get_info()['device']) if inference_state.attention_mask is not None else None,
                'past_key_values': self._extract_relevant_kv_cache(
                    inference_state.past_key_values, 
                    submodel.layer_start, 
                    submodel.layer_end, 
                    submodel.get_info()['device']
                ),
                'use_cache': use_cache
            }
        
        self.stats['state_transfer_time'] += time.time() - transfer_start
        
        # æ‰§è¡Œæ¨ç†
        with torch.no_grad():
            result = submodel(**model_input)
        
        # æ›´æ–°æ¨ç†çŠ¶æ€
        inference_state.hidden_states = result['hidden_states']
        if use_cache and result['past_key_values'] is not None:
            inference_state.past_key_values = self._merge_kv_cache(
                inference_state.past_key_values, 
                result['past_key_values'],
                submodel.layer_start,
                submodel.layer_end
            )
        
        # æ›´æ–°attention_mask
        if submodel.is_first_partition:
            inference_state.attention_mask = current_attention_mask
    
    # ç»Ÿè®¡æ›´æ–°
    self.stats['total_inference_time'] += time.time() - start_time
    self.stats['inference_count'] += 1
    
    return {
        'logits': result['logits'],
        'hidden_states': inference_state.hidden_states,
        'past_key_values': inference_state.past_key_values if use_cache else None
    }
```

#### KV-Cacheç®¡ç†

```python
def _extract_relevant_kv_cache(
    self,
    past_key_values: Optional[List[torch.FloatTensor]],
    layer_start: int,
    layer_end: int,
    target_device: str
) -> Optional[List[torch.FloatTensor]]:
    """æå–ä¸å½“å‰å­æ¨¡å‹ç›¸å…³çš„KVç¼“å­˜"""
    
    if past_key_values is None:
        return None
    
    relevant_cache = []
    num_layers_in_submodel = layer_end - layer_start + 1
    
    for local_idx in range(num_layers_in_submodel):
        global_layer_idx = layer_start + local_idx
        if global_layer_idx < len(past_key_values) and past_key_values[global_layer_idx] is not None:
            key_cache, value_cache = past_key_values[global_layer_idx]
            relevant_cache.append((
                key_cache.to(target_device),
                value_cache.to(target_device)
            ))
        else:
            relevant_cache.append(None)
    
    return relevant_cache

def _merge_kv_cache(
    self,
    global_cache: Optional[List[torch.FloatTensor]],
    local_cache: Optional[List[torch.FloatTensor]],
    layer_start: int,
    layer_end: int
) -> Optional[List[torch.FloatTensor]]:
    """åˆå¹¶å±€éƒ¨KVç¼“å­˜åˆ°å…¨å±€ç¼“å­˜"""
    
    if local_cache is None:
        return global_cache
    
    # åˆå§‹åŒ–å…¨å±€ç¼“å­˜
    if global_cache is None:
        total_layers = max(sm.layer_end for sm in self.submodels) + 1
        global_cache = [None] * total_layers
    
    # æ›´æ–°å¯¹åº”å±‚çš„ç¼“å­˜
    num_layers_in_submodel = layer_end - layer_start + 1
    
    for local_idx, layer_cache in enumerate(local_cache):
        if local_idx < num_layers_in_submodel:
            global_layer_idx = layer_start + local_idx
            if global_layer_idx < len(global_cache):
                global_cache[global_layer_idx] = layer_cache
    
    return global_cache
```

## ğŸš€ åˆ†å±‚æ¨ç†æ€»ä½“æµç¨‹

### æ•´ä½“æµç¨‹å›¾

```
è¾“å…¥æ–‡æœ¬
    â†“
[1] åˆ†è¯ç¼–ç 
    â†“
[2] æ¨¡å‹åˆ†å±‚
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [3] åˆ†å¸ƒå¼æ¨ç†å¾ªç¯                                           â”‚
â”‚                                                             â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ [3.1] å‡†å¤‡å½“å‰è¾“å…¥                                       â”‚ â”‚
â”‚ â”‚   - ç¬¬ä¸€æ­¥ï¼šå®Œæ•´åºåˆ—                                     â”‚ â”‚
â”‚ â”‚   - åç»­æ­¥ï¼šæœ€åä¸€ä¸ªtoken (if KV-cache)                 â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                         â†“                                   â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ [3.2] å­æ¨¡å‹æ¨ç†é“¾                                       â”‚ â”‚
â”‚ â”‚                                                         â”‚ â”‚
â”‚ â”‚   å­æ¨¡å‹0 (GPU:0)  â†’  å­æ¨¡å‹1 (GPU:1)  â†’  ...          â”‚ â”‚
â”‚ â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚ â”‚
â”‚ â”‚   â”‚Embed+Layers â”‚â”€â”€â”€â”€â–¶â”‚   Layers    â”‚â”€â”€â”€â”€â–¶              â”‚ â”‚
â”‚ â”‚   â”‚  0-15       â”‚     â”‚   16-31     â”‚                   â”‚ â”‚
â”‚ â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚ â”‚
â”‚ â”‚        â†“                     â†“                          â”‚ â”‚
â”‚ â”‚   Hidden States      Hidden States                      â”‚ â”‚
â”‚ â”‚   KV-Cache          KV-Cache                            â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                         â†“                                   â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ [3.3] æœ€ç»ˆè¾“å‡º                                           â”‚ â”‚
â”‚ â”‚   - æœ€ååˆ†å±‚: Norm + LM Head â†’ Logits                   â”‚ â”‚
â”‚ â”‚   - é‡‡æ ·: Top-k/Top-p/Temperature                       â”‚ â”‚
â”‚ â”‚   - ç”Ÿæˆ: Next Token                                    â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                         â†“                                   â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ [3.4] çŠ¶æ€æ›´æ–°                                           â”‚ â”‚
â”‚ â”‚   - æ›´æ–°ç”Ÿæˆåºåˆ—                                         â”‚ â”‚
â”‚ â”‚   - åˆå¹¶KV-Cache                                        â”‚ â”‚
â”‚ â”‚   - æ£€æŸ¥åœæ­¢æ¡ä»¶                                         â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
è§£ç è¾“å‡ºæ–‡æœ¬
```

### è¯¦ç»†æµç¨‹æè¿°

#### 1. åˆå§‹åŒ–é˜¶æ®µ

```python
# åˆ›å»ºåˆ†å±‚ç­–ç•¥
strategy = PartitionStrategy(
    num_partitions=2,
    strategy_type="uniform",
    target_devices=["cuda:0", "cuda:1"]
)

# åˆ†å±‚æ¨¡å‹
partitioner = LlamaPartitioner(model_path=model_path)
submodels = partitioner.partition(strategy=strategy, copy_weights=True)

# åˆ›å»ºåˆ†å¸ƒå¼æ¨ç†å¼•æ“
inference_engine = DistributedInference(
    submodels=submodels,
    generation_config=GenerationConfig(use_cache=True)
)
```

#### 2. æ¨ç†æµç¨‹

```python
def generate_text_step_by_step(prompt: str, max_tokens: int = 50):
    """é€æ­¥å±•ç¤ºåˆ†å¸ƒå¼æ¨ç†æµç¨‹"""
    
    # [1] è¾“å…¥é¢„å¤„ç†
    input_ids = tokenizer.encode(prompt, return_tensors='pt')
    generated_ids = input_ids.clone()
    past_key_values = None
    
    print(f"ğŸ¯ å¼€å§‹ç”Ÿæˆï¼Œè¾“å…¥é•¿åº¦: {input_ids.shape[1]}")
    
    for step in range(max_tokens):
        print(f"\nğŸ“ ç”Ÿæˆæ­¥éª¤ {step + 1}")
        
        # [2] å‡†å¤‡å½“å‰è¾“å…¥
        if step == 0:
            current_input = generated_ids
            print(f"   è¾“å…¥: å®Œæ•´åºåˆ— {current_input.shape}")
        else:
            current_input = generated_ids[:, -1:]
            print(f"   è¾“å…¥: æœ€åtoken {current_input.shape} (KV-cacheå¯ç”¨)")
        
        # [3] åˆ†å¸ƒå¼å‰å‘ä¼ æ’­
        print(f"   ğŸ”„ å¼€å§‹åˆ†å¸ƒå¼æ¨ç†...")
        
        result = inference_engine.forward_pass(
            input_ids=current_input,
            past_key_values=past_key_values,
            use_cache=True
        )
        
        # [4] é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
        next_token_logits = result['logits'][0, -1, :]
        next_token = torch.argmax(next_token_logits, dim=-1)
        
        # [5] æ›´æ–°çŠ¶æ€
        generated_ids = torch.cat([generated_ids, next_token.unsqueeze(0).unsqueeze(0)], dim=1)
        past_key_values = result['past_key_values']
        
        # [6] è§£ç å¹¶æ˜¾ç¤º
        token_text = tokenizer.decode([next_token.item()], skip_special_tokens=True)
        print(f"   ç”Ÿæˆtoken: '{token_text}' (åºåˆ—é•¿åº¦: {generated_ids.shape[1]})")
        
        # [7] æ£€æŸ¥åœæ­¢æ¡ä»¶
        if next_token.item() == tokenizer.eos_token_id:
            print(f"   ğŸ›‘ é‡åˆ°EOSï¼Œåœæ­¢ç”Ÿæˆ")
            break
    
    # [8] æœ€ç»ˆè¾“å‡º
    final_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
    print(f"\nâœ… ç”Ÿæˆå®Œæˆï¼")
    print(f"ğŸ“„ æœ€ç»ˆæ–‡æœ¬: {final_text}")
    
    return final_text
```

#### 3. å­æ¨¡å‹åè°ƒæœºåˆ¶

```python
def detailed_submodel_coordination(self, input_data):
    """è¯¦ç»†å±•ç¤ºå­æ¨¡å‹åè°ƒè¿‡ç¨‹"""
    
    print("ğŸ”€ å­æ¨¡å‹åè°ƒå¼€å§‹")
    
    inference_state = InferenceState()
    
    for i, submodel in enumerate(self.submodels):
        print(f"\nğŸ“¡ å­æ¨¡å‹ {i} (è®¾å¤‡: {submodel.get_info()['device']})")
        print(f"   å±‚èŒƒå›´: {submodel.layer_start}-{submodel.layer_end}")
        
        # 1. å‡†å¤‡è¾“å…¥
        if submodel.is_first_partition:
            print("   ğŸ¯ ç¬¬ä¸€åˆ†å±‚: ä½¿ç”¨input_ids")
            model_input = self._prepare_first_partition_input(input_data, inference_state)
        else:
            print("   ğŸ”— ä¸­é—´åˆ†å±‚: ä½¿ç”¨hidden_states")
            model_input = self._prepare_middle_partition_input(inference_state, submodel)
        
        # 2. æ•°æ®ä¼ è¾“
        print(f"   ğŸ“¤ æ•°æ®ä¼ è¾“åˆ° {submodel.get_info()['device']}")
        start_transfer = time.time()
        for key, value in model_input.items():
            if isinstance(value, torch.Tensor):
                model_input[key] = value.to(submodel.get_info()['device'])
        transfer_time = time.time() - start_transfer
        print(f"   â±ï¸  ä¼ è¾“è€—æ—¶: {transfer_time:.3f}s")
        
        # 3. å­æ¨¡å‹æ¨ç†
        print(f"   ğŸ§  å¼€å§‹æ¨ç†...")
        start_inference = time.time()
        with torch.no_grad():
            result = submodel(**model_input)
        inference_time = time.time() - start_inference
        print(f"   â±ï¸  æ¨ç†è€—æ—¶: {inference_time:.3f}s")
        
        # 4. çŠ¶æ€æ›´æ–°
        print(f"   ğŸ”„ æ›´æ–°æ¨ç†çŠ¶æ€")
        inference_state.hidden_states = result['hidden_states']
        if result['past_key_values'] is not None:
            inference_state.past_key_values = self._merge_kv_cache(
                inference_state.past_key_values,
                result['past_key_values'],
                submodel.layer_start,
                submodel.layer_end
            )
        
        # 5. è¾“å‡ºä¿¡æ¯
        if submodel.is_last_partition:
            print("   ğŸ¯ æœ€ååˆ†å±‚: äº§ç”Ÿlogits")
            print(f"   ğŸ“Š Logitså½¢çŠ¶: {result['logits'].shape}")
        else:
            print(f"   ğŸ“Š Hidden stateså½¢çŠ¶: {result['hidden_states'].shape}")
    
    print("\nâœ… å­æ¨¡å‹åè°ƒå®Œæˆ")
    return result
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–ä¸æµ‹è¯•ç»“æœ

### KV-Cacheä¼˜åŒ–æˆæœ

#### æ€§èƒ½å¯¹æ¯”æµ‹è¯•ç»“æœ

```
ğŸ“Š KV-Cacheæ€§èƒ½æµ‹è¯•ç»“æœ
============================================================
æµ‹è¯•é…ç½®:
- æ¨¡å‹: Llama-2-7B
- åˆ†å±‚: 2ä¸ªå­æ¨¡å‹ (GPU:0 + GPU:1)
- åºåˆ—é•¿åº¦: 20 tokens

æ€§èƒ½å¯¹æ¯”:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    æŒ‡æ ‡         â”‚   æ— KV-Cache â”‚   æœ‰KV-Cache â”‚   æ€§èƒ½æå‡   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ€»ç”Ÿæˆæ—¶é—´      â”‚    1.55s     â”‚    0.84s     â”‚    1.8x      â”‚
â”‚ å¹³å‡æ¯æ­¥æ—¶é—´    â”‚    0.045s    â”‚    0.015s    â”‚    3.0x      â”‚
â”‚ é¦–tokenå»¶è¿Ÿ    â”‚    0.411s    â”‚    0.015s    â”‚   27.4x      â”‚
â”‚ åç»­tokenç¨³å®šæ€§ â”‚   é€’å¢è¶‹åŠ¿    â”‚   ä¿æŒç¨³å®š    â”‚     âœ…       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

é•¿åºåˆ—ç¨³å®šæ€§åˆ†æ (50 tokens):
- æ—©æœŸå¹³å‡æ—¶é—´: 0.019s
- åæœŸå¹³å‡æ—¶é—´: 0.014s  
- æ—¶é—´å¢é•¿æ¯”ç‡: 0.73x âœ… ä¿æŒç¨³å®š
- ååé‡: 19.8 tokens/ç§’
```

#### å…³é”®ä¿®å¤ç‚¹

1. **æ³¨æ„åŠ›æ©ç ç»´åº¦ä¿®å¤**:
   ```python
   # ä¿®å¤å‰: æ©ç ç»´åº¦ä¸åŒ¹é…
   # Error: Attention mask should be of size (1, 1, 1, 1), but is torch.Size([1, 1, 1, 11])
   
   # ä¿®å¤å: æ­£ç¡®å¤„ç†KV-cacheæ—¶çš„æ©ç ç»´åº¦
   if past_length > 0 and attention_mask.shape[1] != (past_length + seq_length):
       if attention_mask.shape[1] == seq_length:
           past_mask = torch.ones((batch_size, past_length), ...)
           attention_mask = torch.cat([past_mask, attention_mask], dim=1)
   ```

2. **KV-cacheç´¢å¼•æ˜ å°„ä¿®å¤**:
   ```python
   # ä¿®å¤å‰: ä½¿ç”¨é”™è¯¯çš„å…¨å±€ç´¢å¼•
   global_layer_idx = self.layer_start + idx
   
   # ä¿®å¤å: ä½¿ç”¨æ­£ç¡®çš„æœ¬åœ°ç´¢å¼•
   local_layer_idx = idx  # ç›´æ¥ä½¿ç”¨æœ¬åœ°å±‚ç´¢å¼•
   ```

### ç³»ç»Ÿæ•´ä½“æ€§èƒ½

```
ğŸš€ ç³»ç»Ÿæ•´ä½“æ€§èƒ½æµ‹è¯•
============================================================
æµ‹è¯•ç¯å¢ƒ:
- ç¡¬ä»¶: 2x NVIDIA GPU + CPU
- æ¨¡å‹: Llama-2-7B (32å±‚ï¼Œ7Bå‚æ•°)
- åˆ†å±‚ç­–ç•¥: å‡åŒ€åˆ†å±‚ (16å±‚/åˆ†å±‚)

æ€§èƒ½æŒ‡æ ‡:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æŒ‡æ ‡                â”‚ æµ‹è¯•ç»“æœ        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨¡å‹åŠ è½½æ—¶é—´        â”‚ ~3.5s          â”‚
â”‚ åˆ†å±‚å¤„ç†æ—¶é—´        â”‚ ~1.2s          â”‚
â”‚ æƒé‡å¤åˆ¶æ—¶é—´        â”‚ ~2.8s          â”‚
â”‚ æ¨ç†åˆå§‹åŒ–æ—¶é—´      â”‚ ~0.5s          â”‚
â”‚ å•tokenç”Ÿæˆæ—¶é—´     â”‚ 0.014-0.019s   â”‚
â”‚ å†…å­˜ä½¿ç”¨(æ¯GPU)     â”‚ ~4.5GB         â”‚
â”‚ è·¨è®¾å¤‡ä¼ è¾“å»¶è¿Ÿ      â”‚ <0.001s        â”‚
â”‚ KV-cacheæœ‰æ•ˆæ€§      â”‚ âœ… å®Œå…¨æ­£å¸¸     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

è´¨é‡æŒ‡æ ‡:
- ç”Ÿæˆæ–‡æœ¬è¿è´¯æ€§: âœ… ä¼˜ç§€
- åˆ†å±‚ä¸€è‡´æ€§: âœ… å®Œå…¨ä¸€è‡´  
- é•¿åºåˆ—ç¨³å®šæ€§: âœ… ç¨³å®š
- é”™è¯¯ç‡: 0% (æ— æ¨ç†é”™è¯¯)
```

## ğŸ” æŠ€æœ¯éš¾ç‚¹ä¸è§£å†³æ–¹æ¡ˆ

### 1. KV-Cacheåœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­çš„æŒ‘æˆ˜

**é—®é¢˜**: KV-Cacheæœºåˆ¶è¦æ±‚ç²¾ç¡®çš„åºåˆ—é•¿åº¦å’Œç´¢å¼•ç®¡ç†ï¼Œåœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­å®¹æ˜“å‡ºç°ç»´åº¦ä¸åŒ¹é…å’Œç´¢å¼•é”™è¯¯ã€‚

**è§£å†³æ–¹æ¡ˆ**:
- å®ç°äº†ç²¾ç¡®çš„æ³¨æ„åŠ›æ©ç ç»´åº¦ç®¡ç†
- å»ºç«‹äº†æ¸…æ™°çš„å…¨å±€-å±€éƒ¨ç´¢å¼•æ˜ å°„æœºåˆ¶
- å®Œå–„äº†KV-cacheçš„æå–ã€ä¼ é€’å’Œåˆå¹¶é€»è¾‘

### 2. è·¨è®¾å¤‡çŠ¶æ€ä¼ é€’ä¼˜åŒ–

**é—®é¢˜**: å¤§å¼ é‡åœ¨è®¾å¤‡é—´ä¼ è¾“ä¼šé€ æˆæ€§èƒ½ç“¶é¢ˆã€‚

**è§£å†³æ–¹æ¡ˆ**:
- åªä¼ é€’å¿…è¦çš„éšè—çŠ¶æ€ï¼Œä¸ä¼ é€’ä¸­é—´è®¡ç®—ç»“æœ
- å®ç°äº†æ™ºèƒ½çš„KV-cacheåˆ†ç‰‡å’Œé‡ç»„
- ä¼˜åŒ–äº†è®¾å¤‡é—´æ•°æ®ä¼ è¾“æ—¶æœº

### 3. å†…å­˜ç®¡ç†ä¸è´Ÿè½½å‡è¡¡

**é—®é¢˜**: ä¸åŒåˆ†å±‚çš„å†…å­˜å’Œè®¡ç®—è´Ÿè½½å¯èƒ½ä¸å‡è¡¡ã€‚

**è§£å†³æ–¹æ¡ˆ**:
- å®ç°äº†å¤šç§åˆ†å±‚ç­–ç•¥(å‡åŒ€ã€å†…å­˜ã€è®¡ç®—ã€æ··åˆ)
- æä¾›äº†çµæ´»çš„è®¾å¤‡é…ç½®æœºåˆ¶
- æ”¯æŒåŠ¨æ€è´Ÿè½½ç›‘æ§å’Œè°ƒæ•´

## ğŸ“ˆ åº”ç”¨åœºæ™¯ä¸æ‰©å±•æ€§

### å½“å‰æ”¯æŒçš„åº”ç”¨åœºæ™¯

1. **äº¤äº’å¼å¯¹è¯**: å®æ—¶é—®ç­”å’Œå¯¹è¯ç³»ç»Ÿ
2. **æ–‡æœ¬ç”Ÿæˆ**: é•¿æ–‡æœ¬åˆ›ä½œå’Œå†…å®¹ç”Ÿæˆ
3. **æ‰¹é‡æ¨ç†**: å¤§è§„æ¨¡æ–‡æœ¬å¤„ç†ä»»åŠ¡
4. **ç ”ç©¶å®éªŒ**: æ¨¡å‹åˆ†å±‚å’Œåˆ†å¸ƒå¼æ¨ç†ç ”ç©¶

### ç³»ç»Ÿæ‰©å±•æ€§

1. **æ¨¡å‹æ”¯æŒ**: 
   - å½“å‰: Llamaç³»åˆ—æ¨¡å‹
   - æ‰©å±•: å¯é€‚é…å…¶ä»–Transformeræ¶æ„

2. **è®¾å¤‡æ”¯æŒ**:
   - å½“å‰: NVIDIA GPU + CPU
   - æ‰©å±•: AMD GPUã€TPUç­‰å…¶ä»–åŠ é€Ÿå™¨

3. **åˆ†å±‚ç­–ç•¥**:
   - å½“å‰: 5ç§é¢„å®šä¹‰ç­–ç•¥
   - æ‰©å±•: è‡ªå®šä¹‰ç­–ç•¥API

4. **ä¼˜åŒ–æŠ€æœ¯**:
   - å½“å‰: KV-Cacheã€FP16
   - æ‰©å±•: é‡åŒ–ã€ç¨€ç–åŒ–ç­‰

## ğŸ“‹ æ€»ç»“

LlamaDistributoræˆåŠŸå®ç°äº†é«˜æ•ˆçš„åˆ†å±‚åˆ†å¸ƒå¼æ¨ç†ç³»ç»Ÿï¼Œä¸»è¦æˆæœåŒ…æ‹¬ï¼š

### ğŸ¯ æ ¸å¿ƒæˆå°±

1. **å®Œæ•´çš„åˆ†å±‚æ¡†æ¶**: å®ç°äº†ä»åˆ†æã€åˆ†å±‚åˆ°æ¨ç†çš„å®Œæ•´æµç¨‹
2. **é«˜æ•ˆçš„KV-Cache**: è§£å†³äº†åˆ†å¸ƒå¼ç¯å¢ƒä¸­KV-cacheçš„å…³é”®æŠ€æœ¯éš¾é¢˜
3. **çµæ´»çš„é…ç½®**: æ”¯æŒå¤šç§åˆ†å±‚ç­–ç•¥å’Œè®¾å¤‡é…ç½®
4. **ä¼˜ç§€çš„æ€§èƒ½**: å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡(1.8xæ€»ä½“ï¼Œ3.0xå•æ­¥)
5. **ç¨³å®šçš„è´¨é‡**: ç¡®ä¿äº†åˆ†å¸ƒå¼æ¨ç†ç»“æœçš„ä¸€è‡´æ€§å’Œå¯é æ€§

### ğŸš€ æŠ€æœ¯åˆ›æ–°

1. **æ™ºèƒ½ç´¢å¼•æ˜ å°„**: åˆ›æ–°æ€§åœ°è§£å†³äº†å…¨å±€-å±€éƒ¨ç´¢å¼•è½¬æ¢é—®é¢˜
2. **çŠ¶æ€ä¼ é€’ä¼˜åŒ–**: å®ç°äº†é«˜æ•ˆçš„è·¨è®¾å¤‡çŠ¶æ€ä¼ é€’æœºåˆ¶
3. **æ³¨æ„åŠ›æ©ç ç®¡ç†**: ç²¾ç¡®å¤„ç†äº†KV-cacheåœºæ™¯ä¸‹çš„æ©ç ç»´åº¦é—®é¢˜
4. **æ¨¡å—åŒ–è®¾è®¡**: å»ºç«‹äº†é«˜åº¦æ¨¡å—åŒ–å’Œå¯æ‰©å±•çš„ç³»ç»Ÿæ¶æ„

### ğŸ“Š éªŒè¯æˆæœ

é€šè¿‡å…¨é¢çš„æµ‹è¯•éªŒè¯ï¼Œç³»ç»Ÿåœ¨æ€§èƒ½ã€ç¨³å®šæ€§ã€ä¸€è‡´æ€§ç­‰æ–¹é¢éƒ½è¾¾åˆ°äº†é¢„æœŸç›®æ ‡ï¼Œä¸ºå¤§æ¨¡å‹çš„åˆ†å¸ƒå¼éƒ¨ç½²å’Œæ¨ç†æä¾›äº†å¯é çš„è§£å†³æ–¹æ¡ˆã€‚

è¿™ä¸ªç³»ç»Ÿä¸ä»…è§£å†³äº†å½“å‰å¤§æ¨¡å‹éƒ¨ç½²çš„å®é™…é—®é¢˜ï¼Œä¹Ÿä¸ºæœªæ¥æ›´å¤§è§„æ¨¡æ¨¡å‹çš„åˆ†å¸ƒå¼æ¨ç†å¥ å®šäº†æŠ€æœ¯åŸºç¡€ã€‚ 