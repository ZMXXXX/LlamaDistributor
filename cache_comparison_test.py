#!/usr/bin/env python3
"""
KVç¼“å­˜æ€§èƒ½å¯¹æ¯”æµ‹è¯•

å¯¹æ¯”ä½¿ç”¨å’Œä¸ä½¿ç”¨KVç¼“å­˜çš„æ€§èƒ½å·®å¼‚
"""

import torch
import time
from transformers import AutoTokenizer

from llamadist.partitioner.strategies import PartitionStrategy
from llamadist.partitioner.splitter import LlamaPartitioner
from llamadist.inference.coordinator import DistributedInference, GenerationConfig

def benchmark_cache_vs_no_cache(model_path: str = "/home/zmx/models/Llama/Llama-2-7b-hf"):
    """å¯¹æ¯”ä½¿ç”¨å’Œä¸ä½¿ç”¨KVç¼“å­˜çš„æ€§èƒ½"""
    
    print("ğŸ”¥ KVç¼“å­˜æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    print("=" * 60)
    
    # é…ç½®
    devices = ["cuda:0", "cuda:1"] if torch.cuda.device_count() >= 2 else ["cpu", "cpu"]
    
    # åˆ›å»ºåˆ†å±‚ç­–ç•¥
    strategy = PartitionStrategy(
        num_partitions=2,
        strategy_type="uniform", 
        target_devices=devices
    )
    
    # åˆ†å±‚æ¨¡å‹
    partitioner = LlamaPartitioner(model_path=model_path)
    submodels = partitioner.partition(strategy=strategy, analyze_first=False, copy_weights=True)
    
    # åŠ è½½åˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # æµ‹è¯•æç¤º
    prompt = "The benefits of using cache in language models include"
    input_ids = tokenizer.encode(prompt, return_tensors='pt')
    first_device = submodels[0].get_info()['device'] 
    input_ids = input_ids.to(first_device)
    
    print(f"ğŸ“ æµ‹è¯•æç¤º: {prompt}")
    print(f"ğŸ’» ä½¿ç”¨è®¾å¤‡: {devices}")
    
    # æµ‹è¯•å‚æ•°
    max_tokens = 20  # å‡å°‘tokenæ•°é‡ä»¥åŠ å¿«æµ‹è¯•
    
    # === æµ‹è¯•1: ä¸ä½¿ç”¨KVç¼“å­˜ ===
    print("\nğŸŒ æµ‹è¯•1: ä¸ä½¿ç”¨KVç¼“å­˜")
    inference_engine_no_cache = DistributedInference(
        submodels=submodels,
        generation_config=GenerationConfig(use_cache=False)
    )
    
    times_no_cache = []
    sequence_lengths = []
    all_input_ids = input_ids.clone()
    
    start_time = time.time()
    for step in range(max_tokens):
        step_start = time.time()
        
        # æ¯æ¬¡ä¼ é€’å®Œæ•´åºåˆ—
        result = inference_engine_no_cache.forward_pass(
            input_ids=all_input_ids,
            past_key_values=None,
            use_cache=False
        )
        
        # ç®€å•è´ªå¿ƒé‡‡æ ·
        next_token = torch.argmax(result['logits'][0, -1, :], dim=-1)
        # ç¡®ä¿next_tokenåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        next_token = next_token.to(first_device)
        all_input_ids = torch.cat([all_input_ids, next_token.unsqueeze(0).unsqueeze(0)], dim=1)
        
        step_time = time.time() - step_start
        times_no_cache.append(step_time)
        sequence_lengths.append(all_input_ids.shape[1])
        
        if step % 3 == 0:
            print(f"   æ­¥éª¤ {step}: {step_time:.3f}s (åºåˆ—é•¿åº¦: {all_input_ids.shape[1]})")
        
        if next_token.item() == tokenizer.eos_token_id:
            break
    
    total_time_no_cache = time.time() - start_time
    
    # === æµ‹è¯•2: ä½¿ç”¨KVç¼“å­˜ ===
    print("\nğŸš€ æµ‹è¯•2: ä½¿ç”¨KVç¼“å­˜")
    inference_engine_with_cache = DistributedInference(
        submodels=submodels,
        generation_config=GenerationConfig(use_cache=True)
    )
    
    times_with_cache = []
    generated_ids = input_ids.clone()
    past_key_values = None
    
    start_time = time.time()
    for step in range(max_tokens):
        step_start = time.time()
        
        # ç¬¬ä¸€æ­¥ä¼ é€’å®Œæ•´åºåˆ—ï¼Œåç»­åªä¼ é€’æœ€åä¸€ä¸ªtoken
        current_input = generated_ids if step == 0 else generated_ids[:, -1:]
        
        result = inference_engine_with_cache.forward_pass(
            input_ids=current_input,
            past_key_values=past_key_values,
            use_cache=True
        )
        
        # ç®€å•è´ªå¿ƒé‡‡æ ·
        next_token = torch.argmax(result['logits'][0, -1, :], dim=-1)
        # ç¡®ä¿next_tokenåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š 
        next_token = next_token.to(first_device)
        generated_ids = torch.cat([generated_ids, next_token.unsqueeze(0).unsqueeze(0)], dim=1)
        past_key_values = result['past_key_values']
        
        step_time = time.time() - step_start
        times_with_cache.append(step_time)
        
        if step % 3 == 0:
            print(f"   æ­¥éª¤ {step}: {step_time:.3f}s (åºåˆ—é•¿åº¦: {generated_ids.shape[1]})")
        
        if next_token.item() == tokenizer.eos_token_id:
            break
    
    total_time_with_cache = time.time() - start_time
    
    # === ç»“æœå¯¹æ¯” ===
    print("\nğŸ“Š æ€§èƒ½å¯¹æ¯”ç»“æœ")
    print("=" * 60)
    print(f"ä¸ä½¿ç”¨ç¼“å­˜æ€»æ—¶é—´: {total_time_no_cache:.2f}s")
    print(f"ä½¿ç”¨ç¼“å­˜æ€»æ—¶é—´:   {total_time_with_cache:.2f}s") 
    speedup = total_time_no_cache/total_time_with_cache if total_time_with_cache > 0 else 0
    print(f"æ€§èƒ½æå‡:         {speedup:.1f}x")
    
    # å¹³å‡æ¯æ­¥æ—¶é—´
    avg_time_no_cache = sum(times_no_cache) / len(times_no_cache) if times_no_cache else 0
    avg_time_with_cache = sum(times_with_cache) / len(times_with_cache) if times_with_cache else 0
    
    print(f"\nå¹³å‡æ¯æ­¥æ—¶é—´:")
    print(f"ä¸ä½¿ç”¨ç¼“å­˜: {avg_time_no_cache:.3f}s")
    print(f"ä½¿ç”¨ç¼“å­˜:   {avg_time_with_cache:.3f}s")
    if avg_time_with_cache > 0:
        print(f"æ¯æ­¥æå‡:   {avg_time_no_cache/avg_time_with_cache:.1f}x")
    
    # æœ€åå‡ æ­¥å¯¹æ¯”ï¼ˆæ˜¾ç¤ºäºŒæ¬¡å¢é•¿ï¼‰
    if len(times_no_cache) >= 5:
        print(f"\næ—¶é—´å¢é•¿è¶‹åŠ¿:")
        for i in range(min(5, len(times_no_cache))):
            no_cache_time = times_no_cache[i]
            with_cache_time = times_with_cache[i] if i < len(times_with_cache) else 0
            seq_len = sequence_lengths[i] if i < len(sequence_lengths) else 0
            ratio = no_cache_time / with_cache_time if with_cache_time > 0 else 0
            print(f"  æ­¥éª¤ {i}: æ— ç¼“å­˜={no_cache_time:.3f}s, æœ‰ç¼“å­˜={with_cache_time:.3f}s, æå‡={ratio:.1f}x, åºåˆ—é•¿åº¦={seq_len}")
    
    # ç”Ÿæˆçš„æ–‡æœ¬å¯¹æ¯”
    text_no_cache = tokenizer.decode(all_input_ids[0], skip_special_tokens=True)
    text_with_cache = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
    
    print(f"\nğŸ“„ ç”Ÿæˆæ–‡æœ¬å¯¹æ¯”:")
    print(f"æ— ç¼“å­˜ç»“æœ: {text_no_cache}")
    print(f"æœ‰ç¼“å­˜ç»“æœ: {text_with_cache}")
    
    return {
        'times_no_cache': times_no_cache,
        'times_with_cache': times_with_cache,
        'sequence_lengths': sequence_lengths,
        'total_time_no_cache': total_time_no_cache,
        'total_time_with_cache': total_time_with_cache
    }

def analyze_complexity_growth(results):
    """åˆ†æè®¡ç®—å¤æ‚åº¦å¢é•¿"""
    print("\nğŸ” è®¡ç®—å¤æ‚åº¦åˆ†æ")
    print("=" * 60)
    
    if len(results['times_no_cache']) < 3:
        print("æ•°æ®ç‚¹ä¸è¶³ï¼Œæ— æ³•åˆ†æ")
        return
    
    # åˆ†ææ— ç¼“å­˜æ—¶é—´å¢é•¿
    print("ä¸ä½¿ç”¨ç¼“å­˜çš„æ—¶é—´å¢é•¿æ¨¡å¼:")
    for i in range(1, min(len(results['times_no_cache']), 10)):
        current_time = results['times_no_cache'][i]
        previous_time = results['times_no_cache'][i-1]
        growth_ratio = current_time / previous_time if previous_time > 0 else 0
        seq_len = results['sequence_lengths'][i] if i < len(results['sequence_lengths']) else 0
        print(f"  æ­¥éª¤ {i}: {current_time:.3f}s (å¢é•¿ {growth_ratio:.2f}x, åºåˆ—é•¿åº¦ {seq_len})")
    
    # åˆ†ææœ‰ç¼“å­˜æ—¶é—´ç¨³å®šæ€§
    if results['times_with_cache']:
        print("\nä½¿ç”¨ç¼“å­˜çš„æ—¶é—´ç¨³å®šæ€§:")
        cache_times = results['times_with_cache']
        avg_cache_time = sum(cache_times) / len(cache_times)
        variance = sum((t - avg_cache_time) ** 2 for t in cache_times) / len(cache_times)
        std_dev = variance ** 0.5
        
        print(f"  å¹³å‡æ—¶é—´: {avg_cache_time:.3f}s")
        print(f"  æ ‡å‡†å·®:   {std_dev:.3f}s")
        print(f"  å˜å¼‚ç³»æ•°: {std_dev/avg_cache_time:.3f} (è¶Šå°è¶Šç¨³å®š)")

if __name__ == "__main__":
    try:
        results = benchmark_cache_vs_no_cache()
        analyze_complexity_growth(results)
        
        print("\n" + "=" * 60)
        print("ğŸ¯ æµ‹è¯•ç»“è®º:")
        print("1. KVç¼“å­˜æ˜¾è‘—æå‡æ€§èƒ½ï¼Œé¿å…é‡å¤è®¡ç®—")
        print("2. ä¸ä½¿ç”¨ç¼“å­˜æ—¶ï¼Œç”Ÿæˆæ—¶é—´éšåºåˆ—é•¿åº¦å¢é•¿è€Œå¢é•¿")
        print("3. åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­ï¼Œç¼“å­˜è¿˜èƒ½å‡å°‘è·¨è®¾å¤‡æ•°æ®ä¼ è¾“")
        print("4. å»ºè®®åœ¨ç”Ÿäº§ç¯å¢ƒä¸­æ€»æ˜¯å¯ç”¨KVç¼“å­˜")
        print("5. ç‰¹åˆ«æ˜¯ç”Ÿæˆé•¿æ–‡æœ¬æ—¶ï¼Œæ€§èƒ½å·®å¼‚ä¼šæ›´åŠ æ˜æ˜¾")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc() 