# LlamaDistributor å®‰è£…å’Œä½¿ç”¨æŒ‡å—

## ğŸ“¦ å®‰è£…

### å‰ç½®è¦æ±‚

1. **Python ç¯å¢ƒ**: Python >= 3.8
2. **QLLM é¡¹ç›®**: éœ€è¦å…ˆå®‰è£…å’Œé…ç½®QLLMé¡¹ç›®
3. **PyTorch**: å»ºè®®ä½¿ç”¨ PyTorch >= 1.12.0
4. **ç³»ç»Ÿå†…å­˜**: æ¨èè‡³å°‘ 16GB RAMï¼ˆç”¨äºåŠ è½½è¾ƒå¤§æ¨¡å‹ï¼‰

### å®‰è£… QLLM (å‰ç½®ä¾èµ–)

LlamaDistributoråŸºäºQLLMé¡¹ç›®ï¼Œéœ€è¦å…ˆå®‰è£…QLLMï¼š

```bash
# å…‹éš†QLLMé¡¹ç›®ï¼ˆå¦‚æœè¿˜æ²¡æœ‰çš„è¯ï¼‰
git clone https://github.com/your-org/QLLM.git
cd QLLM

# å®‰è£…QLLMä¾èµ–
pip install -r requirements.txt
pip install -e .

# ç¡®ä¿QLLMåœ¨Pythonè·¯å¾„ä¸­
export PYTHONPATH=$PYTHONPATH:/path/to/QLLM
```

### å®‰è£… LlamaDistributor

```bash
# æ–¹æ³•1: ä»æºç å®‰è£…
git clone https://github.com/your-org/LlamaDistributor.git
cd LlamaDistributor
pip install -r requirements.txt
pip install -e .

# æ–¹æ³•2: ç›´æ¥ä»å½“å‰ç›®å½•å®‰è£…
cd LlamaDistributor
pip install -e .
```

### éªŒè¯å®‰è£…

```python
# æµ‹è¯•å¯¼å…¥
import llamadist
print(llamadist.get_info())

# è¿è¡ŒåŸºç¡€ç¤ºä¾‹
python examples/basic_partition.py
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. åŸºç¡€åˆ†å±‚ç¤ºä¾‹

```python
from llamadist import PartitionStrategy, LlamaPartitioner, DistributedInference

# åˆ›å»ºåˆ†å±‚ç­–ç•¥
strategy = PartitionStrategy(
    num_partitions=4,
    strategy_type="uniform",
    target_devices=["cuda:0", "cuda:1", "cuda:2", "cuda:3"]
)

# åˆå§‹åŒ–åˆ†å±‚å™¨
partitioner = LlamaPartitioner(model_path="path/to/llama/model")

# æ‰§è¡Œåˆ†å±‚
submodels = partitioner.partition(strategy)

# ä¿å­˜åˆ†å±‚ç»“æœ
partitioner.save_partitioned_models(submodels, "output_dir")
```

### 2. åˆ†å¸ƒå¼æ¨ç†

```python
from llamadist import DistributedInference
from llamadist.inference.coordinator import GenerationConfig

# åˆ›å»ºæ¨ç†å¼•æ“
inference = DistributedInference(submodels)

# é…ç½®ç”Ÿæˆå‚æ•°
config = GenerationConfig(
    max_length=512,
    temperature=0.8,
    top_p=0.9,
    do_sample=True
)

# ç”Ÿæˆæ–‡æœ¬
result = inference.generate_text(
    prompt="ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½",
    tokenizer=tokenizer,
    generation_config=config
)
print(result)
```

### 3. ä½¿ç”¨é…ç½®æ–‡ä»¶

```python
from llamadist import LlamaDistConfig

# åˆ›å»ºé…ç½®
config = LlamaDistConfig(
    model_path="path/to/model",
    num_partitions=4,
    strategy_type="memory",
    max_memory_per_partition="4GB",
    target_devices=["cuda:0", "cuda:1", "cpu", "cpu"]
)

# ä¿å­˜é…ç½®
config.save("my_config.json")

# åŠ è½½é…ç½®
config = LlamaDistConfig.load("my_config.json")
```

## ğŸ“‹ è¯¦ç»†ä½¿ç”¨æ•™ç¨‹

### åˆ†å±‚ç­–ç•¥è¯¦è§£

#### 1. å‡åŒ€åˆ†å±‚ (Uniform)
```python
strategy = PartitionStrategy(
    num_partitions=4,
    strategy_type="uniform"
)
```
- å°†æ¨¡å‹å±‚æ•°å¹³å‡åˆ†é…åˆ°å„ä¸ªåˆ†å±‚
- é€‚ç”¨äºè®¾å¤‡é…ç½®ç›¸ä¼¼çš„åœºæ™¯

#### 2. å†…å­˜åˆ†å±‚ (Memory)
```python
strategy = PartitionStrategy(
    num_partitions=4,
    strategy_type="memory",
    max_memory_per_partition="4GB"
)
```
- æ ¹æ®å†…å­˜é™åˆ¶è¿›è¡Œåˆ†å±‚
- é€‚ç”¨äºå†…å­˜å—é™çš„ç¯å¢ƒ

#### 3. è®¡ç®—åˆ†å±‚ (Compute)
```python
strategy = PartitionStrategy(
    num_partitions=4,
    strategy_type="compute",
    load_balance_factor=0.8
)
```
- æ ¹æ®è®¡ç®—è´Ÿè½½è¿›è¡Œåˆ†å±‚
- ç¡®ä¿å„åˆ†å±‚è®¡ç®—é‡ç›¸å¯¹å‡è¡¡

#### 4. æ··åˆç­–ç•¥ (Mixed)
```python
strategy = PartitionStrategy(
    num_partitions=4,
    strategy_type="mixed",
    max_memory_per_partition="4GB",
    memory_weight=0.6
)
```
- åŒæ—¶è€ƒè™‘å†…å­˜å’Œè®¡ç®—è´Ÿè½½
- é€šè¿‡æƒé‡å¹³è¡¡ä¸¤ç§çº¦æŸ

#### 5. è‡ªå®šä¹‰åˆ†å±‚ (Custom)
```python
strategy = PartitionStrategy(
    num_partitions=3,
    strategy_type="custom",
    custom_boundaries=[(0, 10), (11, 20), (21, 31)]
)
```
- ç”¨æˆ·è‡ªå®šä¹‰å±‚è¾¹ç•Œ
- æœ€å¤§çµæ´»æ€§

### è®¾å¤‡é…ç½®

#### GPUé…ç½®
```python
# å¤šGPUé…ç½®
target_devices = ["cuda:0", "cuda:1", "cuda:2", "cuda:3"]

# æ··åˆè®¾å¤‡é…ç½®
target_devices = ["cuda:0", "cuda:1", "cpu", "cpu"]
```

#### å†…å­˜ä¼˜åŒ–
```python
# å¯ç”¨é‡åŒ–
config = LlamaDistConfig(
    quantization="8bit",
    max_memory_per_partition="4GB"
)
```

### æ¨ç†ä¼˜åŒ–

#### å¼‚æ­¥æ¨ç†
```python
inference = DistributedInference(
    submodels=submodels,
    enable_async=True,
    max_workers=8
)

# å¼‚æ­¥ç”Ÿæˆ
future = inference.async_generate(input_ids)
result = future.result()
```

#### æ‰¹é‡æ¨ç†
```python
# æ‰¹é‡å¤„ç†
batch_inputs = [input_ids_1, input_ids_2, input_ids_3]
results = inference.batch_generate(batch_inputs, max_batch_size=2)
```

#### KVç¼“å­˜ä¼˜åŒ–
```python
config = GenerationConfig(
    use_cache=True,
    max_length=1024
)

# å¯ç”¨ç¼“å­˜çš„ç”Ÿæˆ
result = inference.generate(input_ids, generation_config=config)
```

## ğŸ”§ é«˜çº§åŠŸèƒ½

### æ€§èƒ½ç›‘æ§
```python
# è·å–æ€§èƒ½ç»Ÿè®¡
stats = inference.get_stats()
print(f"å¹³å‡æ¨ç†æ—¶é—´: {stats['avg_inference_time']:.4f}s")
print(f"ååé‡: {stats['tokens_per_second']:.2f} tokens/s")
```

### æ¨¡å‹åˆ†æ
```python
# è¯¦ç»†æ¨¡å‹åˆ†æ
analyzer = LlamaModelAnalyzer(model_path)
model_info = analyzer.analyze_model(detailed=True)

# ä¿å­˜åˆ†æç»“æœ
analyzer.save_analysis("model_analysis.json")
```

### é…ç½®æ¨¡æ¿
```python
from llamadist.utils.config import create_default_configs

# è·å–é¢„å®šä¹‰é…ç½®
configs = create_default_configs()
memory_config = configs["memory_optimized"]
performance_config = configs["performance"]
```

## ğŸ› æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **QLLMå¯¼å…¥å¤±è´¥**
   ```bash
   # ç¡®ä¿QLLMåœ¨Pythonè·¯å¾„ä¸­
   export PYTHONPATH=$PYTHONPATH:/path/to/QLLM
   ```

2. **å†…å­˜ä¸è¶³é”™è¯¯**
   ```python
   # å‡å°‘åˆ†å±‚å¤§å°æˆ–ä½¿ç”¨CPU
   config.max_memory_per_partition = "2GB"
   config.target_devices = ["cpu"] * 4
   ```

3. **CUDAå†…å­˜é”™è¯¯**
   ```python
   # æ¸…ç†GPUç¼“å­˜
   torch.cuda.empty_cache()
   ```

### è°ƒè¯•æ¨¡å¼
```python
import logging
logging.basicConfig(level=logging.DEBUG)

# å¯ç”¨è¯¦ç»†æ—¥å¿—
config.detailed_analysis = True
```

## ğŸ“Š æ€§èƒ½åŸºå‡†

### å†…å­˜ä½¿ç”¨å¯¹æ¯”
| æ¨¡å‹å¤§å° | åŸå§‹ | 4åˆ†å±‚ | 8åˆ†å±‚ | èŠ‚çœç‡ |
|---------|------|-------|-------|--------|
| 7B      | 13GB | 3.5GB | 2GB   | 75%    |
| 13B     | 26GB | 7GB   | 4GB   | 80%    |
| 30B     | 60GB | 15GB  | 8GB   | 85%    |

### æ¨ç†æ€§èƒ½
- **å»¶è¿Ÿ**: å¢åŠ 10-20%ï¼ˆç”±äºçŠ¶æ€ä¼ é€’ï¼‰
- **ååé‡**: åœ¨æ‰¹é‡æ¨ç†ä¸­å¯æå‡2-4å€
- **æ‰©å±•æ€§**: æ”¯æŒä»»æ„æ•°é‡çš„è®¾å¤‡

## ğŸ¤ è´¡çŒ®æŒ‡å—

æ¬¢è¿è´¡çŒ®ä»£ç ï¼è¯·å‚è€ƒï¼š
1. Forké¡¹ç›®
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯
3. æäº¤æ›´æ”¹
4. åˆ›å»ºPull Request

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨Apache 2.0è®¸å¯è¯ã€‚

## ğŸ†˜ è·å–å¸®åŠ©

- ğŸ“š [æ–‡æ¡£](https://llamadist.readthedocs.io/)
- ğŸ› [Issues](https://github.com/llamadist/LlamaDistributor/issues)
- ğŸ’¬ [è®¨è®º](https://github.com/llamadist/LlamaDistributor/discussions) 