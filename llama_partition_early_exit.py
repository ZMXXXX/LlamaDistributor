#! /usr/bin/env python3
"""
Llama åˆ†å±‚åˆ†å‰²æ¨ç†
"""

import torch
import sys
import time
import argparse
import os
import json
from datetime import datetime
from pathlib import Path
from typing import Optional, List, Dict, Any
from transformers import LlamaTokenizer, LlamaForCausalLM
import torch.nn.functional as F
import matplotlib
from tabulate import tabulate

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from llamadist import (
    PartitionStrategy, 
    LlamaPartitioner, 
    SingleDeviceInference, 
    StrategyType
)
from llamadist.inference.coordinator import GenerationConfig

# å…¨å±€æµ‹è¯•å‚æ•°
test_prompts = [
    "Llama is a large language model,",
    "USA is a country in North America,",
    "The capital of USA is Washington, D.C.,",
    "write a poem about LOVE",
    "The answer of 1+1 is",
    "give me a joke"
]

# å…¨å±€ç”Ÿæˆå‚æ•°ï¼ˆä½œä¸ºé»˜è®¤å€¼ï¼‰
DEFAULT_TEMPERATURE = 0.8
DEFAULT_TOP_P = 0.9
DEFAULT_DO_SAMPLE = True
DEFAULT_MAX_NEW_TOKENS = 100


def load_strategies_from_config(config_file: str = "configs/strategies_config.json", device: str = "cuda:0") -> List[Dict[str, Any]]:
    """
    ä»JSONé…ç½®æ–‡ä»¶åŠ è½½ç­–ç•¥åˆ—è¡¨
    
    Args:
        config_file: é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸ºstrategies_config.json
        device: è®¾å¤‡åç§°ï¼Œç”¨äºè®¾ç½®strategyä¸­çš„single_deviceå‚æ•°
        
    Returns:
        List[Dict]: åŒ…å«ç­–ç•¥å¯¹è±¡çš„å­—å…¸åˆ—è¡¨
    """
    # è·å–é…ç½®æ–‡ä»¶çš„å®Œæ•´è·¯å¾„
    config_path = Path(__file__).parent / config_file
    
    if not config_path.exists():
        raise FileNotFoundError(f"ç­–ç•¥é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
    
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config_data = json.load(f)
    except json.JSONDecodeError as e:
        raise ValueError(f"é…ç½®æ–‡ä»¶JSONæ ¼å¼é”™è¯¯: {e}")
    except Exception as e:
        raise RuntimeError(f"è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
    
    if "strategies" not in config_data:
        raise ValueError("é…ç½®æ–‡ä»¶ä¸­ç¼ºå°‘'strategies'å­—æ®µ")
    
    strategies_to_test = []
    
    for strategy_config in config_data["strategies"]:
        try:
            # éªŒè¯å¿…éœ€å­—æ®µ
            required_fields = ["name", "num_partitions", "strategy_type"]
            for field in required_fields:
                if field not in strategy_config:
                    raise ValueError(f"ç­–ç•¥é…ç½®ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
            
            # è½¬æ¢strategy_typeå­—ç¬¦ä¸²ä¸ºæšä¸¾
            strategy_type_str = strategy_config["strategy_type"]
            if strategy_type_str == "SINGLE_DEVICE":
                strategy_type = StrategyType.SINGLE_DEVICE
            else:
                # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ å…¶ä»–ç­–ç•¥ç±»å‹çš„æ”¯æŒ
                raise ValueError(f"ä¸æ”¯æŒçš„ç­–ç•¥ç±»å‹: {strategy_type_str}")
            
            # å¤„ç†custom_boundaries
            custom_boundaries = strategy_config.get("custom_boundaries")
            if custom_boundaries is not None:
                # å°†listè½¬æ¢ä¸ºtupleä»¥ç¬¦åˆPartitionStrategyçš„è¦æ±‚
                custom_boundaries = [tuple(boundary) for boundary in custom_boundaries]
            
            # åˆ›å»ºPartitionStrategyå¯¹è±¡
            strategy = PartitionStrategy(
                num_partitions=strategy_config["num_partitions"],
                strategy_type=strategy_type,
                single_device=device,
                custom_boundaries=custom_boundaries
            )
            
            strategies_to_test.append({
                "name": strategy_config["name"],
                "strategy": strategy,
                "description": strategy_config.get("description", ""),
                "exit_position": strategy_config.get("exit_position")
            })
            
        except Exception as e:
            print(f"è­¦å‘Š: è·³è¿‡æ— æ•ˆçš„ç­–ç•¥é…ç½® '{strategy_config.get('name', 'unknown')}': {e}")
            continue
    
    if not strategies_to_test:
        raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„ç­–ç•¥é…ç½®")
    
    print(f"æˆåŠŸåŠ è½½ {len(strategies_to_test)} ä¸ªç­–ç•¥é…ç½®:")
    for strategy_dict in strategies_to_test:
        description = f" - {strategy_dict['description']}" if strategy_dict['description'] else ""
        print(f"  - {strategy_dict['name']}{description}")
    
    return strategies_to_test



def sample_next_token(logits, temperature=1.0, top_p=0.9, do_sample=True):
    """
    ä»logitsä¸­é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
    ä½¿ç”¨transformerså†…ç½®çš„é‡‡æ ·é€»è¾‘ä»¥ä¿æŒä¸€è‡´æ€§
    
    Args:
        logits: æ¨¡å‹è¾“å‡ºçš„logits tensor, shape: (batch_size, seq_len, vocab_size)
        temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶éšæœºæ€§ï¼Œé»˜è®¤1.0
        top_p: top-pé‡‡æ ·å‚æ•°ï¼Œé»˜è®¤0.9
        do_sample: æ˜¯å¦è¿›è¡Œé‡‡æ ·ï¼Œå¦‚æœFalseåˆ™ä½¿ç”¨è´ªå©ªæœç´¢
        
    Returns:
        torch.Tensor: é‡‡æ ·å¾—åˆ°çš„ä¸‹ä¸€ä¸ªtokenï¼Œshape: (batch_size, 1)
    """
    from transformers.generation.utils import (
        TemperatureLogitsWarper,
        TopPLogitsWarper,
        LogitsProcessorList
    )
    
    # è·å–æœ€åä¸€ä¸ªä½ç½®çš„logits
    logits = logits[:, -1, :]  # (batch_size, vocab_size)
    
    if not do_sample:
        # è´ªå©ªæœç´¢ï¼šé€‰æ‹©æ¦‚ç‡æœ€å¤§çš„token
        next_token = torch.argmax(logits, dim=-1, keepdim=True)
        return next_token
    
    # ä½¿ç”¨transformerså†…ç½®çš„logits processors
    logits_processors = LogitsProcessorList()
    
    # æ·»åŠ æ¸©åº¦scaling
    if temperature != 1.0:
        logits_processors.append(TemperatureLogitsWarper(temperature))
    
    # æ·»åŠ top-p filtering
    if top_p < 1.0:
        logits_processors.append(TopPLogitsWarper(top_p))
    
    # åº”ç”¨logits processors
    if len(logits_processors) > 0:
        # åˆ›å»ºdummy input_idsç”¨äºlogits processoræ¥å£
        dummy_input_ids = torch.zeros((logits.shape[0], 1), dtype=torch.long, device=logits.device)
        logits = logits_processors(dummy_input_ids, logits)
    
    # ä»å¤„ç†åçš„logitsä¸­é‡‡æ ·
    probs = F.softmax(logits, dim=-1)
    next_token = torch.multinomial(probs, num_samples=1)
    
    return next_token


def benchmark_baseline_inference(
    model_path: str, 
    tokenizer, 
    device: str, 
    temperature: float = DEFAULT_TEMPERATURE,
    top_p: float = DEFAULT_TOP_P,
    do_sample: bool = DEFAULT_DO_SAMPLE,
    max_new_tokens: int = DEFAULT_MAX_NEW_TOKENS
) -> dict:
    """
    æµ‹è¯•å®Œæ•´æ¨¡å‹æ¨ç†æ€§èƒ½ä½œä¸ºbaseline

    Args:
        model_path: æ¨¡å‹è·¯å¾„
        tokenizer: åˆ†è¯å™¨
        device: è®¾å¤‡
        temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶éšæœºæ€§ï¼Œé»˜è®¤0.8
        top_p: top-pé‡‡æ ·å‚æ•°ï¼Œé»˜è®¤0.9
        do_sample: æ˜¯å¦å¯ç”¨é‡‡æ ·ï¼Œé»˜è®¤True
        max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°ï¼Œé»˜è®¤100

    Returns:
        dict: æµ‹è¯•ç»“æœå­—å…¸ï¼ŒåŒ…å«å„ç§æ€§èƒ½æŒ‡æ ‡
    """
    # å‚æ•°éªŒè¯
    if temperature <= 0:
        raise ValueError(f"temperatureå¿…é¡»å¤§äº0ï¼Œå½“å‰å€¼: {temperature}")
    if not 0 < top_p <= 1:
        raise ValueError(f"top_på¿…é¡»åœ¨(0,1]èŒƒå›´å†…ï¼Œå½“å‰å€¼: {top_p}")
    if max_new_tokens <= 0:
        raise ValueError(f"max_new_tokenså¿…é¡»å¤§äº0ï¼Œå½“å‰å€¼: {max_new_tokens}")
    
    print(f"å®Œæ•´æ¨¡å‹æ¨ç†")
    print("="*60)
    print(f"è®¾å¤‡: {device}")
    print(f"ç”Ÿæˆå‚æ•°: temperature={temperature}, top_p={top_p}, do_sample={do_sample}, max_new_tokens={max_new_tokens}")

    # æ£€æŸ¥è®¾å¤‡å¯ç”¨æ€§
    if device.startswith("cuda") and not torch.cuda.is_available():
        raise RuntimeError("CUDAä¸å¯ç”¨ï¼Œä½†æŒ‡å®šäº†CUDAè®¾å¤‡")
    
    load_start_time = time.time()

    try:
        # åŠ è½½åŸå§‹æ¨¡å‹ 
        model = LlamaForCausalLM.from_pretrained(
            model_path,
            device_map=device,
            torch_dtype=torch.float16
        )
        model.eval()
    except Exception as e:
        raise RuntimeError(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")

    load_time = time.time() - load_start_time
    print(f"åŸå§‹æ¨¡å‹åŠ è½½å®Œæˆï¼Œè€—æ—¶: {load_time:.2f}ç§’")

    # åˆå§‹åŒ–æ€§èƒ½æŒ‡æ ‡
    total_tokens_generated = 0  # ç”Ÿæˆçš„æ€»tokenæ•°
    total_decode_time = 0  # è§£ç æ—¶é—´ï¼ˆé€tokenç”Ÿæˆæ—¶é—´ï¼‰
    peak_memory_usage = 0  # å³°å€¼å†…å­˜ä½¿ç”¨
    first_token_latencies = []  # æ¯ä¸ªpromptçš„é¦–tokenå»¶è¿Ÿ
    total_generation_time = 0 # æ€»ç”Ÿæˆæ—¶é—´

    for prompt in test_prompts:
        # è®°å½•æ¯ä¸ªpromptçš„å¼€å§‹æ—¶é—´
        prompt_start_time = time.time()
        
        # tokenization
        input_ids = tokenizer.encode(prompt, return_tensors="pt").to(device)

        # è§£ç é˜¶æ®µ
        first_token_generated = False
        generation_start_time = time.time() 
        
        with torch.no_grad():

            current_ids = input_ids.clone()
            outputs = model(current_ids)

            for token_idx in range(max_new_tokens):
                # å‰å‘ä¼ æ’­è·å–logits
                token_start_time = time.time()
                outputs = model(current_ids)
                logits = outputs.logits

                # é‡‡æ ·
                next_token = sample_next_token(logits, temperature, top_p, do_sample)
                current_ids = torch.cat([current_ids, next_token], dim=-1)
                
                token_end_time = time.time()
                token_decode_time = token_end_time - token_start_time
                total_decode_time += token_decode_time

                # è®°å½•é¦–tokenå»¶è¿Ÿï¼ˆåªè®°å½•ä¸€æ¬¡ï¼‰
                if not first_token_generated:
                    first_token_latency = token_end_time - prompt_start_time
                    first_token_latencies.append(first_token_latency)
                    first_token_generated = True
                    
                    # ç«‹å³è§£ç å¹¶æ‰“å°ç¬¬ä¸€ä¸ªtokenï¼Œç”¨äºéªŒè¯TTFTè®¡ç®—
                    first_token_text = tokenizer.decode(next_token[0], skip_special_tokens=True)
                    print(f"\n--- [å®Œæ•´æ¨¡å‹] ç¬¬ä¸€ä¸ªtoken '{first_token_text}' ç”Ÿæˆå®Œæˆï¼ŒTTFT: {first_token_latency*1000:.3f}ms")

                total_tokens_generated += 1

                # ç›‘æ§å†…å­˜ä½¿ç”¨ï¼ˆæ¯éš”å‡ ä¸ªtokenæ£€æŸ¥ä¸€æ¬¡å³å¯ï¼‰
                if token_idx % 10 == 0:
                    current_memory_usage = torch.cuda.memory_allocated() / 1024**2
                    peak_memory_usage = max(peak_memory_usage, current_memory_usage)

       
        # è§£ç å’Œæ˜¾ç¤ºç”Ÿæˆçš„æ–‡æœ¬
        generated_tokens = current_ids[0, len(input_ids[0]):].tolist()  # åªå–æ–°ç”Ÿæˆçš„token
        generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)

        # è®°å½•è¿™ä¸ªpromptçš„æ€»ç”Ÿæˆæ—¶é—´ï¼ˆä¸åŒ…å«æ‰“å°æ—¶é—´ï¼‰
        prompt_generation_time = time.time() - generation_start_time
        total_generation_time += prompt_generation_time
        
        print(f"\n--- Prompt: {prompt}")
        print(f"Generated: {generated_text}")
        print(f"Generated tokens: {len(generated_tokens)}")
            
    # åœ¨æ‰€æœ‰promptå¤„ç†å®Œåè®¡ç®—æœ€ç»ˆæŒ‡æ ‡
    if total_tokens_generated > 0:
        average_token_decode_time = total_decode_time / total_tokens_generated
        average_throughput = total_tokens_generated / total_generation_time
        average_latency = total_generation_time / total_tokens_generated
        average_first_token_latency = sum(first_token_latencies) / len(first_token_latencies) if first_token_latencies else 0
    else:
        average_throughput = 0
        average_latency = 0
        average_first_token_latency = 0

    # æ‰“å°ç»“æœ
    print(f"\n=== å®Œæ•´æ¨¡å‹æ¨ç†æµ‹è¯•ç»“æœ ===")
    print(f"æ¨¡å‹åŠ è½½æ—¶é—´: {load_time:.3f}ç§’")
    print(f"æ€»ç”Ÿæˆæ—¶é—´: {total_generation_time:.3f}ç§’")
    print(f"æ€»è§£ç æ—¶é—´: {total_decode_time:.3f}ç§’")
    print(f"ç”Ÿæˆtokenæ€»æ•°: {total_tokens_generated}")
    print(f"å¹³å‡ååé‡: {average_throughput:.3f} tokens/ç§’")
    print(f"å¹³å‡æ¯tokenç”Ÿæˆå»¶è¿Ÿ: {average_latency*1000:.3f}æ¯«ç§’/token")
    print(f"å¹³å‡é¦–tokenå»¶è¿Ÿ(TTFT): {average_first_token_latency*1000:.3f}æ¯«ç§’")
    print(f"å³°å€¼GPUå†…å­˜ä½¿ç”¨: {peak_memory_usage:.3f}MB")

    return {
        "load_time": load_time,
        "total_decode_time": total_decode_time,
        "total_generation_time": total_generation_time,
        "total_tokens_generated": total_tokens_generated,
        "average_throughput": average_throughput,
        "average_latency": average_latency,
        "average_first_token_latency": average_first_token_latency,
        "peak_memory_usage": peak_memory_usage
    }




def benchmark_partition_inference(
    strategies_to_test: list[dict],
    model_path: str, 
    tokenizer, 
    device: str, 
    temperature: float = DEFAULT_TEMPERATURE,
    top_p: float = DEFAULT_TOP_P,
    do_sample: bool = DEFAULT_DO_SAMPLE,
    max_new_tokens: int = DEFAULT_MAX_NEW_TOKENS
) -> dict:
    """
    æµ‹è¯•åˆ†å±‚åˆ†å‰²æ¨ç†æ€§èƒ½
    """
    print("æŒ‰å±‚åˆ†å‰²æ¨ç†")
    print(f"è®¾å¤‡: {device}")
    print(f"ç”Ÿæˆå‚æ•°: temperature={temperature}, top_p={top_p}, do_sample={do_sample}, max_new_tokens={max_new_tokens}")
    
    print("="*60)
    
    # æ£€æŸ¥è®¾å¤‡å¯ç”¨æ€§
    if device.startswith("cuda") and not torch.cuda.is_available():
        raise RuntimeError("CUDAä¸å¯ç”¨ï¼Œä½†æŒ‡å®šäº†CUDAè®¾å¤‡")
    
    # åˆ›å»ºåˆ†å±‚å™¨
    partitioner = LlamaPartitioner(model_path=model_path)
    model_info = partitioner.analyze_model(detailed=False, device=device)
    print("åˆ†å‰²æ¨¡å‹ä¿¡æ¯ï¼š")
    print(f"æ¨¡å‹å±‚æ•°: {model_info.num_layers}")
    print(f"éšè—ç»´åº¦: {model_info.hidden_size}")
    print(f"æ€»å‚æ•°: {model_info.total_params:,}")
    print(f"ä¼°è®¡å†…å­˜: {model_info.total_memory / (1024**3):.2f} GB")

    # å­˜å‚¨æ‰€æœ‰ç­–ç•¥çš„ç»“æœ
    all_results = {}

    for strategy in strategies_to_test:
        strategy_exit_position = strategy.get('exit_position')
        exit_info = f" (Early-exit: ç¬¬{strategy_exit_position}ä¸ªsubmodelå)" if strategy_exit_position is not None else " (æ­£å¸¸æ¨ç†åˆ°æœ€åä¸€å±‚)"
        print(f"æŒ‰æµ‹è¯•ç­–ç•¥: {strategy['name']}{exit_info}å¼€å§‹åˆ†å±‚...")
        print("-" * 30)

        partition_start_time = time.time()
        # åˆ›å»ºåˆ†å±‚
        partitions = strategy['strategy'].create_partitions(model_info)
            
        print("å½“å‰åˆ†å±‚ç­–ç•¥ï¼š")
        for i, partition in enumerate(partitions):
            layer_count = partition.layer_end - partition.layer_start + 1
            print(f"     åˆ†å±‚ {i}: å±‚{partition.layer_start}-{partition.layer_end} ({layer_count}å±‚) @ {partition.device}")
            
        # æ‰§è¡Œåˆ†å±‚
        print("æ‰§è¡Œæ¨¡å‹åˆ†å±‚...")
        partition_start_time = time.time()
        submodels = partitioner.partition(
            strategy = strategy['strategy'],
            copy_weights = True
        )

        

        print("åˆ›å»ºåˆ†å±‚å¼•æ“...")
        
        inference_engine = SingleDeviceInference(
            submodels=submodels,
            generation_config = GenerationConfig(
                max_new_tokens = max_new_tokens,
                temperature = temperature,
                top_p = top_p,
                do_sample = do_sample,
                eos_token_id = tokenizer.eos_token_id,
                pad_token_id = tokenizer.pad_token_id,
                exit_position = strategy_exit_position
            ),
            device = device
        )
        
        # ğŸ”§ ä¿®å¤ï¼šä¸ºearly-exitè®¾ç½®åŸå§‹æ¨¡å‹çš„æƒé‡
        if strategy_exit_position is not None:
            print("æ£€æµ‹åˆ°early-exité…ç½®ï¼Œæ­£åœ¨è·å–åŸå§‹æ¨¡å‹æƒé‡...")
            # ä¸´æ—¶åŠ è½½åŸå§‹æ¨¡å‹ä»¥è·å–lm_headå’Œnormæƒé‡
            try:
                original_model = LlamaForCausalLM.from_pretrained(
                    model_path,
                    device_map="cpu",  # å…ˆåŠ è½½åˆ°CPUä»¥èŠ‚çœæ˜¾å­˜
                    torch_dtype=torch.float16
                )
                
                # è®¾ç½®lm_headæƒé‡
                if hasattr(original_model, 'lm_head') and original_model.lm_head is not None:
                    inference_engine.set_original_lm_head_weights(original_model.lm_head.weight.data)
                    print("âœ… å·²è®¾ç½®åŸå§‹lm_headæƒé‡")
                
                # è®¾ç½®normæƒé‡ï¼ˆå¦‚æœéœ€è¦early-exitå­æ¨¡å‹ä¸­ä½¿ç”¨ï¼‰
                if hasattr(original_model.model, 'norm') and original_model.model.norm is not None:
                    # ä¸ºæ¨ç†å¼•æ“è®¾ç½®åŸå§‹normæƒé‡
                    inference_engine._original_norm_weights = original_model.model.norm.weight.data.clone()
                    print("âœ… å·²è®¾ç½®åŸå§‹normæƒé‡")
                
                # æ¸…ç†åŸå§‹æ¨¡å‹ä»¥é‡Šæ”¾å†…å­˜
                del original_model
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                print("âœ… å·²æ¸…ç†ä¸´æ—¶åŠ è½½çš„åŸå§‹æ¨¡å‹")
                    
            except Exception as e:
                print(f"âš ï¸  è­¦å‘Šï¼šæ— æ³•åŠ è½½åŸå§‹æ¨¡å‹æƒé‡ï¼Œearly-exitå¯èƒ½æ•ˆæœä¸ä½³: {e}")

        partition_time = time.time() - partition_start_time
        

        for prompt in test_prompts:
            generated_text = inference_engine.generate_text(
                prompt=prompt,
                tokenizer=tokenizer,
                return_full_text=False
            )
            generated_tokens = len(tokenizer.encode(generated_text))

            print(f"\n--- Prompt: {prompt}")
            print(f"Generated: {generated_text}")
            print(f"Generated tokens: {generated_tokens}")
        
        stats = inference_engine.get_stats()

        
        print(f"\n=== åˆ†å‰²æ¨¡å‹æ¨ç†æµ‹è¯•ç»“æœ ===")
        print(f"æ¨¡å‹åˆ†å±‚æ—¶é—´: {partition_time:.3f}ç§’")
        print(f"æ€»ç”Ÿæˆæ—¶é—´: {stats['total_generation_time']:.3f}ç§’")
        print(f"æ€»è§£ç æ—¶é—´: {stats['token_decode_time']:.3f}ç§’")
        print(f"ç”Ÿæˆtokenæ€»æ•°: {stats['total_tokens_generated']}")
        print(f"å¹³å‡ååé‡: {stats['tokens_per_second']:.3f} tokens/ç§’")
        print(f"å¹³å‡æ¯tokenç”Ÿæˆå»¶è¿Ÿ: {stats['total_generation_time']/stats['total_tokens_generated']*1000:.3f}æ¯«ç§’/token")
        print(f"å¹³å‡é¦–tokenå»¶è¿Ÿ(TTFT): {stats['total_time_to_first_token']/len(test_prompts)*1000:.3f}æ¯«ç§’")

        # å­˜å‚¨å½“å‰ç­–ç•¥çš„ç»“æœ
        strategy_result = {
            "partition_time": partition_time,
            "total_generation_time": stats['total_generation_time'],
            "total_decode_time": stats['token_decode_time'],
            "total_tokens_generated": stats['total_tokens_generated'],
            "average_throughput": stats['tokens_per_second'],
            "average_latency": stats['total_generation_time']/stats['total_tokens_generated'],
            "average_first_token_latency": stats['total_time_to_first_token']/len(test_prompts),
            "peak_memory_usage": 0  # è¿™é‡Œå¯ä»¥åç»­æ·»åŠ å†…å­˜ç›‘æ§
        }
        
        all_results[strategy['name']] = strategy_result

        del submodels
        del inference_engine
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    return all_results


def create_comparison_table(baseline_result: dict, partition_results: dict, benchmark: bool = False):
    """
    åˆ›å»ºå®Œæ•´æ¨ç†å’Œåˆ†å‰²æ¨ç†çš„å¯¹æ¯”è¡¨æ ¼ï¼Œå¹¶ä¿å­˜åˆ°benchmarkå­æ–‡ä»¶å¤¹
    
    Args:
        baseline_result: å®Œæ•´æ¨ç†çš„ç»“æœå­—å…¸
        partition_results: åˆ†å‰²æ¨ç†çš„ç»“æœå­—å…¸ï¼ŒåŒ…å«å¤šä¸ªç­–ç•¥çš„ç»“æœ
        benchmark: æ˜¯å¦ç”Ÿæˆbenchmarkå›¾è¡¨å’Œè¯¦ç»†åˆ†æ
    """
    # æ ¹æ®benchmarkå‚æ•°å†³å®šæ˜¯å¦åˆ›å»ºæ–‡ä»¶å¤¹å’Œæ—¶é—´æˆ³
    if benchmark:
        # åˆ›å»ºbenchmarkæ–‡ä»¶å¤¹
        benchmark_dir = Path("benchmark")
        benchmark_dir.mkdir(exist_ok=True)
        
        # ç”Ÿæˆæ—¶é—´æˆ³ç”¨äºæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    else:
        benchmark_dir = None
        timestamp = None
    
    try:
        # å¯¼å…¥tabulateç”¨äºåˆ›å»ºè¡¨æ ¼
        from tabulate import tabulate
        use_tabulate = True
    except ImportError:
        print("æœªå®‰è£…tabulateåº“ï¼Œå°†ä½¿ç”¨ç®€å•æ ¼å¼è¾“å‡º")
        print("è¯·è¿è¡Œ: pip install tabulate")
        use_tabulate = False
    
    print("\n" + "="*80)
    print("æ€§èƒ½å¯¹æ¯”è¡¨æ ¼")
    print("="*80)
    
    # å®šä¹‰æŒ‡æ ‡æ˜ å°„
    metrics = [
        ("æ¨¡å‹åŠ è½½/åˆ†å±‚æ—¶é—´ (ç§’)", "load_time", "partition_time"),
        ("æ€»ç”Ÿæˆæ—¶é—´ (ç§’)", "total_generation_time", "total_generation_time"),
        ("æ€»è§£ç æ—¶é—´ (ç§’)", "total_decode_time", "total_decode_time"),
        ("ç”Ÿæˆtokenæ€»æ•°", "total_tokens_generated", "total_tokens_generated"),
        ("å¹³å‡ååé‡ (tokens/ç§’)", "average_throughput", "average_throughput"),
        ("å¹³å‡æ¯tokenå»¶è¿Ÿ (æ¯«ç§’)", "average_latency", "average_latency"),
        ("å¹³å‡é¦–tokenå»¶è¿Ÿ (æ¯«ç§’)", "average_first_token_latency", "average_first_token_latency"),
        ("å³°å€¼å†…å­˜ä½¿ç”¨ (MB)", "peak_memory_usage", "peak_memory_usage")
    ]
    
    # å‡†å¤‡è¡¨æ ¼æ•°æ®
    headers = ["æŒ‡æ ‡", "å®Œæ•´æ¨ç†"]
    for strategy_name in partition_results.keys():
        headers.append(f"åˆ†å‰²æ¨ç†-{strategy_name}")
    
    table_data = []
    csv_data = [headers]  # ç”¨äºä¿å­˜CSVæ ¼å¼
    
    for metric_name, baseline_key, partition_key in metrics:
        row = [metric_name]
        
        # æ·»åŠ å®Œæ•´æ¨ç†çš„å€¼
        baseline_value = baseline_result.get(baseline_key, 0)
        if "å»¶è¿Ÿ" in metric_name or "æ¯«ç§’" in metric_name:
            # å»¶è¿Ÿç›¸å…³æŒ‡æ ‡è½¬æ¢ä¸ºæ¯«ç§’
            if baseline_key in ["average_latency", "average_first_token_latency"]:
                baseline_value = baseline_value * 1000
            row.append(f"{baseline_value:.3f}")
        elif "ååé‡" in metric_name or "tokens/ç§’" in metric_name:
            row.append(f"{baseline_value:.3f}")
        elif metric_name == "ç”Ÿæˆtokenæ€»æ•°":
            row.append(f"{int(baseline_value)}")
        else:
            row.append(f"{baseline_value:.3f}")
        
        # æ·»åŠ åˆ†å‰²æ¨ç†çš„å€¼
        for strategy_name, strategy_result in partition_results.items():
            partition_value = strategy_result.get(partition_key, 0)
            if "å»¶è¿Ÿ" in metric_name or "æ¯«ç§’" in metric_name:
                # å»¶è¿Ÿç›¸å…³æŒ‡æ ‡è½¬æ¢ä¸ºæ¯«ç§’
                if partition_key in ["average_latency", "average_first_token_latency"]:
                    partition_value = partition_value * 1000
                row.append(f"{partition_value:.3f}")
            elif "ååé‡" in metric_name or "tokens/ç§’" in metric_name:
                row.append(f"{partition_value:.3f}")
            elif metric_name == "ç”Ÿæˆtokenæ€»æ•°":
                row.append(f"{int(partition_value)}")
            else:
                row.append(f"{partition_value:.3f}")
        
        table_data.append(row)
        csv_data.append(row)
    
    # æ‰“å°è¡¨æ ¼åˆ°æ§åˆ¶å°
    if use_tabulate:
        table_str = tabulate(table_data, headers=headers, tablefmt="grid")
        print(table_str)
    else:
        _print_simple_table(table_data, headers)
    
    # åªæœ‰åœ¨benchmarkæ¨¡å¼ä¸‹æ‰ä¿å­˜æ–‡ä»¶
    if benchmark:
        # ä¿å­˜è¡¨æ ¼åˆ°æ–‡ä»¶
        table_file = benchmark_dir / f"performance_comparison_{timestamp}.txt"
        with open(table_file, 'w', encoding='utf-8') as f:
            f.write("æ€§èƒ½å¯¹æ¯”è¡¨æ ¼\n")
            f.write("="*80 + "\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            if use_tabulate:
                f.write(table_str)
            else:
                # å†™å…¥ç®€å•æ ¼å¼è¡¨æ ¼
                header_line = " | ".join(f"{h:<20}" for h in headers)
                f.write(header_line + "\n")
                f.write("-" * len(header_line) + "\n")
                for row in table_data:
                    row_line = " | ".join(f"{str(cell):<20}" for cell in row)
                    f.write(row_line + "\n")
        
        # ä¿å­˜CSVæ ¼å¼
        csv_file = benchmark_dir / f"performance_comparison_{timestamp}.csv"
        with open(csv_file, 'w', encoding='utf-8') as f:
            for row in csv_data:
                f.write(",".join(str(cell) for cell in row) + "\n")
    
    # è®¡ç®—å¹¶æ˜¾ç¤ºæ€§èƒ½æå‡/ä¸‹é™
    print("\n" + "="*80)
    print("æ€§èƒ½å¯¹æ¯”åˆ†æ (ç›¸å¯¹äºå®Œæ•´æ¨ç†)")
    print("="*80)
    
    comparison_data = []
    comparison_headers = ["æŒ‡æ ‡"]
    for strategy_name in partition_results.keys():
        comparison_headers.append(f"{strategy_name} (å˜åŒ–%)")
    
    key_metrics = [
        ("å¹³å‡ååé‡", "average_throughput", "average_throughput", "higher_is_better"),
        ("å¹³å‡æ¯tokenå»¶è¿Ÿ", "average_latency", "average_latency", "lower_is_better"),
        ("å¹³å‡é¦–tokenå»¶è¿Ÿ", "average_first_token_latency", "average_first_token_latency", "lower_is_better"),
        ("æ€»ç”Ÿæˆæ—¶é—´", "total_generation_time", "total_generation_time", "lower_is_better")
    ]
    
    for metric_name, baseline_key, partition_key, direction in key_metrics:
        row = [metric_name]
        baseline_value = baseline_result.get(baseline_key, 0)
        
        for strategy_name, strategy_result in partition_results.items():
            partition_value = strategy_result.get(partition_key, 0)
            
            if baseline_value != 0:
                change_percent = ((partition_value - baseline_value) / baseline_value) * 100
                
                # æ ¹æ®æŒ‡æ ‡æ–¹å‘ç¡®å®šæ˜¯æ”¹è¿›è¿˜æ˜¯é€€åŒ–
                if direction == "higher_is_better":
                    status = "â†‘" if change_percent > 0 else "â†“"
                else:  # lower_is_better
                    status = "â†“" if change_percent > 0 else "â†‘"
                
                row.append(f"{change_percent:+.2f}% {status}")
            else:
                row.append("N/A")
        
        comparison_data.append(row)
    
    if use_tabulate:
        comparison_table = tabulate(comparison_data, headers=comparison_headers, tablefmt="grid")
        print(comparison_table)
    else:
        _print_simple_table(comparison_data, comparison_headers)
    print("\nè¯´æ˜: â†‘ è¡¨ç¤ºæ€§èƒ½æå‡, â†“ è¡¨ç¤ºæ€§èƒ½ä¸‹é™")
    
    # åªæœ‰åœ¨benchmarkæ¨¡å¼ä¸‹æ‰ä¿å­˜æ€§èƒ½åˆ†æå’Œç”Ÿæˆå›¾è¡¨
    if benchmark:
        # ä¿å­˜æ€§èƒ½åˆ†æè¡¨æ ¼
        analysis_file = benchmark_dir / f"performance_analysis_{timestamp}.txt"
        with open(analysis_file, 'w', encoding='utf-8') as f:
            f.write("æ€§èƒ½å¯¹æ¯”åˆ†æ (ç›¸å¯¹äºå®Œæ•´æ¨ç†)\n")
            f.write("="*80 + "\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            if use_tabulate:
                f.write(comparison_table)
            else:
                header_line = " | ".join(f"{h:<20}" for h in comparison_headers)
                f.write(header_line + "\n")
                f.write("-" * len(header_line) + "\n")
                for row in comparison_data:
                    row_line = " | ".join(f"{str(cell):<20}" for cell in row)
                    f.write(row_line + "\n")
            f.write("\n\nè¯´æ˜: â†‘ è¡¨ç¤ºæ€§èƒ½æå‡, â†“ è¡¨ç¤ºæ€§èƒ½ä¸‹é™")
        
        # ç”Ÿæˆæ€§èƒ½å¯¹æ¯”å›¾è¡¨
        _create_performance_charts(baseline_result, partition_results, benchmark_dir, timestamp)
        
        print(f"\nğŸ“ ç»“æœå·²ä¿å­˜åˆ°benchmarkæ–‡ä»¶å¤¹:")
        print(f"   - æ€§èƒ½è¡¨æ ¼: {table_file}")
        print(f"   - CSVæ•°æ®: {csv_file}")
        print(f"   - æ€§èƒ½åˆ†æ: {analysis_file}")
        print(f"   - æ€§èƒ½å›¾è¡¨: benchmark/performance_charts_{timestamp}.png")
    else:
        print(f"\nâœ… æ€§èƒ½å¯¹æ¯”å®Œæˆ (å¦‚éœ€ä¿å­˜å›¾è¡¨å’Œè¯¦ç»†åˆ†æï¼Œè¯·ä½¿ç”¨ --benchmark å‚æ•°)")


def _print_simple_table(table_data, headers):
    """æ‰“å°ç®€å•æ ¼å¼è¡¨æ ¼"""
    # æ‰“å°æ ‡é¢˜è¡Œ
    header = " | ".join(f"{h:<20}" for h in headers)
    print(header)
    print("-" * len(header))
    
    # æ‰“å°æ•°æ®è¡Œ
    for row in table_data:
        row_line = " | ".join(f"{str(cell):<20}" for cell in row)
        print(row_line)


def _create_performance_charts(baseline_result: dict, partition_results: dict, 
                              benchmark_dir: Path, timestamp: str):
    """
    åˆ›å»ºæ€§èƒ½å¯¹æ¯”å›¾è¡¨
    
    Args:
        baseline_result: å®Œæ•´æ¨ç†ç»“æœ
        partition_results: åˆ†å‰²æ¨ç†ç»“æœ
        benchmark_dir: ä¿å­˜ç›®å½•
        timestamp: æ—¶é—´æˆ³
    """
    try:
        import matplotlib.pyplot as plt
        import numpy as np
        
        # é…ç½®ä¸­æ–‡å­—ä½“
        try:
            # å°è¯•è®¾ç½®ä¸­æ–‡å­—ä½“
            import matplotlib.font_manager as fm
            # æŸ¥æ‰¾ç³»ç»Ÿä¸­çš„ä¸­æ–‡å­—ä½“
            chinese_fonts = []
            for font in fm.fontManager.ttflist:
                # ä¼˜å…ˆé€‰æ‹©çœŸæ­£çš„ä¸­æ–‡å­—ä½“
                if any(keyword in font.name for keyword in ['Noto Sans CJK', 'SimHei', 'Microsoft YaHei', 'WenQuanYi']):
                    chinese_fonts.append(font.name)
            
            if chinese_fonts:
                # å»é‡å¹¶é€‰æ‹©æœ€ä½³å­—ä½“
                unique_fonts = list(set(chinese_fonts))
                # ä¼˜å…ˆçº§ï¼šNoto Sans CJK > SimHei > Microsoft YaHei > WenQuanYi
                for preferred in ['Noto Sans CJK', 'SimHei', 'Microsoft YaHei', 'WenQuanYi']:
                    for font in unique_fonts:
                        if preferred in font:
                            plt.rcParams['font.sans-serif'] = [font, 'DejaVu Sans']
                            plt.rcParams['axes.unicode_minus'] = False
                            use_chinese = True
                            print(f"ä½¿ç”¨ä¸­æ–‡å­—ä½“: {font}")
                            break
                    if 'use_chinese' in locals():
                        break
                else:
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¼˜å…ˆå­—ä½“ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„
                    plt.rcParams['font.sans-serif'] = [unique_fonts[0], 'DejaVu Sans']
                    plt.rcParams['axes.unicode_minus'] = False
                    use_chinese = True
                    print(f"ä½¿ç”¨ä¸­æ–‡å­—ä½“: {unique_fonts[0]}")
            else:
                # å¦‚æœæ²¡æœ‰ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨è‹±æ–‡æ ‡ç­¾
                use_chinese = False
                print("æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œå°†ä½¿ç”¨è‹±æ–‡æ ‡ç­¾")
        except:
            use_chinese = False
            
    except ImportError:
        print("æœªå®‰è£…matplotlibï¼Œè·³è¿‡å›¾è¡¨ç”Ÿæˆ")
        print("è¯·è¿è¡Œ: pip install matplotlib")
        return
    
    # å‡†å¤‡æ•°æ®
    strategies = list(partition_results.keys())
    strategies.insert(0, "Complete Model" if not use_chinese else "å®Œæ•´æ¨ç†")
    
    # å…³é”®æŒ‡æ ‡
    throughput_data = [baseline_result.get("average_throughput", 0)]
    latency_data = [baseline_result.get("average_latency", 0) * 1000]  # è½¬æ¢ä¸ºæ¯«ç§’
    ttft_data = [baseline_result.get("average_first_token_latency", 0) * 1000]  # è½¬æ¢ä¸ºæ¯«ç§’
    generation_time_data = [baseline_result.get("total_generation_time", 0)]
    
    for strategy_result in partition_results.values():
        throughput_data.append(strategy_result.get("average_throughput", 0))
        latency_data.append(strategy_result.get("average_latency", 0) * 1000)
        ttft_data.append(strategy_result.get("average_first_token_latency", 0) * 1000)
        generation_time_data.append(strategy_result.get("total_generation_time", 0))
    
    # åˆ›å»º4ä¸ªå­å›¾
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
    
    # è®¾ç½®æ ‡é¢˜
    main_title = 'LLMæ¨ç†æ€§èƒ½å¯¹æ¯”' if use_chinese else 'LLM Inference Performance Comparison'
    fig.suptitle(main_title, fontsize=16, fontweight='bold')
    
    # é¢œè‰²è®¾ç½® - æ˜äº®ç”ŸåŠ¨ä½†ä¸è¿‡åˆ†é¥±å’Œçš„é…è‰²ï¼ˆæ”¯æŒ12+ç§ç­–ç•¥ï¼‰
    colors = [
        '#4A90E2',  # æ˜äº®çš„è“è‰²
        '#7ED321',  # é²œç»¿è‰²
        '#F5A623',  # æ©™é»„è‰²
        '#BD10E0',  # ç´«è‰²
        '#50E3C2',  # é’ç»¿è‰²
        '#F8E71C',  # æŸ æª¬é»„
        '#B8E986',  # æµ…ç»¿è‰²
        '#9013FE',  # è“ç´«è‰²
        '#FF6B6B',  # çŠç‘šçº¢
        '#4ECDC4',  # è–„è·ç»¿
        '#45B7D1',  # å¤©è“è‰²
        '#96CEB4',  # è–„è·ç»¿
        '#FFEAA7',  # å¥¶æ²¹é»„
        '#DDA0DD',  # æ·¡ç´«è‰²
        '#98D8C8',  # æµ…è“ç»¿
        '#F7DC6F',  # é‡‘é»„è‰²
        '#BB8FCE',  # è–°è¡£è‰ç´«
        '#85C1E9',  # æµ…è“è‰²
        '#F8C471',  # æ¡ƒè‰²
        '#82E0AA'   # æ·¡ç»¿è‰²
    ]
    
    # 1. å¹³å‡ååé‡å¯¹æ¯”
    bars1 = ax1.bar(strategies, throughput_data, color=[colors[i % len(colors)] for i in range(len(strategies))])
    title1 = 'å¹³å‡ååé‡ (tokens/ç§’)' if use_chinese else 'Average Throughput (tokens/sec)'
    ylabel1 = 'Tokens/ç§’' if use_chinese else 'Tokens/sec'
    ax1.set_title(title1, fontweight='bold')
    ax1.set_ylabel(ylabel1)
    ax1.tick_params(axis='x', rotation=45)
    # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, value in zip(bars1, throughput_data):
        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                f'{value:.2f}', ha='center', va='bottom')
    
    # 2. å¹³å‡æ¯tokenå»¶è¿Ÿå¯¹æ¯”
    bars2 = ax2.bar(strategies, latency_data, color=[colors[i % len(colors)] for i in range(len(strategies))])
    title2 = 'å¹³å‡æ¯tokenå»¶è¿Ÿ (æ¯«ç§’)' if use_chinese else 'Average Token Latency (ms)'
    ylabel2 = 'æ¯«ç§’' if use_chinese else 'Milliseconds'
    ax2.set_title(title2, fontweight='bold')
    ax2.set_ylabel(ylabel2)
    ax2.tick_params(axis='x', rotation=45)
    for bar, value in zip(bars2, latency_data):
        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                f'{value:.1f}', ha='center', va='bottom')
    
    # 3. é¦–tokenå»¶è¿Ÿå¯¹æ¯”
    bars3 = ax3.bar(strategies, ttft_data, color=[colors[i % len(colors)] for i in range(len(strategies))])
    title3 = 'å¹³å‡é¦–tokenå»¶è¿Ÿ TTFT (æ¯«ç§’)' if use_chinese else 'Average TTFT (ms)'
    ylabel3 = 'æ¯«ç§’' if use_chinese else 'Milliseconds'
    ax3.set_title(title3, fontweight='bold')
    ax3.set_ylabel(ylabel3)
    ax3.tick_params(axis='x', rotation=45)
    for bar, value in zip(bars3, ttft_data):
        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                f'{value:.1f}', ha='center', va='bottom')
    
    # 4. æ€»ç”Ÿæˆæ—¶é—´å¯¹æ¯”
    bars4 = ax4.bar(strategies, generation_time_data, color=[colors[i % len(colors)] for i in range(len(strategies))])
    title4 = 'æ€»ç”Ÿæˆæ—¶é—´ (ç§’)' if use_chinese else 'Total Generation Time (sec)'
    ylabel4 = 'ç§’' if use_chinese else 'Seconds'
    ax4.set_title(title4, fontweight='bold')
    ax4.set_ylabel(ylabel4)
    ax4.tick_params(axis='x', rotation=45)
    for bar, value in zip(bars4, generation_time_data):
        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'{value:.2f}', ha='center', va='bottom')
    
    # è°ƒæ•´å¸ƒå±€
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    chart_file = benchmark_dir / f"performance_charts_{timestamp}.png"
    plt.savefig(chart_file, dpi=300, bbox_inches='tight')
    plt.close()
    
    # åˆ›å»ºæ€§èƒ½å˜åŒ–ç™¾åˆ†æ¯”å›¾è¡¨
    _create_percentage_change_chart(baseline_result, partition_results, benchmark_dir, timestamp, use_chinese)


def _create_percentage_change_chart(baseline_result: dict, partition_results: dict,
                                   benchmark_dir: Path, timestamp: str, use_chinese: bool):
    """
    åˆ›å»ºæ€§èƒ½å˜åŒ–ç™¾åˆ†æ¯”å›¾è¡¨
    """
    try:
        import matplotlib.pyplot as plt
        import numpy as np
    except ImportError:
        return
    
    strategies = list(partition_results.keys())
    
    # æ ¹æ®è¯­è¨€è®¾ç½®æŒ‡æ ‡åç§°
    if use_chinese:
        metrics = ["ååé‡", "æ¯tokenå»¶è¿Ÿ", "é¦–tokenå»¶è¿Ÿ", "æ€»ç”Ÿæˆæ—¶é—´"]
    else:
        metrics = ["Throughput", "Token Latency", "TTFT", "Generation Time"]
    
    # è®¡ç®—ç™¾åˆ†æ¯”å˜åŒ–
    changes = {strategy: [] for strategy in strategies}
    
    for strategy_name, strategy_result in partition_results.items():
        # ååé‡ (higher is better)
        baseline_throughput = baseline_result.get("average_throughput", 0)
        partition_throughput = strategy_result.get("average_throughput", 0)
        throughput_change = ((partition_throughput - baseline_throughput) / baseline_throughput * 100) if baseline_throughput != 0 else 0
        
        # æ¯tokenå»¶è¿Ÿ (lower is better, æ‰€ä»¥å–è´Ÿå€¼è®©æ”¹è¿›æ˜¾ç¤ºä¸ºæ­£)
        baseline_latency = baseline_result.get("average_latency", 0)
        partition_latency = strategy_result.get("average_latency", 0)
        latency_change = -((partition_latency - baseline_latency) / baseline_latency * 100) if baseline_latency != 0 else 0
        
        # é¦–tokenå»¶è¿Ÿ (lower is better)
        baseline_ttft = baseline_result.get("average_first_token_latency", 0)
        partition_ttft = strategy_result.get("average_first_token_latency", 0)
        ttft_change = -((partition_ttft - baseline_ttft) / baseline_ttft * 100) if baseline_ttft != 0 else 0
        
        # æ€»ç”Ÿæˆæ—¶é—´ (lower is better)
        baseline_time = baseline_result.get("total_generation_time", 0)
        partition_time = strategy_result.get("total_generation_time", 0)
        time_change = -((partition_time - baseline_time) / baseline_time * 100) if baseline_time != 0 else 0
        
        changes[strategy_name] = [throughput_change, latency_change, ttft_change, time_change]
    
    # åˆ›å»ºå›¾è¡¨
    fig, ax = plt.subplots(figsize=(12, 8))
    
    x = np.arange(len(metrics))
    width = 0.25
    # ä½¿ç”¨ä¸ä¸»å›¾è¡¨ç›¸åŒçš„æ˜äº®ç”ŸåŠ¨é…è‰²
    colors = [
        '#4A90E2',  # æ˜äº®çš„è“è‰²
        '#7ED321',  # é²œç»¿è‰²
        '#F5A623',  # æ©™é»„è‰²
        '#BD10E0',  # ç´«è‰²
        '#50E3C2',  # é’ç»¿è‰²
        '#F8E71C',  # æŸ æª¬é»„
        '#B8E986',  # æµ…ç»¿è‰²
        '#9013FE',  # è“ç´«è‰²
        '#FF6B6B',  # çŠç‘šçº¢
        '#4ECDC4',  # è–„è·ç»¿
        '#45B7D1',  # å¤©è“è‰²
        '#96CEB4',  # è–„è·ç»¿
        '#FFEAA7',  # å¥¶æ²¹é»„
        '#DDA0DD',  # æ·¡ç´«è‰²
        '#98D8C8',  # æµ…è“ç»¿
        '#F7DC6F',  # é‡‘é»„è‰²
        '#BB8FCE',  # è–°è¡£è‰ç´«
        '#85C1E9',  # æµ…è“è‰²
        '#F8C471',  # æ¡ƒè‰²
        '#82E0AA'   # æ·¡ç»¿è‰²
    ]
    
    for i, (strategy_name, strategy_changes) in enumerate(changes.items()):
        offset = (i - 1) * width
        bars = ax.bar(x + offset, strategy_changes, width, label=strategy_name, color=colors[i % len(colors)])
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, value in zip(bars, strategy_changes):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2, height + (0.5 if height >= 0 else -1),
                   f'{value:+.1f}%', ha='center', va='bottom' if height >= 0 else 'top')
    
    # è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜
    if use_chinese:
        ax.set_xlabel('æ€§èƒ½æŒ‡æ ‡')
        ax.set_ylabel('æ€§èƒ½æ”¹è¿› (%)')
        ax.set_title('æ€§èƒ½æ”¹è¿›å¯¹æ¯” (ç›¸å¯¹äºå®Œæ•´æ¨ç†)\næ­£å€¼è¡¨ç¤ºæ€§èƒ½æå‡ï¼Œè´Ÿå€¼è¡¨ç¤ºæ€§èƒ½ä¸‹é™', fontweight='bold')
    else:
        ax.set_xlabel('Performance Metrics')
        ax.set_ylabel('Performance Improvement (%)')
        ax.set_title('Performance Improvement Comparison (vs Complete Model)\nPositive values indicate improvement, negative values indicate degradation', fontweight='bold')
    
    ax.set_xticks(x)
    ax.set_xticklabels(metrics)
    ax.legend()
    ax.grid(True, alpha=0.3)
    ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    change_chart_file = benchmark_dir / f"performance_improvement_{timestamp}.png"
    plt.savefig(change_chart_file, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"   - æ€§èƒ½æ”¹è¿›å›¾: benchmark/performance_improvement_{timestamp}.png")


def main(
    model_path: str = "/home/zmx/models/Llama/layerskip-llama2-7B",
    device: str = "cuda:0",
    temperature: float = DEFAULT_TEMPERATURE,
    top_p: float = DEFAULT_TOP_P,
    do_sample: bool = DEFAULT_DO_SAMPLE,
    max_new_tokens: int = DEFAULT_MAX_NEW_TOKENS,
    benchmark: bool = False,
    config_file: str = "configs/strategies_config.json"
):
    tokenizer = LlamaTokenizer.from_pretrained(model_path)
    
    # è¿è¡Œå®Œæ•´æ¨ç†åŸºå‡†æµ‹è¯•
    baseline_result = benchmark_baseline_inference(
        model_path=model_path,
        tokenizer=tokenizer,
        device=device,
        temperature=temperature,
        top_p=top_p,
        do_sample=do_sample,
        max_new_tokens=max_new_tokens
    )
    
    # ä»JSONé…ç½®æ–‡ä»¶åŠ è½½åˆ†å±‚ç­–ç•¥åˆ—è¡¨
    try:
        strategies_to_test = load_strategies_from_config(config_file=config_file, device=device)
    except Exception as e:
        print(f"é”™è¯¯: åŠ è½½ç­–ç•¥é…ç½®å¤±è´¥: {e}")
        print("è¯·æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”æ ¼å¼æ­£ç¡®")
        return

    # è¿è¡Œåˆ†å±‚æ¨ç†åŸºå‡†æµ‹è¯•
    partition_results = benchmark_partition_inference(
        strategies_to_test=strategies_to_test,
        model_path=model_path,
        tokenizer=tokenizer,
        device=device,
        temperature=temperature,
        top_p=top_p,
        do_sample=do_sample,
        max_new_tokens=max_new_tokens
    )
    
    # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
    create_comparison_table(baseline_result, partition_results, benchmark=benchmark)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Llamaæ¨¡å‹åˆ†å±‚åˆ†å‰²æ¨ç†æ€§èƒ½æµ‹è¯•")
    parser.add_argument("--model_path", type=str, 
                       default="/home/zmx/models/Llama/layerskip-llama2-7B",
                       help="æ¨¡å‹è·¯å¾„")
    parser.add_argument("--device", type=str, default="cuda:0",
                       help="è®¾å¤‡åç§°")
    parser.add_argument("--temperature", type=float, default=DEFAULT_TEMPERATURE,
                       help=f"æ¸©åº¦å‚æ•° (é»˜è®¤: {DEFAULT_TEMPERATURE})")
    parser.add_argument("--top_p", type=float, default=DEFAULT_TOP_P,
                       help=f"top-pé‡‡æ ·å‚æ•° (é»˜è®¤: {DEFAULT_TOP_P})")
    parser.add_argument("--do_sample", action="store_true", default=DEFAULT_DO_SAMPLE,
                       help=f"æ˜¯å¦å¯ç”¨é‡‡æ · (é»˜è®¤: {DEFAULT_DO_SAMPLE})")
    parser.add_argument("--no_sample", dest="do_sample", action="store_false",
                       help="ç¦ç”¨é‡‡æ ·ï¼Œä½¿ç”¨è´ªå©ªæœç´¢")
    parser.add_argument("--max_new_tokens", type=int, default=DEFAULT_MAX_NEW_TOKENS,
                       help=f"æœ€å¤§ç”Ÿæˆtokenæ•° (é»˜è®¤: {DEFAULT_MAX_NEW_TOKENS})")
    parser.add_argument("--benchmark", action="store_true", default=False,
                       help="æ˜¯å¦ç”Ÿæˆbenchmarkå›¾è¡¨å’Œè¯¦ç»†åˆ†æï¼ˆé»˜è®¤: Falseï¼‰")
    parser.add_argument("--config_file", type=str, default="configs/strategies_config.json",
                       help="ç­–ç•¥é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: configs/strategies_config.jsonï¼‰")
    
    args = parser.parse_args()
    
    # è¿è¡Œæµ‹è¯•å¹¶è‡ªåŠ¨ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼å’Œå›¾ç‰‡
    main(
        model_path=args.model_path,
        device=args.device,
        temperature=args.temperature,
        top_p=args.top_p,
        do_sample=args.do_sample,
        max_new_tokens=args.max_new_tokens,
        benchmark=args.benchmark,
        config_file=args.config_file
    )

