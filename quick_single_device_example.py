#!/usr/bin/env python3
"""
å•è®¾å¤‡åˆ†å±‚æ¨ç†å®é™…æµ‹è¯•

ä½¿ç”¨Llama-2-7Bæ¨¡å‹è¿›è¡Œå®é™…çš„å•è®¾å¤‡åˆ†å±‚æ¨ç†æµ‹è¯•ã€‚
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨SINGLE_DEVICEç­–ç•¥è¿›è¡Œæ¨¡å‹åˆ†å±‚å’Œæ¨ç†ã€‚
"""

import torch
import sys
import time
from pathlib import Path
from transformers import LlamaTokenizer

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from llamadist import (
    PartitionStrategy, 
    LlamaPartitioner, 
    SingleDeviceInference, 
    StrategyType
)
from llamadist.inference.coordinator import GenerationConfig


def test_real_model():
    """ä½¿ç”¨çœŸå®æ¨¡å‹è¿›è¡Œå•è®¾å¤‡åˆ†å±‚æ¨ç†æµ‹è¯•"""
    print("ğŸ” LlamaDistributor - Llama-2-7B å•è®¾å¤‡åˆ†å±‚æ¨ç†æµ‹è¯•")
    print("=" * 60)
    
    # æ¨¡å‹è·¯å¾„
    model_path = "/home/zmx/models/Llama/Llama-2-7b-hf"
    
    # æ£€æŸ¥è®¾å¤‡
    device = "cuda:0" if torch.cuda.is_available() else "cpu"
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    print(f"æ¨¡å‹è·¯å¾„: {model_path}")
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
    if not Path(model_path).exists():
        print(f"âŒ é”™è¯¯ï¼šæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        print("è¯·ç¡®ä¿Llama-2-7Bæ¨¡å‹å·²ä¸‹è½½åˆ°æŒ‡å®šè·¯å¾„")
        return False
    
    try:
        print("\n" + "="*60)
        print("ç¬¬ä¸€é˜¶æ®µï¼šæ¨¡å‹åŠ è½½å’Œåˆ†æ")
        print("="*60)
        
        # 1. åŠ è½½åˆ†è¯å™¨
        print("\n1. åŠ è½½åˆ†è¯å™¨...")
        tokenizer = LlamaTokenizer.from_pretrained(model_path)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        print(f"   è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
        
        # 2. åˆ›å»ºåˆ†å±‚å™¨å¹¶åˆ†ææ¨¡å‹
        print("\n2. åˆ›å»ºåˆ†å±‚å™¨å¹¶åˆ†ææ¨¡å‹...")
        partitioner = LlamaPartitioner(model_path=model_path)
        
        print("   åˆ†ææ¨¡å‹ç»“æ„...")
        model_info = partitioner.analyze_model(detailed=False)
        print(f"   æ¨¡å‹å±‚æ•°: {model_info.num_layers}")
        print(f"   éšè—ç»´åº¦: {model_info.hidden_size}")
        print(f"   æ€»å‚æ•°: {model_info.total_params:,}")
        print(f"   ä¼°è®¡å†…å­˜: {model_info.total_memory / (1024**3):.2f} GB")
        
        print("\n" + "="*60)
        print("ç¬¬äºŒé˜¶æ®µï¼šæµ‹è¯•ä¸åŒåˆ†å±‚ç­–ç•¥")
        print("="*60)
        
        # æµ‹è¯•ä¸åŒçš„åˆ†å±‚ç­–ç•¥
        strategies_to_test = [
            {
                "name": "3åˆ†å±‚-å‡åŒ€",
                "strategy": PartitionStrategy(
                    num_partitions=3,
                    strategy_type=StrategyType.SINGLE_DEVICE,
                    single_device=device
                )
            },
            {
                "name": "3åˆ†å±‚-è‡ªå®šä¹‰",
                "strategy": PartitionStrategy(
                    num_partitions=3,
                    strategy_type=StrategyType.SINGLE_DEVICE,
                    single_device=device,
                    custom_boundaries=[(0, 10), (11, 21), (22, 31)]
                )
            },
            {
                "name": "4åˆ†å±‚-å‡åŒ€",
                "strategy": PartitionStrategy(
                    num_partitions=4,
                    strategy_type=StrategyType.SINGLE_DEVICE,
                    single_device=device
                )
            }
        ]
        
        results = []
        
        for test_config in strategies_to_test:
            print(f"\nğŸ§ª æµ‹è¯•ç­–ç•¥: {test_config['name']}")
            print("-" * 40)
            
            try:
                # åˆ›å»ºåˆ†å±‚
                print("   åˆ›å»ºåˆ†å±‚é…ç½®...")
                partitions = test_config['strategy'].create_partitions(model_info)
                
                print("   åˆ†å±‚è¯¦æƒ…:")
                for i, partition in enumerate(partitions):
                    layer_count = partition.layer_end - partition.layer_start + 1
                    print(f"     åˆ†å±‚ {i}: å±‚{partition.layer_start}-{partition.layer_end} ({layer_count}å±‚) @ {partition.device}")
                
                # æ‰§è¡Œåˆ†å±‚
                print("   æ‰§è¡Œæ¨¡å‹åˆ†å±‚...")
                start_time = time.time()
                submodels = partitioner.partition(
                    strategy=test_config['strategy'],
                    copy_weights=True
                )
                partition_time = time.time() - start_time
                print(f"   åˆ†å±‚å®Œæˆï¼Œè€—æ—¶: {partition_time:.2f}ç§’")
                
                # åˆ›å»ºæ¨ç†å¼•æ“
                print("   åˆ›å»ºæ¨ç†å¼•æ“...")
                inference_engine = SingleDeviceInference(
                    submodels=submodels,
                    generation_config=GenerationConfig(
                        max_new_tokens=30,
                        temperature=0.8,
                        top_p=0.9,
                        do_sample=True,
                        use_cache=True,
                        eos_token_id=tokenizer.eos_token_id,
                        pad_token_id=tokenizer.pad_token_id
                    ),
                    device=device
                )
                
                # æµ‹è¯•æ¨ç†æ€§èƒ½
                print("   æµ‹è¯•æ¨ç†æ€§èƒ½...")
                test_prompts = [
                    "The future of artificial intelligence is",
                    "In a world where technology advances rapidly,"
                ]
                
                total_inference_time = 0
                total_tokens = 0
                
                for prompt in test_prompts:
                    # ç¼–ç è¾“å…¥
                    input_ids = tokenizer.encode(prompt, return_tensors="pt", add_special_tokens=True)
                    
                    # æµ‹è¯•å‰å‘ä¼ æ’­
                    start_time = time.time()
                    with torch.no_grad():
                        result = inference_engine.forward_pass(input_ids, use_cache=True)
                    inference_time = time.time() - start_time
                    
                    # æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ
                    start_time = time.time()
                    generated_text = inference_engine.generate_text(
                        prompt=prompt,
                        tokenizer=tokenizer,
                        return_full_text=False
                    )
                    generation_time = time.time() - start_time
                    
                    total_inference_time += inference_time
                    generated_tokens = len(tokenizer.encode(generated_text))
                    total_tokens += generated_tokens
                    
                    print(f"     æç¤º: {prompt}")
                    print(f"     ç”Ÿæˆ: {generated_text[:50]}{'...' if len(generated_text) > 50 else ''}")
                    print(f"     æ¨ç†æ—¶é—´: {inference_time:.4f}ç§’, ç”Ÿæˆæ—¶é—´: {generation_time:.4f}ç§’")
                
                # è·å–ç»Ÿè®¡ä¿¡æ¯
                stats = inference_engine.get_stats()
                avg_inference_time = total_inference_time / len(test_prompts)
                
                results.append({
                    "name": test_config['name'],
                    "num_partitions": len(submodels),
                    "partition_time": partition_time,
                    "avg_inference_time": avg_inference_time,
                    "total_tokens": total_tokens,
                    "tokens_per_second": stats.get('tokens_per_second', 0)
                })
                
                print(f"   âœ… æµ‹è¯•å®Œæˆ")
                
                # æ¸…ç†å†…å­˜
                del submodels
                del inference_engine
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
            except Exception as e:
                print(f"   âŒ æµ‹è¯•å¤±è´¥: {e}")
                results.append({
                    "name": test_config['name'],
                    "error": str(e)
                })
        
        print("\n" + "="*60)
        print("ç¬¬ä¸‰é˜¶æ®µï¼šæ€§èƒ½æ¯”è¾ƒç»“æœ")
        print("="*60)
        
        # æ˜¾ç¤ºç»“æœæ¯”è¾ƒ
        print(f"{'ç­–ç•¥åç§°':<15} {'åˆ†å±‚æ•°':<8} {'åˆ†å±‚æ—¶é—´':<12} {'æ¨ç†æ—¶é—´':<12} {'ç”Ÿæˆé€Ÿåº¦':<12}")
        print("-" * 70)
        
        for result in results:
            if 'error' not in result:
                print(f"{result['name']:<15} {result['num_partitions']:<8} "
                      f"{result['partition_time']:.2f}s{'':<6} {result['avg_inference_time']:.4f}s{'':<4} "
                      f"{result['tokens_per_second']:.1f}t/s{'':<4}")
            else:
                print(f"{result['name']:<15} {'å¤±è´¥':<8} {result['error'][:40]}")
        
        # æ‰¾å‡ºæœ€ä½³ç­–ç•¥
        successful_results = [r for r in results if 'error' not in r]
        if successful_results:
            best_strategy = min(successful_results, key=lambda x: x['avg_inference_time'])
            print(f"\nğŸ† æœ€ä½³ç­–ç•¥: {best_strategy['name']} (æ¨ç†æ—¶é—´: {best_strategy['avg_inference_time']:.4f}ç§’)")
        
        print("\n" + "="*60)
        print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
        print("="*60)
        
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_memory_usage():
    """æµ‹è¯•ä¸åŒåˆ†å±‚ç­–ç•¥çš„å†…å­˜ä½¿ç”¨"""
    print("\n" + "="*60)
    print("å†…å­˜ä½¿ç”¨åˆ†æ")
    print("="*60)
    
    if not torch.cuda.is_available():
        print("CUDAä¸å¯ç”¨ï¼Œè·³è¿‡å†…å­˜åˆ†æ")
        return
    
    device = "cuda:0"
    model_path = "/home/zmx/models/Llama/Llama-2-7b-hf"
    
    try:
        # è®°å½•åˆå§‹å†…å­˜
        torch.cuda.empty_cache()
        initial_memory = torch.cuda.memory_allocated(device)
        print(f"åˆå§‹GPUå†…å­˜ä½¿ç”¨: {initial_memory / (1024**2):.1f} MB")
        
        # åˆ›å»ºåˆ†å±‚å™¨
        partitioner = LlamaPartitioner(model_path=model_path)
        model_info = partitioner.analyze_model(detailed=False)
        
        # æµ‹è¯•2åˆ†å±‚ç­–ç•¥çš„å†…å­˜ä½¿ç”¨
        strategy = PartitionStrategy(
            num_partitions=2,
            strategy_type=StrategyType.SINGLE_DEVICE,
            single_device=device
        )
        
        print("\nåˆ›å»º2åˆ†å±‚æ¨¡å‹...")
        submodels = partitioner.partition(strategy=strategy, copy_weights=True)
        
        # è®°å½•åˆ†å±‚åå†…å­˜
        partitioned_memory = torch.cuda.memory_allocated(device)
        print(f"åˆ†å±‚åGPUå†…å­˜ä½¿ç”¨: {partitioned_memory / (1024**2):.1f} MB")
        print(f"å†…å­˜å¢é‡: {(partitioned_memory - initial_memory) / (1024**2):.1f} MB")
        
        # åˆ›å»ºæ¨ç†å¼•æ“å¹¶æµ‹è¯•
        inference_engine = SingleDeviceInference(submodels=submodels, device=device)
        
        # æ¨¡æ‹Ÿæ¨ç†
        test_input = torch.tensor([[1, 2, 3, 4, 5]], device=device)
        with torch.no_grad():
            _ = inference_engine.forward_pass(test_input)
        
        # è®°å½•æ¨ç†åå†…å­˜
        inference_memory = torch.cuda.memory_allocated(device)
        print(f"æ¨ç†åGPUå†…å­˜ä½¿ç”¨: {inference_memory / (1024**2):.1f} MB")
        
        # æ˜¾ç¤ºå†…å­˜ä½¿ç”¨è¯¦æƒ…
        print(f"\nå†…å­˜ä½¿ç”¨è¯¦æƒ…:")
        print(f"  åŸºç¡€å†…å­˜: {initial_memory / (1024**2):.1f} MB")
        print(f"  æ¨¡å‹åˆ†å±‚: +{(partitioned_memory - initial_memory) / (1024**2):.1f} MB")
        print(f"  æ¨ç†ç¼“å­˜: +{(inference_memory - partitioned_memory) / (1024**2):.1f} MB")
        print(f"  æ€»è®¡ä½¿ç”¨: {inference_memory / (1024**2):.1f} MB")
        
        # æ¸…ç†
        del submodels
        del inference_engine
        torch.cuda.empty_cache()
        
        final_memory = torch.cuda.memory_allocated(device)
        print(f"æ¸…ç†åGPUå†…å­˜: {final_memory / (1024**2):.1f} MB")
        
    except Exception as e:
        print(f"å†…å­˜åˆ†æå¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    print("è¯·é€‰æ‹©æµ‹è¯•æ¨¡å¼:")
    print("1. å®Œæ•´æ¨¡å‹æµ‹è¯•ï¼ˆæ¨èï¼‰")
    print("2. å†…å­˜ä½¿ç”¨åˆ†æ")
    print("3. å…¨éƒ¨æµ‹è¯•")
    
    try:
        choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1-3): ").strip()
        
        if choice == "1":
            success = test_real_model()
        elif choice == "2":
            test_memory_usage()
            success = True
        elif choice == "3":
            success = test_real_model()
            if success:
                test_memory_usage()
        else:
            print("æ— æ•ˆé€‰æ‹©ï¼Œè¿è¡Œå®Œæ•´æµ‹è¯•...")
            success = test_real_model()
            
        if success:
            print("\nğŸ‰ æµ‹è¯•å®Œæˆï¼å•è®¾å¤‡åˆ†å±‚æ¨ç†åŠŸèƒ½æ­£å¸¸å·¥ä½œã€‚")
        else:
            print("\nâŒ æµ‹è¯•æœªå®Œå…¨æˆåŠŸï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯ã€‚")
            
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")


if __name__ == "__main__":
    main() 