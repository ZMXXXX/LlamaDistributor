#!/usr/bin/env python3
"""
LlamaDistributor ç®€å•æ–‡æœ¬ç”Ÿæˆæ¼”ç¤º

å±•ç¤ºå¦‚ä½•ä½¿ç”¨åˆ†å±‚æŽ¨ç†æ¥ç”Ÿæˆæ–‡æœ¬å›žç­”
"""

import torch
from transformers import AutoTokenizer

from llamadist.partitioner.analyzer import LlamaModelAnalyzer
from llamadist.partitioner.strategies import PartitionStrategy
from llamadist.partitioner.splitter import LlamaPartitioner
from llamadist.inference.coordinator import DistributedInference, GenerationConfig

def simple_generate_text(prompt: str, model_path: str = "/home/zmx/models/Llama/Llama-2-7b-hf"):
    """
    ç®€å•çš„æ–‡æœ¬ç”Ÿæˆå‡½æ•°
    
    Args:
        prompt: è¾“å…¥æç¤º
        model_path: æ¨¡åž‹è·¯å¾„
    
    Returns:
        str: ç”Ÿæˆçš„æ–‡æœ¬
    """
    print(f"ðŸš€ å¼€å§‹åˆ†å¸ƒå¼æ–‡æœ¬ç”Ÿæˆ")
    print(f"ðŸ“ è¾“å…¥æç¤º: '{prompt}'")
    print("=" * 60)
    
    # é…ç½®
    num_partitions = 2
    devices = ["cuda:0", "cuda:1"] if torch.cuda.device_count() >= 2 else ["cpu", "cpu"]
    
    print(f"ðŸ’» ä½¿ç”¨è®¾å¤‡: {devices}")
    
    # 1. åˆ›å»ºåˆ†å±‚ç­–ç•¥
    print("1ï¸âƒ£ åˆ›å»ºåˆ†å±‚ç­–ç•¥...")
    strategy = PartitionStrategy(
        num_partitions=num_partitions,
        strategy_type="uniform",
        target_devices=devices
    )
    
    # 2. åˆ†å±‚æ¨¡åž‹
    print("2ï¸âƒ£ åˆ†å±‚æ¨¡åž‹...")
    partitioner = LlamaPartitioner(model_path=model_path)
    submodels = partitioner.partition(
        strategy=strategy,
        analyze_first=False,
        copy_weights=True
    )
    
    # 3. åˆ›å»ºåˆ†å¸ƒå¼æŽ¨ç†å¼•æ“Ž
    print("3ï¸âƒ£ åˆ›å»ºåˆ†å¸ƒå¼æŽ¨ç†å¼•æ“Ž...")
    inference_engine = DistributedInference(
        submodels=submodels,
        generation_config=GenerationConfig(
            max_new_tokens=20,
            temperature=0.7,
            do_sample=True,
            top_p=0.9,
            use_cache=True
        )
    )
    
    # 4. åŠ è½½åˆ†è¯å™¨
    print("4ï¸âƒ£ åŠ è½½åˆ†è¯å™¨...")
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # è®¾ç½®ç”Ÿæˆé…ç½®ä¸­çš„ç‰¹æ®Štoken
    inference_engine.generation_config.eos_token_id = tokenizer.eos_token_id
    inference_engine.generation_config.pad_token_id = tokenizer.pad_token_id
    
    # 5. ç”Ÿæˆæ–‡æœ¬
    print("5ï¸âƒ£ å¼€å§‹ç”Ÿæˆ...")
    
    # ç¼–ç è¾“å…¥
    input_ids = tokenizer.encode(prompt, return_tensors='pt')
    print(f"   è¾“å…¥tokenæ•°é‡: {input_ids.shape[1]}")
    
    # ç¡®ä¿è¾“å…¥åœ¨ç¬¬ä¸€ä¸ªå­æ¨¡åž‹çš„è®¾å¤‡ä¸Š
    first_device = submodels[0].get_info()['device']
    input_ids = input_ids.to(first_device)
    
    # é€æ­¥ç”Ÿæˆæ–‡æœ¬ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œä¸ä½¿ç”¨KVç¼“å­˜ï¼‰
    generated_tokens = []
    all_input_ids = input_ids
    
    max_new_tokens = 10
    for step in range(max_new_tokens):
        print(f"   ç”Ÿæˆæ­¥éª¤ {step + 1}...")
        
        # æ‰§è¡Œå‰å‘ä¼ æ’­ï¼ˆä¸ä½¿ç”¨ç¼“å­˜ï¼Œæ¯æ¬¡éƒ½ä¼ é€’å®Œæ•´åºåˆ—ï¼‰
        result = inference_engine.forward_pass(
            input_ids=all_input_ids,
            past_key_values=None,
            use_cache=False
        )
        
        # èŽ·å–ä¸‹ä¸€ä¸ªtokençš„logits
        next_token_logits = result['logits'][0, -1, :]
        
        # åº”ç”¨æ¸©åº¦
        next_token_logits = next_token_logits / 0.7
        
        # ç®€å•é‡‡æ ·ï¼ˆä½¿ç”¨top-kï¼‰
        top_k = 50
        if top_k > 0:
            indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]
            next_token_logits[indices_to_remove] = float('-inf')
        
        # é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
        probs = torch.softmax(next_token_logits, dim=-1)
        next_token = torch.multinomial(probs, num_samples=1)
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯EOS token
        if next_token.item() == tokenizer.eos_token_id:
            print("   é‡åˆ°EOS tokenï¼Œåœæ­¢ç”Ÿæˆ")
            break
        
        # è§£ç token
        next_token_text = tokenizer.decode(next_token, skip_special_tokens=True)
        generated_tokens.append(next_token.item())
        print(f"   ç”Ÿæˆtoken: '{next_token_text}' (ID: {next_token.item()})")
        
        # å°†æ–°tokenæ·»åŠ åˆ°åºåˆ—ä¸­ï¼ˆç¡®ä¿åœ¨åŒä¸€è®¾å¤‡ä¸Šï¼‰
        next_token_device = next_token.to(first_device)
        all_input_ids = torch.cat([all_input_ids, next_token_device.unsqueeze(0)], dim=1)
    
    # è§£ç å®Œæ•´ç”Ÿæˆçš„æ–‡æœ¬
    generated_text = tokenizer.decode(all_input_ids[0], skip_special_tokens=True)
    
    print("=" * 60)
    print(f"âœ… ç”Ÿæˆå®Œæˆï¼")
    print(f"ðŸ“„ å®Œæ•´å›žç­”: {generated_text}")
    
    return generated_text

def test_qa_examples():
    """æµ‹è¯•é—®ç­”ç¤ºä¾‹"""
    examples = [
        "What is the capital of France?",
        "How does machine learning work?",
        "Tell me a joke about programming.",
    ]
    
    print("ðŸ§ª æµ‹è¯•å¤šä¸ªé—®ç­”ç¤ºä¾‹")
    print("=" * 60)
    
    for i, question in enumerate(examples):
        print(f"\nðŸ“‹ ç¤ºä¾‹ {i+1}: {question}")
        print("-" * 40)
        
        try:
            answer = simple_generate_text(question)
            print(f"âœ… æˆåŠŸç”Ÿæˆå›žç­”")
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()

if __name__ == "__main__":
    # æµ‹è¯•å•ä¸ªç¤ºä¾‹
    prompt = "The future of artificial intelligence is"
    try:
        result = simple_generate_text(prompt)
        print("\n" + "=" * 60)
        print("ðŸŽ‰ å•ä¸ªç¤ºä¾‹æµ‹è¯•æˆåŠŸï¼")
        
        # æµ‹è¯•å¤šä¸ªç¤ºä¾‹
        print("\n")
        test_qa_examples()
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc() 