"""
å•è®¾å¤‡åˆ†å±‚æ¨ç†åè°ƒå™¨

ä¸“é—¨å¤„ç†åœ¨åŒä¸€è®¾å¤‡ä¸Šçš„åˆ†å±‚æ¨ç†ï¼Œä¸æ¶‰åŠè·¨è®¾å¤‡é€šä¿¡ã€‚
é€‚ç”¨äºæµ‹è¯•åˆ†å±‚æ•ˆæœã€å†…å­˜ä¼˜åŒ–å’Œè°ƒè¯•åˆ†å±‚é€»è¾‘ã€‚
"""

import torch
import torch.nn.functional as F
from typing import List, Dict, Optional, Tuple, Any, Union
import time
import copy
from dataclasses import dataclass

# ä½¿ç”¨å†…ç½®çš„åˆ†å±‚å™¨æ¨¡å—
from ..partitioner.splitter import LlamaSubModel
from .coordinator import GenerationConfig, InferenceState


class SingleDeviceInference:
    """
    å•è®¾å¤‡åˆ†å±‚æ¨ç†å¼•æ“
    
    ä¸“é—¨å¤„ç†åœ¨åŒä¸€è®¾å¤‡ä¸Šçš„å¤šä¸ªå­æ¨¡å‹æ¨ç†ï¼Œç‰¹ç‚¹ï¼š
    1. æ‰€æœ‰å­æ¨¡å‹éƒ½åœ¨åŒä¸€è®¾å¤‡ä¸Š
    2. ä¸éœ€è¦è·¨è®¾å¤‡çŠ¶æ€ä¼ é€’
    3. ç®€åŒ–çš„ç¼“å­˜ç®¡ç†
    4. æ›´é«˜æ•ˆçš„å†…å­˜åˆ©ç”¨
    5. ä¾¿äºè°ƒè¯•å’Œåˆ†æ
    """
    
    def __init__(
        self,
        submodels: List[LlamaSubModel],
        generation_config: Optional[GenerationConfig] = None,
        device: Optional[str] = None
    ):
        """
        åˆå§‹åŒ–å•è®¾å¤‡åˆ†å±‚æ¨ç†å¼•æ“
        
        Args:
            submodels: å­æ¨¡å‹åˆ—è¡¨ï¼ˆæŒ‰åˆ†å±‚é¡ºåºï¼‰
            generation_config: ç”Ÿæˆé…ç½®
            device: ç›®æ ‡è®¾å¤‡ï¼ˆå¦‚æœä¸ºNoneï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå­æ¨¡å‹çš„è®¾å¤‡ï¼‰
        """
        self.submodels = submodels
        self.generation_config = generation_config or GenerationConfig()
        
        # éªŒè¯å­æ¨¡å‹
        self._validate_submodels()
        
        # ç¡®å®šè®¾å¤‡
        self.device = device or self.submodels[0].get_info()['device']
        
        # ç¡®ä¿æ‰€æœ‰å­æ¨¡å‹åœ¨åŒä¸€è®¾å¤‡ä¸Š
        self._ensure_same_device()
        
        # Early-exit submodelç¼“å­˜
        self._early_exit_cache = {}  # æ ¼å¼: {submodel_idx: prepared_submodel}
        self._original_lm_head_weights = None  # ä¿å­˜åŸå§‹lm_headæƒé‡ç”¨äºå¤åˆ¶
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            'total_time_to_first_token': 0.0,
            'token_decode_time': 0.0,
            'total_generation_time': 0.0,
            'layer_processing_time': {},  # æ¯å±‚çš„å¤„ç†æ—¶é—´
            'memory_usage': {},           # æ¯å±‚çš„å†…å­˜ä½¿ç”¨
            'total_tokens_generated': 0,
            'inference_count': 0
        }
        
        print(f"å•è®¾å¤‡åˆ†å±‚æ¨ç†å¼•æ“åˆå§‹åŒ–å®Œæˆï¼Œ{len(self.submodels)}ä¸ªå­æ¨¡å‹ @ {self.device}")
        for sm in self.submodels:
            info = sm.get_info()
            print(f"  åˆ†å±‚ {info['partition_idx']}: å±‚{info['layer_start']}-{info['layer_end']}")
    
    def set_original_lm_head_weights(self, lm_head_weights: torch.Tensor):
        """
        è®¾ç½®åŸå§‹æ¨¡å‹çš„lm_headæƒé‡ï¼Œç”¨äºearly-exit submodel
        
        Args:
            lm_head_weights: åŸå§‹æ¨¡å‹çš„lm_headæƒé‡
        """
        self._original_lm_head_weights = lm_head_weights.clone().to(self.device)
    
    def _validate_submodels(self):
        """éªŒè¯å­æ¨¡å‹çš„æœ‰æ•ˆæ€§"""
        if not self.submodels:
            raise ValueError("å­æ¨¡å‹åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ªåˆ†å±‚
        first_partitions = [sm for sm in self.submodels if sm.is_first_partition]
        last_partitions = [sm for sm in self.submodels if sm.is_last_partition]
        
        if len(first_partitions) != 1:
            raise ValueError("å¿…é¡»æœ‰ä¸”ä»…æœ‰ä¸€ä¸ªç¬¬ä¸€åˆ†å±‚")
        
        if len(last_partitions) != 1:
            raise ValueError("å¿…é¡»æœ‰ä¸”ä»…æœ‰ä¸€ä¸ªæœ€ååˆ†å±‚")
        
        # æŒ‰åˆ†å±‚ç´¢å¼•æ’åº
        self.submodels.sort(key=lambda x: x.partition_idx)
    
    def _ensure_same_device(self):
        """ç¡®ä¿æ‰€æœ‰å­æ¨¡å‹åœ¨åŒä¸€è®¾å¤‡ä¸Š"""
        target_device = self.device
        
        for submodel in self.submodels:
            current_device = submodel.get_info()['device']
            if current_device != target_device:
                print(f"å°†å­æ¨¡å‹ä» {current_device} ç§»åŠ¨åˆ° {target_device}")
                submodel.to(target_device)
    
    def forward_pass(
        self,
        input_ids: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        past_key_values: Optional[List[torch.FloatTensor]] = None,
        use_cache: bool = True,
        exit_position: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œå•è®¾å¤‡åˆ†å±‚submodelsæ•´ä½“å‰å‘ä¼ æ’­æµç¨‹
        
        Args:
            input_ids: è¾“å…¥token ID
            attention_mask: æ³¨æ„åŠ›æ©ç 
            past_key_values: è¿‡å»çš„é”®å€¼å¯¹
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            exit_position: Early-exitä½ç½®ï¼ˆæŒ‡å®šåœ¨å“ªä¸ªsubmodelç»“æŸæ—¶é€€å‡ºï¼‰
        
        Returns:
            Dict: åŒ…å«logitsã€hidden_stateså’Œpast_key_valuesçš„å­—å…¸
        """
        # å°†è¾“å…¥ç§»åŠ¨åˆ°ç›®æ ‡è®¾å¤‡
        input_ids = input_ids.to(self.device)
        if attention_mask is not None:
            attention_mask = attention_mask.to(self.device)
        
        # ç§»åŠ¨KVç¼“å­˜åˆ°ç›®æ ‡è®¾å¤‡
        if past_key_values is not None:
            past_key_values = self._transfer_kv_cache_to_device(past_key_values)
        
        # åˆå§‹åŒ–æ¨ç†çŠ¶æ€
        current_hidden_states = None
        current_past_key_values = past_key_values
        new_cache_list = []
        
        # è®°å½•æ¯å±‚çš„å¤„ç†æ—¶é—´å’Œå†…å­˜ä½¿ç”¨
        layer_stats = {}
        early_exit_submodel = None  # ä¿å­˜early-exitæ—¶çš„æœ€åä¸€ä¸ªsubmodel
        
        # é€šè¿‡æ¯ä¸ªå­æ¨¡å‹è¿›è¡Œæ¨ç†
        for i, submodel in enumerate(self.submodels):
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢ï¼ˆearly-exitï¼‰
            # exit_position = n è¡¨ç¤ºæ‰§è¡Œå‰nä¸ªsubmodelï¼ˆç´¢å¼•0åˆ°n-1ï¼‰
            if exit_position is not None and i >= exit_position:
                break
            
            # æ£€æŸ¥è¿™æ˜¯å¦æ˜¯early-exitçš„æœ€åä¸€ä¸ªsubmodel
            is_early_exit_last = (exit_position is not None and i == exit_position - 1)
            
            # å¦‚æœæ˜¯early-exitçš„æœ€åä¸€ä¸ªsubmodelä¸”å½“å‰ä¸æ˜¯æœ€åä¸€ä¸ªåˆ†å±‚ï¼Œåˆ™è·å–ç¼“å­˜çš„early-exit submodel
            if is_early_exit_last and not submodel.is_last_partition:
                submodel = self._get_early_exit_submodel(i)
                early_exit_submodel = submodel  # ä¿å­˜å‡†å¤‡å¥½çš„submodel
            layer_start_time = time.time()
            
            # è®°å½•å¤„ç†å‰çš„å†…å­˜ä½¿ç”¨
            if torch.cuda.is_available() and self.device.startswith('cuda'):
                torch.cuda.synchronize()
                memory_before = torch.cuda.memory_allocated(self.device)
            else:
                memory_before = 0
            
            if submodel.is_first_partition:
                # ç¬¬ä¸€ä¸ªåˆ†å±‚ä½¿ç”¨input_ids
                current_input_ids = input_ids
                current_attention_mask = attention_mask
                
                if current_past_key_values is not None:
                    # åœ¨æœ‰ç¼“å­˜æ—¶ï¼Œåªä½¿ç”¨æœ€åä¸€ä¸ªtoken
                    current_input_ids = input_ids[:, -1:]
                    # attention_maskéœ€è¦è¦†ç›–å®Œæ•´åºåˆ—ï¼ˆåŒ…æ‹¬è¿‡å»çš„tokensï¼‰
                    if attention_mask is not None:
                        batch_size = input_ids.shape[0]
                        seq_length = current_input_ids.shape[1]
                        
                        # è®¡ç®—è¿‡å»çš„åºåˆ—é•¿åº¦
                        past_length = 0
                        if current_past_key_values is not None:
                            for pkv in current_past_key_values:
                                if pkv is not None and len(pkv) > 0 and pkv[0] is not None:
                                    past_length = pkv[0].shape[2]
                                    break
                        
                        # åˆ›å»ºå®Œæ•´çš„attention_mask
                        total_length = past_length + seq_length
                        current_attention_mask = torch.ones(
                            (batch_size, total_length),
                            dtype=attention_mask.dtype,
                            device=self.device
                        )
                
                model_input = {
                    'input_ids': current_input_ids,
                    'attention_mask': current_attention_mask,
                    'past_key_values': self._extract_relevant_kv_cache(current_past_key_values, submodel.layer_start, submodel.layer_end),
                    'use_cache': use_cache
                }
            else:
                # åç»­åˆ†å±‚ä½¿ç”¨hidden_states
                if current_hidden_states is None:
                    raise RuntimeError(f"å­æ¨¡å‹ {i} éœ€è¦hidden_statesï¼Œä½†å‰ä¸€ä¸ªå­æ¨¡å‹æ²¡æœ‰æä¾›")
                
                model_input = {
                    'hidden_states': current_hidden_states,
                    'attention_mask': attention_mask,
                    'past_key_values': self._extract_relevant_kv_cache(current_past_key_values, submodel.layer_start, submodel.layer_end),
                    'use_cache': use_cache
                }
            
            # æ‰§è¡Œå­æ¨¡å‹æ¨ç†
            with torch.no_grad():
                output = submodel(**model_input)
            
            # æ›´æ–°éšè—çŠ¶æ€
            current_hidden_states = output['hidden_states']
            
            # æ›´æ–°KVç¼“å­˜
            if use_cache and 'past_key_values' in output and output['past_key_values'] is not None:
                # å°†å­æ¨¡å‹çš„ç¼“å­˜åˆå¹¶åˆ°å…¨å±€ç¼“å­˜
                current_past_key_values = self._merge_kv_cache(
                    current_past_key_values,
                    output['past_key_values'],
                    submodel.layer_start,
                    submodel.layer_end
                )
            
            # è®°å½•å±‚å¤„ç†æ—¶é—´å’Œå†…å­˜ä½¿ç”¨
            layer_end_time = time.time()
            layer_time = layer_end_time - layer_start_time
            
            if torch.cuda.is_available() and self.device.startswith('cuda'):
                torch.cuda.synchronize()
                memory_after = torch.cuda.memory_allocated(self.device)
                memory_delta = memory_after - memory_before
            else:
                memory_delta = 0
            
            layer_info = submodel.get_info()
            layer_key = f"partition_{i}_layers_{layer_info['layer_start']}-{layer_info['layer_end']}"
            layer_stats[layer_key] = {
                'processing_time': layer_time,
                'memory_delta': memory_delta,
                'memory_after': memory_after if torch.cuda.is_available() and self.device.startswith('cuda') else 0
            }
            
            # å¦‚æœè¿™æ˜¯early-exitçš„æœ€åä¸€ä¸ªsubmodelï¼Œè®°å½•æ—¥å¿—
            if is_early_exit_last:
                # åªåœ¨ç»Ÿè®¡ä¿¡æ¯ä¸­è®°å½•early-exitï¼Œé¿å…é‡å¤æ‰“å°
                if not hasattr(self, '_early_exit_logged'):
                    print(f"Early-exit activated: executed {exit_position} submodels (0-{exit_position-1}), last submodel layers {submodel.layer_start}-{submodel.layer_end}")
                    self._early_exit_logged = True
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
   
        self.stats['inference_count'] += 1
        self.stats['layer_processing_time'].update(layer_stats)
        
        # æ„å»ºè¿”å›ç»“æœ
        result = {
            'hidden_states': current_hidden_states,
            'past_key_values': current_past_key_values if use_cache else None
        }
        
        # å¤„ç†logitsè¾“å‡º
        # å¦‚æœè¿›è¡Œäº†early-exitï¼Œéœ€è¦ç¡®ä¿æœ‰logitsè¾“å‡º
        # å¯¹äºearly-exitçš„å­æ¨¡å‹ï¼Œæˆ‘ä»¬åœ¨_prepare_early_exit_submodelä¸­å·²ç»æ·»åŠ äº†lm_headå’Œnorm
        if exit_position is not None:
            # Early-exitæƒ…å†µï¼šä½¿ç”¨ä¿å­˜çš„early_exit_submodelï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            if early_exit_submodel is not None:
                # ä½¿ç”¨å‡†å¤‡å¥½çš„early-exit submodel
                last_submodel = early_exit_submodel
            else:
                # å¦‚æœæ²¡æœ‰å‡†å¤‡early-exit submodelï¼Œä½¿ç”¨åŸå§‹çš„æœ€åä¸€ä¸ªsubmodel
                last_processed_idx = exit_position - 1
                last_submodel = self.submodels[last_processed_idx]
            
            if hasattr(last_submodel, 'lm_head') and last_submodel.lm_head is not None:
                # åº”ç”¨å½’ä¸€åŒ–ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                normalized_hidden_states = current_hidden_states
                if hasattr(last_submodel, 'norm') and last_submodel.norm is not None:
                    normalized_hidden_states = last_submodel.norm(current_hidden_states)
                
                logits = last_submodel.lm_head(normalized_hidden_states)
                result['logits'] = logits
            else:
                # å¦‚æœæœ€åä¸€ä¸ªsubmodelæ²¡æœ‰lm_headï¼Œéœ€è¦åˆ›å»ºä¸´æ—¶çš„
                print(f"Warning: Early-exit submodel {exit_position-1} has no lm_head, creating temporary one...")
                temp_submodel = self._prepare_early_exit_submodel(self.submodels[exit_position - 1])
                normalized_hidden_states = current_hidden_states
                if hasattr(temp_submodel, 'norm') and temp_submodel.norm is not None:
                    normalized_hidden_states = temp_submodel.norm(current_hidden_states)
                
                logits = temp_submodel.lm_head(normalized_hidden_states)
                result['logits'] = logits
        else:
            # æ­£å¸¸æƒ…å†µï¼šå¦‚æœæ˜¯æœ€åä¸€ä¸ªåˆ†å±‚ï¼Œåº”è¯¥æœ‰logits
            if hasattr(self.submodels[-1], 'lm_head'):
                logits = self.submodels[-1].lm_head(current_hidden_states)
                result['logits'] = logits
        
        return result
    
    def _get_early_exit_submodel(self, submodel_idx: int) -> LlamaSubModel:
        """
        è·å–early-exitå­æ¨¡å‹ï¼ˆä½¿ç”¨ç¼“å­˜ä¼˜åŒ–ï¼‰
        
        Args:
            submodel_idx: å­æ¨¡å‹ç´¢å¼•
        
        Returns:
            LlamaSubModel: å¸¦æœ‰å½’ä¸€åŒ–å±‚å’Œè¯­è¨€æ¨¡å‹å¤´çš„å­æ¨¡å‹
        """
        # æ£€æŸ¥ç¼“å­˜ä¸­æ˜¯å¦å·²æœ‰å‡†å¤‡å¥½çš„early-exit submodel
        if submodel_idx in self._early_exit_cache:
            return self._early_exit_cache[submodel_idx]
        
        # ç¼“å­˜ä¸­æ²¡æœ‰ï¼Œåˆ›å»ºæ–°çš„early-exit submodel
        original_submodel = self.submodels[submodel_idx]
        early_exit_submodel = self._prepare_early_exit_submodel(original_submodel)
        
        # ç¼“å­˜ç»“æœ
        self._early_exit_cache[submodel_idx] = early_exit_submodel
        
        return early_exit_submodel
    
    def _prepare_early_exit_submodel(self, original_submodel: LlamaSubModel) -> LlamaSubModel:
        """
        ä¸ºearly-exitå­æ¨¡å‹ï¼Œæ·»åŠ å½’ä¸€åŒ–å±‚å’Œè¯­è¨€æ¨¡å‹å¤´
        
        Args:
            original_submodel: åŸå§‹å­æ¨¡å‹
        
        Returns:
            LlamaSubModel: å¸¦æœ‰å½’ä¸€åŒ–å±‚å’Œè¯­è¨€æ¨¡å‹å¤´çš„å­æ¨¡å‹å‰¯æœ¬
        """
        early_exit_submodel = copy.deepcopy(original_submodel)
        
        # æ·»åŠ å½’ä¸€åŒ–å±‚å’Œè¯­è¨€æ¨¡å‹å¤´ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
        if not hasattr(early_exit_submodel, 'norm') or early_exit_submodel.norm is None:
            from ..models.llama_seq import LlamaRMSNorm
            early_exit_submodel.norm = LlamaRMSNorm(
                early_exit_submodel.config.hidden_size,
                eps=early_exit_submodel.config.rms_norm_eps
            ).to(self.device)
            
            # ğŸ”§ ä¿®å¤ï¼šå¦‚æœæœ‰åŸå§‹normæƒé‡ï¼Œå¤åˆ¶è¿‡æ¥
            if hasattr(self, '_original_norm_weights') and self._original_norm_weights is not None:
                early_exit_submodel.norm.weight.data.copy_(self._original_norm_weights.to(self.device))
                print(f"âœ… å·²ä¸ºearly-exit submodelå¤åˆ¶åŸå§‹normæƒé‡")
        
        if not hasattr(early_exit_submodel, 'lm_head') or early_exit_submodel.lm_head is None:
            import torch.nn as nn
            early_exit_submodel.lm_head = nn.Linear(
                early_exit_submodel.config.hidden_size,
                early_exit_submodel.config.vocab_size,
                bias=False
            ).to(self.device)
            
            # å¦‚æœæœ‰åŸå§‹æ¨¡å‹å¯ç”¨ï¼Œå°è¯•å¤åˆ¶æƒé‡
            if self._original_lm_head_weights is not None:
                early_exit_submodel.lm_head.weight.data.copy_(self._original_lm_head_weights)
                print(f"âœ… å·²ä¸ºearly-exit submodelå¤åˆ¶åŸå§‹lm_headæƒé‡")
        
        # æ ‡è®°ä¸ºæœ€ååˆ†å±‚ï¼Œè¿™æ ·forwardæ–¹æ³•ä¼šè¾“å‡ºlogits
        early_exit_submodel.is_last_partition = True
        
        return early_exit_submodel
    
    def _transfer_kv_cache_to_device(self, past_key_values: List[torch.FloatTensor]) -> List[torch.FloatTensor]:
        """å°†KVç¼“å­˜ä¼ è¾“åˆ°ç›®æ ‡è®¾å¤‡"""
        if past_key_values is None:
            return None
        
        transferred_cache = []
        for layer_cache in past_key_values:
            if layer_cache is None:
                transferred_cache.append(None)
            else:
                transferred_layer_cache = tuple(
                    cache_tensor.to(self.device) if cache_tensor is not None else None
                    for cache_tensor in layer_cache
                )
                transferred_cache.append(transferred_layer_cache)
        
        return transferred_cache
    
    def _extract_relevant_kv_cache(
        self,
        past_key_values: Optional[List[torch.FloatTensor]],
        layer_start: int,
        layer_end: int
    ) -> Optional[List[torch.FloatTensor]]:
        """æå–ä¸å½“å‰å­æ¨¡å‹ç›¸å…³çš„KVç¼“å­˜"""
        if past_key_values is None:
            return None
        
        relevant_cache = []
        for layer_idx in range(layer_start, layer_end + 1):
            if layer_idx < len(past_key_values):
                relevant_cache.append(past_key_values[layer_idx])
            else:
                relevant_cache.append(None)
        
        return relevant_cache
    
    def _merge_kv_cache(
        self,
        global_cache: Optional[List[torch.FloatTensor]],
        local_cache: Optional[List[torch.FloatTensor]],
        layer_start: int,
        layer_end: int
    ) -> Optional[List[torch.FloatTensor]]:
        """å°†å­æ¨¡å‹çš„KVç¼“å­˜åˆå¹¶åˆ°å…¨å±€ç¼“å­˜"""
        if local_cache is None:
            return global_cache
        
        if global_cache is None:
            # åˆå§‹åŒ–å…¨å±€ç¼“å­˜
            total_layers = max(submodel.layer_end for submodel in self.submodels) + 1
            global_cache = [None] * total_layers
        
        # åˆå¹¶ç¼“å­˜
        for i, layer_cache in enumerate(local_cache):
            global_layer_idx = layer_start + i
            if global_layer_idx < len(global_cache):
                global_cache[global_layer_idx] = layer_cache
        
        return global_cache
    
    def generate(
        self,
        input_ids: torch.Tensor,
        generation_config: Optional[GenerationConfig] = None,
        tokenizer: Optional[Any] = None,
        **kwargs
    ) -> torch.Tensor:
        """
        ä½¿ç”¨å•è®¾å¤‡åˆ†å±‚æ¨ç†ç”Ÿæˆæ–‡æœ¬
        
        Args:
            input_ids: è¾“å…¥token ID
            generation_config: ç”Ÿæˆé…ç½®
            tokenizer: åˆ†è¯å™¨
            **kwargs: å…¶ä»–ç”Ÿæˆå‚æ•°
        
        Returns:
            torch.Tensor: ç”Ÿæˆçš„tokenåºåˆ—
        """
        config = generation_config or self.generation_config
        
        # å°†è¾“å…¥ç§»åŠ¨åˆ°ç›®æ ‡è®¾å¤‡
        input_ids = input_ids.to(self.device)
        batch_size = input_ids.shape[0]
        
        # ç¡®å®šç”Ÿæˆé•¿åº¦
        max_length = config.max_length
        max_new_tokens = config.max_new_tokens
        if max_new_tokens is not None:
            max_length = input_ids.shape[1] + max_new_tokens
        
        # åˆå§‹åŒ–ç”ŸæˆçŠ¶æ€
        generated_tokens = input_ids.clone()
        past_key_values = None
        unfinished_sequences = torch.ones(batch_size, dtype=torch.long, device=self.device)
        
        # è®°å½•ç”Ÿæˆå¼€å§‹æ—¶é—´
        is_first_token = True
        decode_start_time = time.time()
        
        for step in range(input_ids.shape[1], max_length):
            # å‡†å¤‡å½“å‰æ­¥çš„è¾“å…¥
            if step == input_ids.shape[1]:
                # ç¬¬ä¸€æ­¥ï¼Œä½¿ç”¨å®Œæ•´è¾“å…¥
                current_input_ids = generated_tokens
            else:
                # åç»­æ­¥éª¤ï¼Œåªä½¿ç”¨æœ€åä¸€ä¸ªtoken
                current_input_ids = generated_tokens[:, -1:]
            
            # æ‰§è¡Œå‰å‘ä¼ æ’­
            with torch.no_grad():
                outputs = self.forward_pass(
                    input_ids=current_input_ids,
                    past_key_values=past_key_values,
                    use_cache=config.use_cache,
                    exit_position=config.exit_position
                )
            
            # è·å–ä¸‹ä¸€ä¸ªtokençš„logits
            next_token_logits = outputs['logits'][:, -1, :]
            
            # åº”ç”¨æ¸©åº¦
            if config.temperature != 1.0:
                next_token_logits = next_token_logits / config.temperature
            
            # é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
            if config.do_sample:
                # Top-ké‡‡æ ·
                if config.top_k > 0:
                    top_k_logits, top_k_indices = torch.topk(next_token_logits, config.top_k)
                    next_token_logits[next_token_logits < top_k_logits[:, -1, None]] = -float('inf')
                
                # Top-pé‡‡æ ·
                if config.top_p < 1.0:
                    sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
                    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
                    sorted_indices_to_remove = cumulative_probs > config.top_p
                    sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()
                    sorted_indices_to_remove[:, 0] = 0
                    
                    for batch_idx in range(batch_size):
                        indices_to_remove = sorted_indices[batch_idx][sorted_indices_to_remove[batch_idx]]
                        next_token_logits[batch_idx][indices_to_remove] = -float('inf')
                
                # å¤šé¡¹å¼é‡‡æ ·
                probs = F.softmax(next_token_logits, dim=-1)
                next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
            else:
                # è´ªå©ªè§£ç 
                next_tokens = torch.argmax(next_token_logits, dim=-1)
            
            # åªä¸ºæœªå®Œæˆçš„åºåˆ—æ›´æ–°token
            next_tokens = next_tokens * unfinished_sequences + (config.pad_token_id or 0) * (1 - unfinished_sequences)
            
            # æ·»åŠ æ–°token
            generated_tokens = torch.cat([generated_tokens, next_tokens.unsqueeze(-1)], dim=-1)
            
            # æ›´æ–°KVç¼“å­˜
            if config.use_cache:
                past_key_values = outputs['past_key_values']
            
            if is_first_token:
                # ä½¿ç”¨promptå¼€å§‹æ—¶é—´è®¡ç®—TTFTï¼Œå¦‚æœæ²¡æœ‰è®¾ç½®åˆ™ä½¿ç”¨decodeå¼€å§‹æ—¶é—´ï¼ˆå‘åå…¼å®¹ï¼‰
                if hasattr(self, '_current_prompt_start_time') and hasattr(self, '_is_new_prompt') and self._is_new_prompt:
                    first_token_time = time.time() - self._current_prompt_start_time
                    self._is_new_prompt = False  # é‡ç½®æ ‡è®°
                else:
                    first_token_time = time.time() - decode_start_time
                
                self.stats['total_time_to_first_token'] += first_token_time
                is_first_token = False
                
                # ç«‹å³è§£ç å¹¶æ‰“å°ç¬¬ä¸€ä¸ªtokenï¼Œç”¨äºéªŒè¯TTFTè®¡ç®—
                if tokenizer is not None:
                    first_token_text = tokenizer.decode(next_tokens[0], skip_special_tokens=True)
                    print(f"\n--- [åˆ†å±‚æ¨¡å‹] ç¬¬ä¸€ä¸ªtoken '{first_token_text}' ç”Ÿæˆå®Œæˆï¼ŒTTFT: {first_token_time*1000:.3f}ms")

            # æ£€æŸ¥æ˜¯å¦æœ‰åºåˆ—å®Œæˆ
            if config.eos_token_id is not None:
                unfinished_sequences = unfinished_sequences.mul((next_tokens != config.eos_token_id).long())
            
            # å¦‚æœæ‰€æœ‰åºåˆ—éƒ½å®Œæˆï¼Œæå‰åœæ­¢
            if unfinished_sequences.max() == 0:
                break
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        decode_time = time.time() - decode_start_time   
        tokens_generated = generated_tokens.shape[1] - input_ids.shape[1]
        self.stats['token_decode_time'] += decode_time
        self.stats['total_tokens_generated'] += tokens_generated * batch_size
        
        return generated_tokens
    
    def generate_text(
        self,
        prompt: str,
        tokenizer: Any,
        generation_config: Optional[GenerationConfig] = None,
        return_full_text: bool = False,
        **kwargs
    ) -> str:
        """
        ç”Ÿæˆæ–‡æœ¬å“åº”
        
        Args:
            prompt: è¾“å…¥æç¤º
            tokenizer: åˆ†è¯å™¨
            generation_config: ç”Ÿæˆé…ç½®
            return_full_text: æ˜¯å¦è¿”å›å®Œæ•´æ–‡æœ¬ï¼ˆåŒ…æ‹¬è¾“å…¥ï¼‰
            **kwargs: å…¶ä»–å‚æ•°
        
        Returns:
            str: ç”Ÿæˆçš„æ–‡æœ¬
        """
        # å¼€å§‹è®¡æ—¶ - ä»tokenizationå¼€å§‹ï¼Œä¸å®Œæ•´æ¨¡å‹ä¿æŒä¸€è‡´
        prompt_start_time = time.time()
        
        # ç¼–ç è¾“å…¥
        input_ids = tokenizer.encode(prompt, return_tensors="pt", add_special_tokens=True)
        input_length = input_ids.shape[1]

        generation_start_time = time.time()
        
        # æ ‡è®°è¿™æ˜¯ä¸€ä¸ªæ–°çš„promptï¼Œç”¨äºTTFTè®¡ç®—
        self._current_prompt_start_time = prompt_start_time
        self._is_new_prompt = True
        
        # ç”Ÿæˆ
        with torch.no_grad():
            generated_ids = self.generate(input_ids, generation_config, tokenizer, **kwargs)
        
        # è§£ç ç»“æœ
        if return_full_text:
            generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
        else:
            new_tokens = generated_ids[0][input_length:]
            generated_text = tokenizer.decode(new_tokens, skip_special_tokens=True)
        
        generation_time = time.time() - generation_start_time
        self.stats['total_generation_time'] += generation_time

        return generated_text
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        
        avg_token_decode_time = (
            self.stats['token_decode_time'] / self.stats['total_tokens_generated']
            if self.stats['total_tokens_generated'] > 0 else 0
        )
        
        return {
            **self.stats,
            'avg_token_decode_time': avg_token_decode_time,
            'tokens_per_second': self.stats['total_tokens_generated'] / self.stats['total_generation_time'] if self.stats['total_generation_time'] > 0 else 0,
            'device': self.device,
            'num_submodels': len(self.submodels)
        }
    
    def get_layer_analysis(self) -> Dict[str, Any]:
        """è·å–åˆ†å±‚åˆ†æä¿¡æ¯"""
        layer_analysis = {}
        
        for layer_key, layer_stats in self.stats['layer_processing_time'].items():
            layer_analysis[layer_key] = {
                'avg_processing_time': layer_stats['processing_time'],
                'memory_usage': layer_stats.get('memory_delta', 0),
                'relative_time_percentage': 0  # å°†åœ¨åé¢è®¡ç®—
            }
        
        # è®¡ç®—ç›¸å¯¹æ—¶é—´ç™¾åˆ†æ¯”
        total_layer_time = sum(
            stats['processing_time'] 
            for stats in self.stats['layer_processing_time'].values()
        )
        
        if total_layer_time > 0:
            for layer_key in layer_analysis:
                processing_time = layer_analysis[layer_key]['avg_processing_time']
                layer_analysis[layer_key]['relative_time_percentage'] = (
                    processing_time / total_layer_time * 100
                )
        
        return layer_analysis
    
    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.stats = {
            'time_to_first_token': 0.0,
            'token_decode_time': 0.0,
            'total_generation_time': 0.0,
            'layer_processing_time': {},
            'memory_usage': {},
            'total_tokens_generated': 0,
            'inference_count': 0
        }
        # é‡ç½®early-exitæ—¥å¿—æ ‡å¿—
        if hasattr(self, '_early_exit_logged'):
            delattr(self, '_early_exit_logged')
    
    def clear_early_exit_cache(self):
        """æ¸…ç†early-exit submodelç¼“å­˜ï¼Œé‡Šæ”¾å†…å­˜"""
        cached_count = len(self._early_exit_cache)
        
        # åˆ é™¤ç¼“å­˜çš„submodelä»¥é‡Šæ”¾å†…å­˜
        for submodel_idx, cached_submodel in self._early_exit_cache.items():
            # å°†ç¼“å­˜çš„submodelç§»åŠ¨åˆ°CPUå¹¶åˆ é™¤ä»¥é‡Šæ”¾GPUå†…å­˜
            if hasattr(cached_submodel, 'cpu'):
                cached_submodel.cpu()
            del cached_submodel
        
        self._early_exit_cache.clear()
        
        # å¦‚æœæœ‰CUDAï¼Œæ¸…ç†GPUå†…å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        if cached_count > 0:
            print(f"Early-exit submodelç¼“å­˜å·²æ¸…ç†ï¼ˆ{cached_count}ä¸ªç¼“å­˜é¡¹ï¼‰")
    
    def get_cache_info(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ä¿¡æ¯"""
        return {
            'cached_submodels': list(self._early_exit_cache.keys()),
            'cache_size': len(self._early_exit_cache),
            'has_original_lm_head': self._original_lm_head_weights is not None
        }
    
    def print_layer_analysis(self):
        """æ‰“å°åˆ†å±‚åˆ†æç»“æœ"""
        print("\n" + "="*60)
        print("å•è®¾å¤‡åˆ†å±‚æ¨ç†åˆ†æ")
        print("="*60)
        
        analysis = self.get_layer_analysis()
        stats = self.get_stats()
        
        print(f"è®¾å¤‡: {self.device}")
        print(f"å­æ¨¡å‹æ•°é‡: {len(self.submodels)}")
        print(f"æ€»æ¨ç†æ¬¡æ•°: {stats['inference_count']}")
        print(f"ç”Ÿæˆé€Ÿåº¦: {stats['tokens_per_second']:.2f} tokens/ç§’")
        
        print("\nåˆ†å±‚æ€§èƒ½åˆ†æ:")
        print("-" * 60)
        for layer_key, layer_info in analysis.items():
            print(f"{layer_key}:")
            print(f"  å¤„ç†æ—¶é—´: {layer_info['avg_processing_time']:.4f}ç§’ ({layer_info['relative_time_percentage']:.1f}%)")
            if layer_info['memory_usage'] > 0:
                print(f"  å†…å­˜å¢é‡: {layer_info['memory_usage'] / (1024**2):.1f} MB") 