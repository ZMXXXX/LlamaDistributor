#!/usr/bin/env python3
"""
LlamaDistributor æ¼”ç¤ºè„šæœ¬

å±•ç¤ºå¦‚ä½•ä½¿ç”¨LlamaDistributorè¿›è¡Œæ¨¡å‹åˆ†å±‚å’Œåˆ†å¸ƒå¼æ¨ç†
"""

import torch
from transformers import AutoTokenizer

from llamadist.partitioner.analyzer import LlamaModelAnalyzer
from llamadist.partitioner.strategies import PartitionStrategy
from llamadist.partitioner.splitter import LlamaPartitioner
from llamadist.inference.coordinator import DistributedInference, GenerationConfig

def main():
    print("ğŸš€ LlamaDistributor æ¼”ç¤º")
    print("=" * 50)
    
    # é…ç½®
    model_path = "/home/zmx/models/Llama/Llama-2-7b-hf"
    num_partitions = 2
    devices = ["cuda:0", "cuda:1"] if torch.cuda.device_count() >= 2 else ["cpu", "cpu"]
    
    print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"ğŸ”§ åˆ†å±‚æ•°é‡: {num_partitions}")
    print(f"ğŸ’» ç›®æ ‡è®¾å¤‡: {devices}")
    print()
    
    # 1. æ¨¡å‹åˆ†æ
    print("1ï¸âƒ£ åˆ†ææ¨¡å‹ç»“æ„...")
    analyzer = LlamaModelAnalyzer(model_path=model_path)
    model_info = analyzer.analyze_model(detailed=False)
    
    print(f"   âœ“ æ¨¡å‹: {model_info.model_name}")
    print(f"   âœ“ å±‚æ•°: {model_info.num_layers}")
    print(f"   âœ“ å‚æ•°: {model_info.total_params / 1e9:.2f}B")
    print(f"   âœ“ å†…å­˜: {model_info.total_memory / 1e9:.2f}GB")
    print()
    
    # 2. åˆ›å»ºåˆ†å±‚ç­–ç•¥
    print("2ï¸âƒ£ åˆ›å»ºåˆ†å±‚ç­–ç•¥...")
    strategy = PartitionStrategy(
        num_partitions=num_partitions,
        strategy_type="uniform",
        target_devices=devices
    )
    
    partitions = strategy.create_partitions(model_info)
    print(f"   âœ“ åˆ›å»ºäº† {len(partitions)} ä¸ªåˆ†å±‚:")
    for i, partition in enumerate(partitions):
        print(f"     - åˆ†å±‚ {i}: å±‚ {partition.layer_start}-{partition.layer_end} -> {partition.device}")
    print()
    
    # 3. æ‰§è¡Œæ¨¡å‹åˆ†å±‚
    print("3ï¸âƒ£ æ‰§è¡Œæ¨¡å‹åˆ†å±‚...")
    partitioner = LlamaPartitioner(model_path=model_path)
    submodels = partitioner.partition(
        strategy=strategy,
        analyze_first=False,
        copy_weights=True
    )
    
    print(f"   âœ“ æˆåŠŸåˆ›å»º {len(submodels)} ä¸ªå­æ¨¡å‹")
    for sm in submodels:
        info = sm.get_info()
        print(f"     - å­æ¨¡å‹ {info['partition_idx']}: {info['memory_usage']/1e6:.1f}MB @ {info['device']}")
    print()
    
    # 4. åˆ›å»ºåˆ†å¸ƒå¼æ¨ç†å¼•æ“
    print("4ï¸âƒ£ åˆ›å»ºåˆ†å¸ƒå¼æ¨ç†å¼•æ“...")
    inference_engine = DistributedInference(
        submodels=submodels,
        generation_config=GenerationConfig(
            max_new_tokens=10,
            temperature=1.0,
            do_sample=False
        )
    )
    print("   âœ“ åˆ†å¸ƒå¼æ¨ç†å¼•æ“å·²å°±ç»ª")
    print()
    
    # 5. æµ‹è¯•æ¨ç†
    print("5ï¸âƒ£ æµ‹è¯•åˆ†å¸ƒå¼æ¨ç†...")
    
    # åŠ è½½åˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # æµ‹è¯•è¾“å…¥
    test_prompts = [
        "Hello, how are you?",
        "The capital of France is",
        "In machine learning,"
    ]
    
    for i, prompt in enumerate(test_prompts):
        print(f"   æµ‹è¯• {i+1}: '{prompt}'")
        
        # ç¼–ç è¾“å…¥
        input_ids = tokenizer.encode(prompt, return_tensors='pt')
        print(f"     è¾“å…¥å½¢çŠ¶: {input_ids.shape}")
        
        # æ‰§è¡Œæ¨ç†
        import time
        start_time = time.time()
        result = inference_engine.forward_pass(
            input_ids=input_ids,
            use_cache=False
        )
        inference_time = time.time() - start_time
        
        print(f"     æ¨ç†è€—æ—¶: {inference_time:.3f}ç§’")
        print(f"     è¾“å‡ºå½¢çŠ¶: {result['logits'].shape}")
        
        # è·å–é¢„æµ‹çš„ä¸‹ä¸€ä¸ªtoken
        next_token_logits = result['logits'][0, -1, :]
        next_token_id = torch.argmax(next_token_logits).item()
        next_token = tokenizer.decode([next_token_id])
        print(f"     é¢„æµ‹ä¸‹ä¸€ä¸ªtoken: '{next_token}'")
        print()
    
    # 6. æ€§èƒ½ç»Ÿè®¡
    print("6ï¸âƒ£ æ€§èƒ½ç»Ÿè®¡...")
    stats = inference_engine.get_stats()
    print(f"   âœ“ æ¨ç†æ¬¡æ•°: {stats['inference_count']}")
    print(f"   âœ“ æ€»æ¨ç†æ—¶é—´: {stats['total_inference_time']:.3f}ç§’")
    print(f"   âœ“ å¹³å‡æ¨ç†æ—¶é—´: {stats['avg_inference_time']:.3f}ç§’")
    print(f"   âœ“ çŠ¶æ€ä¼ è¾“æ—¶é—´: {stats['state_transfer_time']:.3f}ç§’")
    print()
    
    print("ğŸ‰ æ¼”ç¤ºå®Œæˆï¼LlamaDistributoræˆåŠŸè¿è¡Œ")
    print("=" * 50)
    
    return submodels, inference_engine

if __name__ == "__main__":
    submodels, engine = main() 