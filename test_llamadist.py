#!/usr/bin/env python3
"""
LlamaDistributoræµ‹è¯•è„šæœ¬

æµ‹è¯•æ¨¡å‹åˆ†æã€åˆ†å±‚ã€åˆ†å¸ƒå¼æ¨ç†ç­‰åŠŸèƒ½
"""

import os
import torch
from transformers import AutoTokenizer
import time

# å¯¼å…¥LlamaDistributorç»„ä»¶
from llamadist.partitioner.analyzer import LlamaModelAnalyzer
from llamadist.partitioner.strategies import PartitionStrategy
from llamadist.partitioner.splitter import LlamaPartitioner
from llamadist.inference.coordinator import DistributedInference, GenerationConfig
from llamadist.submodels.manager import SubModelManager

def test_basic_functionality():
    """æµ‹è¯•åŸºæœ¬åŠŸèƒ½"""
    print("=" * 60)
    print("LlamaDistributor åŸºæœ¬åŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    
    # æ¨¡å‹è·¯å¾„
    model_path = "/home/zmx/models/Llama/Llama-2-7b-hf"
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„
    if not os.path.exists(model_path):
        print(f"é”™è¯¯ï¼šæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨ {model_path}")
        return False
    
    print(f"âœ“ æ¨¡å‹è·¯å¾„éªŒè¯é€šè¿‡: {model_path}")
    
    # æ£€æŸ¥CUDAå¯ç”¨æ€§
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"âœ“ ä½¿ç”¨è®¾å¤‡: {device}")
    
    try:
        # 1. æµ‹è¯•æ¨¡å‹åˆ†æ
        print("\n1. æµ‹è¯•æ¨¡å‹åˆ†æå™¨...")
        analyzer = LlamaModelAnalyzer(model_path=model_path)
        
        # ç®€å•åˆ†æï¼ˆé¿å…å†…å­˜é—®é¢˜ï¼‰
        model_info = analyzer.analyze_model(
            sample_input_shape=(1, 64),  # è¾ƒå°çš„è¾“å…¥
            device="cpu",  # ä½¿ç”¨CPUè¿›è¡Œåˆ†æ
            detailed=False  # ç®€å•åˆ†æ
        )
        
        print(f"   æ¨¡å‹åç§°: {model_info.model_name}")
        print(f"   å±‚æ•°: {model_info.num_layers}")
        print(f"   å‚æ•°æ€»æ•°: {model_info.total_params / 1e9:.2f}B")
        print(f"   æ€»å†…å­˜: {model_info.total_memory / 1e9:.2f}GB")
        print("âœ“ æ¨¡å‹åˆ†æå®Œæˆ")
        
        # 2. æµ‹è¯•åˆ†å±‚ç­–ç•¥
        print("\n2. æµ‹è¯•åˆ†å±‚ç­–ç•¥...")
        strategy = PartitionStrategy(
            num_partitions=2,
            strategy_type="uniform",
            devices=["cpu", "cpu"]  # éƒ½ä½¿ç”¨CPU
        )
        
        partitions = strategy.create_partitions(model_info)
        print(f"   åˆ›å»ºäº† {len(partitions)} ä¸ªåˆ†å±‚:")
        for i, partition in enumerate(partitions):
            print(f"     åˆ†å±‚ {i}: å±‚ {partition.layer_start}-{partition.layer_end}")
        print("âœ“ åˆ†å±‚ç­–ç•¥æµ‹è¯•å®Œæˆ")
        
        # 3. æµ‹è¯•æ¨¡å‹åˆ†å±‚å™¨
        print("\n3. æµ‹è¯•æ¨¡å‹åˆ†å±‚å™¨...")
        partitioner = LlamaPartitioner(model_path=model_path)
        
        # åˆ›å»ºå­æ¨¡å‹ï¼ˆè¾ƒå°è§„æ¨¡ï¼‰
        submodels = partitioner.partition(
            strategy=strategy,
            analyze_first=False,  # ä½¿ç”¨å·²æœ‰çš„åˆ†æç»“æœ
            copy_weights=True
        )
        
        print(f"   æˆåŠŸåˆ›å»º {len(submodels)} ä¸ªå­æ¨¡å‹")
        for sm in submodels:
            info = sm.get_info()
            print(f"     å­æ¨¡å‹ {info['partition_idx']}: {info['memory_usage']/1e6:.1f}MB")
        print("âœ“ æ¨¡å‹åˆ†å±‚å®Œæˆ")
        
        # 4. æµ‹è¯•åˆ†å¸ƒå¼æ¨ç†
        print("\n4. æµ‹è¯•åˆ†å¸ƒå¼æ¨ç†...")
        
        # åŠ è½½åˆ†è¯å™¨
        try:
            tokenizer = AutoTokenizer.from_pretrained(model_path)
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            print("   âœ“ åˆ†è¯å™¨åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"   è­¦å‘Šï¼šåˆ†è¯å™¨åŠ è½½å¤±è´¥: {e}")
            print("   ä½¿ç”¨æ¨¡æ‹Ÿè¾“å…¥è¿›è¡Œæµ‹è¯•...")
            tokenizer = None
        
        # åˆ›å»ºåˆ†å¸ƒå¼æ¨ç†å¼•æ“
        inference_engine = DistributedInference(
            submodels=submodels,
            generation_config=GenerationConfig(
                max_new_tokens=5,  # åªç”Ÿæˆå°‘é‡token
                temperature=1.0,
                do_sample=False  # ä½¿ç”¨greedyè§£ç 
            )
        )
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        if tokenizer is not None:
            # ä½¿ç”¨çœŸå®è¾“å…¥
            test_prompt = "Hello, how are you?"
            input_ids = tokenizer.encode(test_prompt, return_tensors='pt')
        else:
            # ä½¿ç”¨æ¨¡æ‹Ÿè¾“å…¥
            input_ids = torch.randint(1, 1000, (1, 8))
        
        print(f"   è¾“å…¥å½¢çŠ¶: {input_ids.shape}")
        
        # å‰å‘ä¼ æ’­æµ‹è¯•
        start_time = time.time()
        result = inference_engine.forward_pass(
            input_ids=input_ids,
            use_cache=False  # æš‚æ—¶ä¸ä½¿ç”¨ç¼“å­˜
        )
        forward_time = time.time() - start_time
        
        print(f"   å‰å‘ä¼ æ’­è€—æ—¶: {forward_time:.3f}ç§’")
        print(f"   è¾“å‡ºlogitså½¢çŠ¶: {result['logits'].shape}")
        print("âœ“ åˆ†å¸ƒå¼æ¨ç†æµ‹è¯•å®Œæˆ")
        
        # 5. æµ‹è¯•æ–‡æœ¬ç”Ÿæˆï¼ˆå¦‚æœæœ‰åˆ†è¯å™¨ï¼‰
        if tokenizer is not None:
            print("\n5. æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ...")
            try:
                start_time = time.time()
                generated_text = inference_engine.generate_text(
                    prompt="Hello",
                    tokenizer=tokenizer,
                    return_full_text=True
                )
                generation_time = time.time() - start_time
                
                print(f"   ç”Ÿæˆè€—æ—¶: {generation_time:.3f}ç§’")
                print(f"   ç”Ÿæˆæ–‡æœ¬: {generated_text[:100]}...")
                print("âœ“ æ–‡æœ¬ç”Ÿæˆæµ‹è¯•å®Œæˆ")
            except Exception as e:
                print(f"   æ–‡æœ¬ç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
        
        # 6. æµ‹è¯•å­æ¨¡å‹ç®¡ç†å™¨
        print("\n6. æµ‹è¯•å­æ¨¡å‹ç®¡ç†å™¨...")
        manager = SubModelManager(base_dir="./test_models")
        
        # ä¿å­˜åˆ†å±‚æ¨¡å‹
        model_name = "test_llama2_7b"
        manager.save_partitioned_model(
            submodels=submodels,
            model_name=model_name,
            description="Llama-2-7Bæµ‹è¯•æ¨¡å‹",
            overwrite=True
        )
        
        # åˆ—å‡ºæ¨¡å‹
        models = manager.list_models()
        print(f"   ç®¡ç†å™¨ä¸­çš„æ¨¡å‹æ•°é‡: {len(models)}")
        
        # éªŒè¯æ¨¡å‹
        validation_result = manager.validate_model(model_name)
        print(f"   æ¨¡å‹éªŒè¯ç»“æœ: {'é€šè¿‡' if validation_result['valid'] else 'å¤±è´¥'}")
        
        print("âœ“ å­æ¨¡å‹ç®¡ç†å™¨æµ‹è¯•å®Œæˆ")
        
        # 7. æ€§èƒ½ç»Ÿè®¡
        print("\n7. æ€§èƒ½ç»Ÿè®¡...")
        stats = inference_engine.get_stats()
        print(f"   æ¨ç†æ¬¡æ•°: {stats['inference_count']}")
        print(f"   æ€»æ¨ç†æ—¶é—´: {stats['total_inference_time']:.3f}ç§’")
        if stats['inference_count'] > 0:
            print(f"   å¹³å‡æ¨ç†æ—¶é—´: {stats['avg_inference_time']:.3f}ç§’")
        print("âœ“ æ€§èƒ½ç»Ÿè®¡å®Œæˆ")
        
        print("\n" + "=" * 60)
        print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼LlamaDistributoré…ç½®æˆåŠŸ")
        print("=" * 60)
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_environment():
    """æµ‹è¯•ç¯å¢ƒé…ç½®"""
    print("=" * 60)
    print("ç¯å¢ƒé…ç½®æµ‹è¯•")
    print("=" * 60)
    
    # æ£€æŸ¥Pythonç‰ˆæœ¬
    import sys
    print(f"Pythonç‰ˆæœ¬: {sys.version}")
    
    # æ£€æŸ¥PyTorch
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"CUDAè®¾å¤‡æ•°é‡: {torch.cuda.device_count()}")
        print(f"å½“å‰CUDAè®¾å¤‡: {torch.cuda.current_device()}")
        print(f"CUDAè®¾å¤‡åç§°: {torch.cuda.get_device_name()}")
    
    # æ£€æŸ¥transformers
    import transformers
    print(f"Transformersç‰ˆæœ¬: {transformers.__version__}")
    
    # æ£€æŸ¥LlamaDistributor
    import llamadist
    print(f"LlamaDistributorç‰ˆæœ¬: {llamadist.__version__}")
    
    print("âœ“ ç¯å¢ƒé…ç½®æ£€æŸ¥å®Œæˆ")


if __name__ == "__main__":
    print("LlamaDistributor å®Œæ•´æµ‹è¯•")
    print("ä½¿ç”¨æ¨¡å‹: /home/zmx/models/Llama/Llama-2-7b-hf")
    print()
    
    # æµ‹è¯•ç¯å¢ƒ
    test_environment()
    print()
    
    # æµ‹è¯•åŸºæœ¬åŠŸèƒ½
    success = test_basic_functionality()
    
    if success:
        print("\nğŸ‰ æ­å–œï¼LlamaDistributorç¯å¢ƒé…ç½®æˆåŠŸå¹¶é€šè¿‡æ‰€æœ‰æµ‹è¯•ï¼")
    else:
        print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯å¹¶ä¿®å¤ã€‚") 