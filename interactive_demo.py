#!/usr/bin/env python3
"""
LlamaDistributor äº¤äº’å¼é—®ç­”æ¼”ç¤º

ç”¨æˆ·å¯ä»¥è¾“å…¥é—®é¢˜ï¼Œé€šè¿‡åˆ†å±‚æŽ¨ç†èŽ·å¾—å›žç­”
"""

import torch
from transformers import AutoTokenizer
import time

from llamadist.partitioner.strategies import PartitionStrategy
from llamadist.partitioner.splitter import LlamaPartitioner
from llamadist.inference.coordinator import DistributedInference, GenerationConfig

class LlamaDistributorQA:
    """LlamaDistributoré—®ç­”ç³»ç»Ÿ"""
    
    def __init__(self, model_path: str = "/home/zmx/models/Llama/Llama-2-7b-hf"):
        """åˆå§‹åŒ–é—®ç­”ç³»ç»Ÿ"""
        self.model_path = model_path
        self.inference_engine = None
        self.tokenizer = None
        self.setup()
    
    def setup(self):
        """è®¾ç½®åˆ†å¸ƒå¼æŽ¨ç†çŽ¯å¢ƒ"""
        print("ðŸš€ åˆå§‹åŒ–LlamaDistributoré—®ç­”ç³»ç»Ÿ...")
        
        # é…ç½®
        num_partitions = 2
        devices = ["cuda:0", "cuda:1"] if torch.cuda.device_count() >= 2 else ["cpu", "cpu"]
        
        print(f"ðŸ’» ä½¿ç”¨è®¾å¤‡: {devices}")
        
        # åˆ›å»ºåˆ†å±‚ç­–ç•¥
        strategy = PartitionStrategy(
            num_partitions=num_partitions,
            strategy_type="uniform",
            target_devices=devices
        )
        
        # åˆ†å±‚æ¨¡åž‹
        print("ðŸ”§ åˆ†å±‚æ¨¡åž‹...")
        partitioner = LlamaPartitioner(model_path=self.model_path)
        submodels = partitioner.partition(
            strategy=strategy,
            analyze_first=False,
            copy_weights=True
        )
        
        # åˆ›å»ºåˆ†å¸ƒå¼æŽ¨ç†å¼•æ“Ž
        self.inference_engine = DistributedInference(
            submodels=submodels,
            generation_config=GenerationConfig(
                max_new_tokens=30,
                temperature=0.7,
                do_sample=True,
                top_p=0.9,
                use_cache=False  # æš‚æ—¶ä¸ä½¿ç”¨ç¼“å­˜ä»¥ç¡®ä¿ç¨³å®šæ€§
            )
        )
        
        # åŠ è½½åˆ†è¯å™¨
        print("ðŸ“š åŠ è½½åˆ†è¯å™¨...")
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        print("âœ… åˆå§‹åŒ–å®Œæˆï¼")
        print("=" * 60)
    
    def answer_question(self, question: str, max_new_tokens: int = 20) -> str:
        """å›žç­”é—®é¢˜"""
        print(f"â“ é—®é¢˜: {question}")
        print("ðŸ’­ æ€è€ƒä¸­...")
        
        start_time = time.time()
        
        # ç¼–ç è¾“å…¥
        input_ids = self.tokenizer.encode(question, return_tensors='pt')
        first_device = self.inference_engine.submodels[0].get_info()['device']
        input_ids = input_ids.to(first_device)
        
        # ç”Ÿæˆå›žç­”
        generated_tokens = []
        all_input_ids = input_ids
        
        for step in range(max_new_tokens):
            # æ‰§è¡Œå‰å‘ä¼ æ’­
            result = self.inference_engine.forward_pass(
                input_ids=all_input_ids,
                past_key_values=None,
                use_cache=False
            )
            
            # èŽ·å–ä¸‹ä¸€ä¸ªtoken
            next_token_logits = result['logits'][0, -1, :]
            next_token_logits = next_token_logits / 0.7
            
            # Top-ké‡‡æ ·
            top_k = 50
            if top_k > 0:
                indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]
                next_token_logits[indices_to_remove] = float('-inf')
            
            probs = torch.softmax(next_token_logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1)
            
            # æ£€æŸ¥åœæ­¢æ¡ä»¶
            if next_token.item() == self.tokenizer.eos_token_id:
                break
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯å¥å·ï¼Œå¦‚æžœæ˜¯åˆ™å¯èƒ½åœæ­¢
            if self.tokenizer.decode([next_token.item()]).strip() == '.':
                generated_tokens.append(next_token.item())
                next_token_device = next_token.to(first_device)
                all_input_ids = torch.cat([all_input_ids, next_token_device.unsqueeze(0)], dim=1)
                break
            
            # æ·»åŠ åˆ°åºåˆ—
            generated_tokens.append(next_token.item())
            next_token_device = next_token.to(first_device)
            all_input_ids = torch.cat([all_input_ids, next_token_device.unsqueeze(0)], dim=1)
        
        # è§£ç å®Œæ•´ç­”æ¡ˆ
        answer = self.tokenizer.decode(all_input_ids[0], skip_special_tokens=True)
        
        elapsed_time = time.time() - start_time
        print(f"âš¡ ç”Ÿæˆæ—¶é—´: {elapsed_time:.2f}ç§’")
        print(f"ðŸ’¬ å›žç­”: {answer}")
        print("-" * 60)
        
        return answer
    
    def interactive_mode(self):
        """äº¤äº’æ¨¡å¼"""
        print("ðŸŽ¯ è¿›å…¥äº¤äº’é—®ç­”æ¨¡å¼")
        print("è¾“å…¥é—®é¢˜ï¼Œè¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
        print("=" * 60)
        
        while True:
            try:
                question = input("ðŸ¤” è¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ").strip()
                
                if question.lower() in ['quit', 'exit', 'é€€å‡º']:
                    print("ðŸ‘‹ å†è§ï¼")
                    break
                
                if not question:
                    print("âŒ è¯·è¾“å…¥æœ‰æ•ˆé—®é¢˜")
                    continue
                
                # å›žç­”é—®é¢˜
                self.answer_question(question)
                
            except KeyboardInterrupt:
                print("\nðŸ‘‹ å†è§ï¼")
                break
            except Exception as e:
                print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
                continue

def main():
    """ä¸»å‡½æ•°"""
    print("ðŸŽ‰ æ¬¢è¿Žä½¿ç”¨LlamaDistributoré—®ç­”ç³»ç»Ÿ!")
    print("=" * 60)
    
    # åˆ›å»ºé—®ç­”ç³»ç»Ÿ
    qa_system = LlamaDistributorQA()
    
    # é¢„è®¾é—®é¢˜æµ‹è¯•
    test_questions = [
        "What is the capital of France?",
        "How does machine learning work?",
        "What is Python programming language?",
    ]
    
    print("ðŸ“‹ é¢„è®¾é—®é¢˜æµ‹è¯•:")
    for i, question in enumerate(test_questions, 1):
        print(f"\n{i}. {question}")
        qa_system.answer_question(question, max_new_tokens=15)
    
    print("\nðŸŽ¯ å¼€å§‹äº¤äº’æ¨¡å¼...")
    qa_system.interactive_mode()

if __name__ == "__main__":
    main() 