#!/usr/bin/env python3
"""
å•è®¾å¤‡åˆ†å±‚æ¨ç†æ¼”ç¤º

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨æ–°çš„SINGLE_DEVICEç­–ç•¥åœ¨åŒä¸€è®¾å¤‡ä¸Šè¿›è¡Œåˆ†å±‚æ¨ç†ã€‚
è¿™ç§ç­–ç•¥é€‚ç”¨äºï¼š
1. æµ‹è¯•åˆ†å±‚æ•ˆæœ
2. å†…å­˜ä¼˜åŒ–
3. åˆ†æä¸åŒå±‚çš„è®¡ç®—å¼€é”€
4. è°ƒè¯•åˆ†å±‚é€»è¾‘
"""

import torch
import time
from transformers import LlamaTokenizer, LlamaConfig
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from llamadist.partitioner.strategies import PartitionStrategy, StrategyType
from llamadist.partitioner.splitter import LlamaPartitioner
from llamadist.inference.single_device_coordinator import SingleDeviceInference
from llamadist.inference.coordinator import GenerationConfig


def demo_single_device_partitioning():
    """æ¼”ç¤ºå•è®¾å¤‡åˆ†å±‚æ¨ç†çš„åŸºæœ¬åŠŸèƒ½"""
    print("ğŸ” LlamaDistributor - å•è®¾å¤‡åˆ†å±‚æ¨ç†æ¼”ç¤º")
    print("=" * 60)
    
    # æ£€æŸ¥è®¾å¤‡
    device = "cuda:0" if torch.cuda.is_available() else "cpu"
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # æ¨¡å‹è·¯å¾„ - éœ€è¦æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
    model_path = "/path/to/llama-2-7b-hf"  # è¯·æ›¿æ¢ä¸ºå®é™…æ¨¡å‹è·¯å¾„
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
    if not Path(model_path).exists():
        print(f"é”™è¯¯ï¼šæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        print("è¯·ä¸‹è½½Llama-2-7Bæ¨¡å‹å¹¶æ›´æ–°model_pathå˜é‡")
        return
    
    try:
        # 1. åˆ›å»ºåˆ†å±‚å™¨
        print("\n1. åˆå§‹åŒ–åˆ†å±‚å™¨...")
        partitioner = LlamaPartitioner(model_path=model_path)
        
        # 2. åˆ†ææ¨¡å‹
        print("2. åˆ†ææ¨¡å‹ç»“æ„...")
        model_info = partitioner.analyze_model()
        print(f"æ¨¡å‹ä¿¡æ¯: {model_info.num_layers}å±‚, {model_info.total_params:,}å‚æ•°")
        
        # 3. æ¼”ç¤ºä¸åŒçš„å•è®¾å¤‡åˆ†å±‚ç­–ç•¥
        demo_strategies = [
            {
                "name": "å‡åŒ€4åˆ†å±‚",
                "strategy": PartitionStrategy(
                    num_partitions=4,
                    strategy_type=StrategyType.SINGLE_DEVICE,
                    single_device=device
                )
            },
            {
                "name": "è‡ªå®šä¹‰åˆ†å±‚ç‚¹",
                "strategy": PartitionStrategy(
                    num_partitions=3,
                    strategy_type=StrategyType.SINGLE_DEVICE,
                    single_device=device,
                    custom_boundaries=[(0, 10), (11, 21), (22, 31)]
                )
            },
            {
                "name": "ä¸å‡åŒ€åˆ†å±‚",
                "strategy": PartitionStrategy(
                    num_partitions=3,
                    strategy_type=StrategyType.SINGLE_DEVICE,
                    single_device=device,
                    custom_boundaries=[(0, 7), (8, 23), (24, 31)]
                )
            }
        ]
        
        for demo in demo_strategies:
            print(f"\n{'='*50}")
            print(f"æ¼”ç¤ºç­–ç•¥: {demo['name']}")
            print(f"{'='*50}")
            
            # åˆ›å»ºåˆ†å±‚é…ç½®
            partitions = demo['strategy'].create_partitions(model_info)
            print(f"åˆ†å±‚é…ç½®:")
            for i, partition in enumerate(partitions):
                print(f"  åˆ†å±‚ {i}: å±‚{partition.layer_start}-{partition.layer_end} @ {partition.device}")
            
            # æ‰§è¡Œåˆ†å±‚
            print("æ‰§è¡Œæ¨¡å‹åˆ†å±‚...")
            submodels = partitioner.partition(
                strategy=demo['strategy'],
                copy_weights=True
            )
            
            # åˆ›å»ºå•è®¾å¤‡æ¨ç†å¼•æ“
            print("åˆ›å»ºå•è®¾å¤‡æ¨ç†å¼•æ“...")
            inference_engine = SingleDeviceInference(
                submodels=submodels,
                generation_config=GenerationConfig(
                    max_new_tokens=20,
                    temperature=0.8,
                    do_sample=True,
                    use_cache=True
                ),
                device=device
            )
            
            # æµ‹è¯•æ¨ç†
            print("æµ‹è¯•æ¨ç†æ€§èƒ½...")
            test_input = torch.tensor([[1, 2, 3, 4, 5]], device=device)
            
            # é¢„çƒ­
            for _ in range(3):
                _ = inference_engine.forward_pass(test_input, use_cache=False)
            
            # æ€§èƒ½æµ‹è¯•
            inference_times = []
            for i in range(10):
                start_time = time.time()
                result = inference_engine.forward_pass(test_input, use_cache=True)
                end_time = time.time()
                inference_times.append(end_time - start_time)
            
            avg_time = sum(inference_times) / len(inference_times)
            print(f"å¹³å‡æ¨ç†æ—¶é—´: {avg_time:.4f}ç§’")
            
            # æ˜¾ç¤ºåˆ†å±‚åˆ†æ
            inference_engine.print_layer_analysis()
            
            # æ¸…ç†å†…å­˜
            del submodels
            del inference_engine
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
    
    except Exception as e:
        print(f"æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


def demo_text_generation():
    """æ¼”ç¤ºå•è®¾å¤‡åˆ†å±‚æ¨ç†çš„æ–‡æœ¬ç”ŸæˆåŠŸèƒ½"""
    print("\n" + "="*60)
    print("å•è®¾å¤‡åˆ†å±‚æ–‡æœ¬ç”Ÿæˆæ¼”ç¤º")
    print("="*60)
    
    # è®¾å¤‡é…ç½®
    device = "cuda:0" if torch.cuda.is_available() else "cpu"
    
    # æ¨¡å‹è·¯å¾„
    model_path = "/path/to/llama-2-7b-hf"  # è¯·æ›¿æ¢ä¸ºå®é™…æ¨¡å‹è·¯å¾„
    
    if not Path(model_path).exists():
        print(f"é”™è¯¯ï¼šæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        return
    
    try:
        # åŠ è½½åˆ†è¯å™¨
        print("åŠ è½½åˆ†è¯å™¨...")
        tokenizer = LlamaTokenizer.from_pretrained(model_path)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # åˆ›å»ºåˆ†å±‚å™¨
        print("åˆ›å»ºåˆ†å±‚å™¨...")
        partitioner = LlamaPartitioner(model_path=model_path)
        model_info = partitioner.analyze_model()
        
        # åˆ›å»ºåˆ†å±‚ç­–ç•¥ - åˆ†æˆ3ä¸ªå­æ¨¡å‹
        strategy = PartitionStrategy(
            num_partitions=3,
            strategy_type=StrategyType.SINGLE_DEVICE,
            single_device=device,
            custom_boundaries=[(0, 10), (11, 21), (22, 31)]
        )
        
        # æ‰§è¡Œåˆ†å±‚
        print("æ‰§è¡Œæ¨¡å‹åˆ†å±‚...")
        submodels = partitioner.partition(strategy=strategy, copy_weights=True)
        
        # åˆ›å»ºæ¨ç†å¼•æ“
        print("åˆ›å»ºæ¨ç†å¼•æ“...")
        inference_engine = SingleDeviceInference(
            submodels=submodels,
            generation_config=GenerationConfig(
                max_new_tokens=50,
                temperature=0.8,
                top_p=0.9,
                do_sample=True,
                use_cache=True,
                eos_token_id=tokenizer.eos_token_id,
                pad_token_id=tokenizer.pad_token_id
            ),
            device=device
        )
        
        # æµ‹è¯•æç¤º
        test_prompts = [
            "The future of artificial intelligence is",
            "In a world where technology advances rapidly,",
            "The most important lesson I learned is",
            "Once upon a time in a distant galaxy,"
        ]
        
        print("\næ–‡æœ¬ç”Ÿæˆæµ‹è¯•:")
        print("-" * 40)
        
        for i, prompt in enumerate(test_prompts):
            print(f"\næç¤º {i+1}: {prompt}")
            
            start_time = time.time()
            generated_text = inference_engine.generate_text(
                prompt=prompt,
                tokenizer=tokenizer,
                return_full_text=True
            )
            generation_time = time.time() - start_time
            
            print(f"ç”Ÿæˆç»“æœ: {generated_text}")
            print(f"ç”Ÿæˆæ—¶é—´: {generation_time:.2f}ç§’")
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        print("\næœ€ç»ˆæ€§èƒ½ç»Ÿè®¡:")
        print("-" * 40)
        stats = inference_engine.get_stats()
        print(f"æ€»æ¨ç†æ¬¡æ•°: {stats['inference_count']}")
        print(f"å¹³å‡æ¨ç†æ—¶é—´: {stats['avg_inference_time']:.4f}ç§’")
        print(f"ç”Ÿæˆé€Ÿåº¦: {stats['tokens_per_second']:.2f} tokens/ç§’")
        print(f"æ€»ç”Ÿæˆtokenæ•°: {stats['total_tokens_generated']}")
        
        # åˆ†å±‚åˆ†æ
        inference_engine.print_layer_analysis()
        
    except Exception as e:
        print(f"æ–‡æœ¬ç”Ÿæˆæ¼”ç¤ºä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


def compare_strategies():
    """æ¯”è¾ƒä¸åŒåˆ†å±‚ç­–ç•¥çš„æ€§èƒ½"""
    print("\n" + "="*60)
    print("åˆ†å±‚ç­–ç•¥æ€§èƒ½æ¯”è¾ƒ")
    print("="*60)
    
    device = "cuda:0" if torch.cuda.is_available() else "cpu"
    model_path = "/path/to/llama-2-7b-hf"  # è¯·æ›¿æ¢ä¸ºå®é™…æ¨¡å‹è·¯å¾„
    
    if not Path(model_path).exists():
        print(f"é”™è¯¯ï¼šæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        return
    
    try:
        # åˆ›å»ºåˆ†å±‚å™¨
        partitioner = LlamaPartitioner(model_path=model_path)
        model_info = partitioner.analyze_model()
        
        # æµ‹è¯•ä¸åŒçš„åˆ†å±‚ç­–ç•¥
        strategies_to_compare = [
            {
                "name": "2åˆ†å±‚",
                "strategy": PartitionStrategy(
                    num_partitions=2,
                    strategy_type=StrategyType.SINGLE_DEVICE,
                    single_device=device
                )
            },
            {
                "name": "4åˆ†å±‚",
                "strategy": PartitionStrategy(
                    num_partitions=4,
                    strategy_type=StrategyType.SINGLE_DEVICE,
                    single_device=device
                )
            },
            {
                "name": "8åˆ†å±‚",
                "strategy": PartitionStrategy(
                    num_partitions=8,
                    strategy_type=StrategyType.SINGLE_DEVICE,
                    single_device=device
                )
            }
        ]
        
        results = []
        
        for strategy_config in strategies_to_compare:
            print(f"\næµ‹è¯•ç­–ç•¥: {strategy_config['name']}")
            
            # åˆ›å»ºå­æ¨¡å‹
            submodels = partitioner.partition(
                strategy=strategy_config['strategy'],
                copy_weights=True
            )
            
            # åˆ›å»ºæ¨ç†å¼•æ“
            inference_engine = SingleDeviceInference(
                submodels=submodels,
                device=device
            )
            
            # æ€§èƒ½æµ‹è¯•
            test_input = torch.tensor([[1, 2, 3, 4, 5]], device=device)
            
            # é¢„çƒ­
            for _ in range(3):
                _ = inference_engine.forward_pass(test_input)
            
            # æµ‹è¯•æ¨ç†æ—¶é—´
            inference_times = []
            for _ in range(20):
                start_time = time.time()
                _ = inference_engine.forward_pass(test_input)
                end_time = time.time()
                inference_times.append(end_time - start_time)
            
            avg_time = sum(inference_times) / len(inference_times)
            std_time = (sum((t - avg_time) ** 2 for t in inference_times) / len(inference_times)) ** 0.5
            
            results.append({
                "name": strategy_config['name'],
                "avg_time": avg_time,
                "std_time": std_time,
                "num_partitions": len(submodels)
            })
            
            print(f"  å¹³å‡æ¨ç†æ—¶é—´: {avg_time:.4f}Â±{std_time:.4f}ç§’")
            
            # æ¸…ç†å†…å­˜
            del submodels
            del inference_engine
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        
        # æ˜¾ç¤ºæ¯”è¾ƒç»“æœ
        print("\nç­–ç•¥æ¯”è¾ƒç»“æœ:")
        print("-" * 50)
        print(f"{'ç­–ç•¥åç§°':<10} {'åˆ†å±‚æ•°':<8} {'å¹³å‡æ—¶é—´(ç§’)':<15} {'æ ‡å‡†å·®':<10}")
        print("-" * 50)
        for result in results:
            print(f"{result['name']:<10} {result['num_partitions']:<8} {result['avg_time']:<15.4f} {result['std_time']:<10.4f}")
        
        # æ‰¾å‡ºæœ€å¿«çš„ç­–ç•¥
        best_strategy = min(results, key=lambda x: x['avg_time'])
        print(f"\næœ€ä½³ç­–ç•¥: {best_strategy['name']} (å¹³å‡æ—¶é—´: {best_strategy['avg_time']:.4f}ç§’)")
        
    except Exception as e:
        print(f"ç­–ç•¥æ¯”è¾ƒä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


def main():
    """ä¸»å‡½æ•°"""
    print("é€‰æ‹©æ¼”ç¤ºæ¨¡å¼:")
    print("1. åŸºæœ¬å•è®¾å¤‡åˆ†å±‚æ¼”ç¤º")
    print("2. æ–‡æœ¬ç”Ÿæˆæ¼”ç¤º")
    print("3. ç­–ç•¥æ€§èƒ½æ¯”è¾ƒ")
    print("4. è¿è¡Œæ‰€æœ‰æ¼”ç¤º")
    
    choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1-4): ").strip()
    
    if choice == "1":
        demo_single_device_partitioning()
    elif choice == "2":
        demo_text_generation()
    elif choice == "3":
        compare_strategies()
    elif choice == "4":
        demo_single_device_partitioning()
        demo_text_generation()
        compare_strategies()
    else:
        print("æ— æ•ˆé€‰æ‹©ï¼Œè¿è¡ŒåŸºæœ¬æ¼”ç¤º...")
        demo_single_device_partitioning()


if __name__ == "__main__":
    main() 